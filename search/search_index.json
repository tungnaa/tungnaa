{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tungnaa","text":""},{"location":"#tungnaa-interactive-voice-instruments","title":"Tungnaa Interactive Voice Instruments","text":"<p>Training and GUI inference for interactive artistic text-to-voice models.</p>"},{"location":"#installation","title":"Installation","text":"<p><code>pip install tungnaa[gui]</code> (to use the instrument)</p> <p><code>pip install tungnaa[train]</code> (if you are installing on a server to train models)</p>"},{"location":"#usage","title":"Usage","text":"<p><code>tungnaa --help</code></p>"},{"location":"#models-from-huggingface","title":"Models from Huggingface","text":"<p><code>git clone git@hf.co:intelligent-instruments-Lab/tungnaa</code></p>"},{"location":"#running-with-the-python-audio-engine","title":"Running with the Python Audio Engine","text":"<ul> <li>[ ] #todo model selection from the Tungnaa gui, including block size and sample rate that match system (currently not possible to have a SR mismatch)</li> <li>[ ] #todo audio device selection from the Tungnaa gui</li> </ul> <pre><code>tungnaa run --tts models/tts/rtalign_044_jvs.ckpt --vocoder models/vocoder/rave3-jvs-warm200k-lobeta_c052f53b23_streaming.ts --audio-out default\n</code></pre>"},{"location":"#using-supercollider-puredata-or-max-as-audio-engine","title":"Using SuperCollider, PureData or Max as Audio Engine","text":"<p>If <code>--latent-audio</code> switch is enabled, Tungnaa will stream RAVE latent trajectories over a single audio-rate channel, which can be piped into another audio engine running the RAVE vocoder. The piping can be done relatively easily on Linux using JACK, and on MacOS using Blackhole.</p> <p>SuperCollider: <code>sclang supercollider/rtvoice-demo.scd</code> </p> <pre><code>tungnaa run --tts models/tts/rtalign_044_jvs.ckpt --latent_audio\n</code></pre>"},{"location":"#training-models","title":"Training Models","text":""},{"location":"#vocoder-training","title":"vocoder training","text":"<p>using victor-shepardson RAVE fork </p> <p>example preprocessing with joining of short files (especially useful for datasets containing many short utterances) </p> <pre><code>rave preprocess \\\n--input_path /path/to/audio/directory \\\n--output_path /path/to/tmp/storage/myravedata \\\n--num_signal 150000 --sampling_rate 48000 \\\n--join_short_files\n</code></pre> <p>example transfer learning using IIL rave-models:</p> <pre><code>rave train --name 001-my-vocoder-name \\\n--config rave-models/voice_multi_b2048_r48000/config.gin --config transfer \\\n--db_path /path/to/tmp/storage/myravedata \\\n--out_path /path/to/rave/runs \\\n--transfer_ckpt rave-models/voice_multi_b2048_r48000/version_0/checkpoints/last.ckpt \\\n--n_signal 150000\n--gpu 0\n</code></pre> <p>example export using sign normalization (latents correlate with louder/brighter sounds):</p> <pre><code>rave export --run /path/to/rave/runs/001-my-vocoder-name \\\n--streaming --normalize_sign --latent_size ...\n</code></pre>"},{"location":"#tungnaa-preprocessing","title":"Tungna\u00e1 preprocessing","text":"<p>see <code>tungnaa prep --help</code>.</p> <p>To use datasets other than vctk or hifitts, it may be necessary to add an adapter function in <code>prep.py</code>.</p> <p>example: <pre><code>tungnaa prep \\\n--datasets '{kind:\"vctk\", path:\"/path/to/VCTK\"}' \\\n--rave-path /path/to/rave_streaming.ts \\\n--out-path /path/to/tmp/dataset_name\n</code></pre></p>"},{"location":"#training","title":"training","text":"<p>see <code>tungnaa trainer --help</code></p> <p>example: <pre><code>tungnaa trainer --experiment 001-my-tts-name \\\n--model-dir /path/for/checkpoints \\\n--log-dir /path/for/logs \\\n--manifest /path/to/tmp/dataset_name/manifest.json \\\n--rave-model /path/to/rave_streaming.ts \n--lr 3e-4 --lr-text 3e-5 --epoch-size 200 --save-epochs 20 \\\n--device cuda:0 \\\ntrain \n</code></pre></p> <p>resume a stopped training: add <code>--checkpoint /path/to/checkpoint</code></p> <p>transfer learning: add <code>--checkpoint /path/to/checkpoint --resume False</code></p>"},{"location":"#in-text-annotations","title":"in-text annotations","text":"<p><code>--speaker_annotate</code> prepends the speaker id determined during prepreprocessing. With <code>--speaker_dataset</code>, it includes the dataset.</p> <p><code>--csv /path/to/file.csv</code> accepts a <code>jvs_labels_encoder_k7.csv</code>-style CSV file. First column contains the audio filename without extension, second column contains an annotation to be prepended to text.</p> <p>If you were to use all three options, you would get:</p> <p><code>\"csvval:[dataset:speaker] original text\"</code></p>"},{"location":"#developing","title":"Developing","text":"<p>Poetry is used for packaging and dependency management. Conda is used for environments and Python version management, and may be replaced by virtualenv or similar.</p> <ol> <li><code>cd tungnaa</code></li> <li><code>conda create -n tungnaa python=3.12 ffmpeg</code></li> <li><code>conda activate tungnaa</code></li> <li><code>poetry install</code></li> </ol> <p>Note that poetry should not be installed in the project environment, but rather from the system package manager, with pipx, or in a separate environment.</p> <p>To add a dependency, use <code>poetry add</code>, or edit <code>pyproject.toml</code> and then run <code>poetry lock; poetry install</code>.</p> <p>To add a model, use <code>dvc add /path/to/model</code>, then <code>git add /path/to/model.dvc</code>. Tungna\u00e1 models should go in <code>models/tts/</code>, and be accompanied by a <code>model.md</code> file. Vocoders should go in <code>models/vocoders</code>.</p>"},{"location":"#docs","title":"docs","text":"<p>run <code>mkdocs serve</code> to build and view documentation</p> <p>run <code>mkdocs gh-deploy</code> to deploy to github pages</p>"},{"location":"#non-python-dependencies","title":"Non-Python Dependencies","text":"<ul> <li>Poetry (https://python-poetry.org/) for packaging</li> </ul>"},{"location":"#python-dependencies","title":"Python Dependencies","text":"<p>See <code>pyproject.toml</code></p>"},{"location":"reference/tungnaa/__init__/","title":"init","text":""},{"location":"reference/tungnaa/__init__/#tungnaa.CanineEmbeddings","title":"<code>CanineEmbeddings</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/text.py</code> <pre><code>class CanineEmbeddings(nn.Module):\n    def __init__(self, \n            pretrained='google/canine-c', \n            end_tokens=True,\n            bottleneck=False,\n            use_positions=True\n            ):\n        # canine-c: pre-trained with autoregressive character loss\n        # canine-s: pre-trained with subword masking loss\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        self.net = CanineModel.from_pretrained(pretrained).char_embeddings\n        if not use_positions:\n            self.net.position_embedding_type = None\n        self.channels = 768\n        self.bottleneck = False\n        if bottleneck:\n            self.bottleneck = True\n            self.proj = nn.Linear(self.channels, bottleneck)\n            self.channels = bottleneck\n\n    def init(self):\n        \"\"\"random initialization\"\"\"\n        self.net = CanineModel(self.net.config)\n\n    def pad(self, tokens, mask=None):\n        pad = 4 - tokens.shape[1]\n        if pad &gt; 0:\n            tokens = torch.cat((\n                tokens, tokens.new_zeros(tokens.shape[0], pad)\n                ), 1)\n            if mask is not None:\n                mask = torch.cat((\n                    mask, mask.new_zeros(mask.shape[0], pad)\n                ), 1)\n        return tokens, mask\n\n    def forward(self, text_t, mask):\n        # no mixing, mask can be ignored\n        h = self.net(text_t)\n        if self.bottleneck:\n            h = self.proj(h)\n        return h\n\n    def encode(self, text, mask=None):\n        if isinstance(text, str):\n            text, *_ = self.tokenize(text)\n        # n = text.shape[1]\n        # text, mask = self.pad(text, mask)\n        h = self(text, mask)\n        # h = h[:, :n] #unpad\n        return h\n\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.CanineEmbeddings.init","title":"<code>init()</code>","text":"<p>random initialization</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def init(self):\n    \"\"\"random initialization\"\"\"\n    self.net = CanineModel(self.net.config)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.CanineEncoder","title":"<code>CanineEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/text.py</code> <pre><code>class CanineEncoder(nn.Module):\n    def __init__(self, \n            pretrained='google/canine-c', \n            end_tokens=True):\n        # canine-c: pre-trained with autoregressive character loss\n        # canine-s: pre-trained with subword masking loss\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # self.net = CanineModel.from_pretrained(pretrained)\n        config = AutoConfig.from_pretrained(pretrained)\n        self.net = AutoModel.from_config(config)\n\n        self.channels = 768\n\n    def init(self):\n        \"\"\"random initialization\"\"\"\n        self.net = CanineModel(self.net.config)\n\n    def pad(self, tokens, mask=None):\n        pad = 4 - tokens.shape[1]\n        if pad &gt; 0:\n            tokens = torch.cat((\n                tokens, tokens.new_zeros(tokens.shape[0], pad)\n                ), 1)\n            if mask is not None:\n                mask = torch.cat((\n                    mask, mask.new_zeros(mask.shape[0], pad)\n                ), 1)\n        return tokens, mask\n\n    def forward(self, text_t, mask):\n        return self.net(text_t, mask, output_hidden_states=True)\n\n    def encode(self, text, mask=None, layer=-1):\n        if isinstance(text, str):\n            text, _, _ = self.tokenize(text)\n        n = text.shape[1]\n        text, mask = self.pad(text, mask)\n        h = self(text, mask).hidden_states[layer]\n        h = h[:, :n] #unpad\n        return h\n\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.CanineEncoder.init","title":"<code>init()</code>","text":"<p>random initialization</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def init(self):\n    \"\"\"random initialization\"\"\"\n    self.net = CanineModel(self.net.config)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.ConcatSpeakers","title":"<code>ConcatSpeakers</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Iterable-style dataset which combines two JSONDatasets</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>class ConcatSpeakers(torch.utils.data.IterableDataset):\n    \"\"\"Iterable-style dataset which combines two JSONDatasets\"\"\"\n    def __init__(self, dataset, n=2, end_tokens=True, drop_prefix=None):\n        super().__init__()\n        \"\"\"\n        \"\"\"\n        self.dataset = dataset\n        self.n = n\n        self.end_tokens = end_tokens\n        self.drop_prefix = drop_prefix #(prob, prefix)\n        self.iter_count = 0\n\n    def worker_init(self, i):\n        s = torch.initial_seed() ^ self.iter_count\n        random.seed(s)\n        torch.random.manual_seed(s)\n\n    def __iter__(self):\n        self.iter_count += 1\n        return self\n    def __next__(self):\n        # get a random element from each dataset\n        items = [\n            self.dataset[torch.randint(len(self.dataset),size=(1,)).item()]\n            for _ in range(self.n)\n        ]\n        # concat audio, text\n        # print(items)\n\n        text = ''.join(item['plain_text'] for item in items)\n\n        # randomly drop certain prefixes -- use this for dropping 'wildcard' annotations from the first utterance only, while always including them for subsequent utterances\n        if self.drop_prefix is not None:\n            s, p = self.drop_prefix\n            if text.startswith(s) and random.random() &lt; p:\n                text = text[len(s):]\n\n        return {\n            'audio': torch.cat([item['audio'] for item in items], 1),\n            'plain_text': text,\n            'audio_path': ';'.join(item['audio_path'] for item in items)\n        }\n\n    # @property\n    # def dataset(self):\n    #     # compat with split\n    #     return self\n\n    def collate_fn(self):\n        return self.datasets[0].dataset.collate_fn()\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.ConvBNBlock","title":"<code>ConvBNBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutions with Batch Normalization and non-linear activation.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>number of output channels.</p> required <code>kernel_size</code> <code>int</code> <p>convolution kernel size.</p> required <code>activation</code> <code>str</code> <p>'relu', 'tanh', None (linear).</p> <code>None</code> Shapes <ul> <li>input: (B, C_in, T)</li> <li>output: (B, C_out, T)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class ConvBNBlock(nn.Module):\n    r\"\"\"Convolutions with Batch Normalization and non-linear activation.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): convolution kernel size.\n        activation (str): 'relu', 'tanh', None (linear).\n\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_out, T)\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, activation=None):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0\n        padding = (kernel_size - 1) // 2\n        self.convolution1d = nn.Conv1d(\n            in_channels, out_channels, kernel_size, padding=padding)\n        self.batch_normalization = nn.BatchNorm1d(\n            out_channels, momentum=0.1, eps=1e-5)\n        self.dropout = nn.Dropout(p=0.5)\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.Identity()\n\n    def forward(self, x, mask:Optional[Tensor]=None):\n        o = self.batch_normalization(x)\n        o = self.activation(o)\n        o = self.dropout(o)\n        if mask is not None:\n            # o = o.where(mask.bool()[:,None], 0)\n            o = o.where(mask[:,None]&gt;0, 0)\n        o = self.convolution1d(o)\n        return o\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormal","title":"<code>DiagonalNormal</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class DiagonalNormal(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        return size * 2\n\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"diagonal normal negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        mu, logsigma = params.chunk(2, -1)\n        logsigma = logsigma.clip(-7, 5) # TODO: clip range\n        loglik = 0.5 * (\n            ((x - mu) / logsigma.exp()) ** 2\n            + self.log2pi\n        ) + logsigma\n        return loglik.sum(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0):\n        mu, logsigma = params.chunk(2, -1)\n        return mu + temperature*logsigma.exp()*torch.randn_like(logsigma)\n\n    def metrics(self, params:Tensor):\n        mu, logsigma = params.chunk(2, -1)\n        return {\n            'logsigma_min': logsigma.min().detach(),\n            'logsigma_median': logsigma.median().detach(),\n            'logsigma_max': logsigma.max().detach(),\n        }\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormal.forward","title":"<code>forward(x, params)</code>","text":"<p>diagonal normal negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor):\n    \"\"\"diagonal normal negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    mu, logsigma = params.chunk(2, -1)\n    logsigma = logsigma.clip(-7, 5) # TODO: clip range\n    loglik = 0.5 * (\n        ((x - mu) / logsigma.exp()) ** 2\n        + self.log2pi\n    ) + logsigma\n    return loglik.sum(-1)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture","title":"<code>DiagonalNormalMixture</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class DiagonalNormalMixture(nn.Module):\n    def __init__(self, n:int=16):\n        \"\"\"n: number of mixture components\"\"\"\n        super().__init__()\n        self.n = n\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        \"\"\"# of distribution parameters as a function of # latent variables\"\"\"\n        return (2 * size + 1) * self.n\n\n    def get_params(self, params:Tensor):\n        \"\"\"\n        Args:\n            params: Tensor[batch, time, n_params] \n        Returns:\n            mu: Tensor[batch, time, n_latent, self.n]\n            logsigma: Tensor[batch, time, n_latent, self.n]\n            logitpi: Tensor[batch, time, self.n]\n        \"\"\"\n        #means, log stddevs, logit weights\n        locscale = params[...,:-self.n]\n        logitpi = params[...,-self.n:]\n\n        mu, logsigma = locscale.view(\n            params.shape[0], params.shape[1], -1, self.n, 2).unbind(-1)\n        # mu, logsigma = locscale.view(*params.shape[:-1], -1, self.n, 2).unbind(-1)\n        return mu, logsigma, logitpi\n\n    def forward(self, x:Tensor, params:Tensor, mode:str='normal'):\n        \"\"\"mixture of diagonal normals negative log likelihood.\n        should broadcast any number of leading dimensions\n        Args:\n            x: Tensor[batch, time, latent] or [..., latent]\n            params: Tensor[batch, time, n_params] or [..., n_params]\n        Return:\n            negative log likelihood: Tensor[batch, time] or [...]\n        \"\"\"\n        x = x[...,None] # broadcast against mixture component\n        mu, logsigma, logitpi = self.get_params(params)\n\n        if mode == 'fixed':\n            logsigma = torch.zeros_like(logsigma)\n        elif mode == 'posthoc':\n            logsigma = (x - mu).abs().log()\n        elif mode != 'normal':\n            raise ValueError(mode)\n\n        logsigma = logsigma.clip(-7, 5) # TODO: clip range?\n        # cmp_loglik = -0.5 * (\n        #     ((x - mu) / logsigma.exp()) ** 2\n        #     + log2pi\n        # ) - logsigma\n        # logpi = logitpi.log_softmax(-1)\n        # return -(cmp_loglik.sum(-2) + logpi).logsumexp(-1)\n        cmp_loglik = (\n            (x - mu) # materialize huge tensor when calling from sample_n\n            / logsigma.exp()) ** 2 \n\n        # reduce latent dim\n        # reduce before summing these components for memory reasons\n        cmp_loglik = -0.5 * (\n            (cmp_loglik.sum(-2) + self.log2pi*x.shape[-2]) \n            ) - logsigma.sum(-2)\n\n        logpi = logitpi.log_softmax(-1)\n        return -(cmp_loglik + logpi).logsumexp(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0, nsamp:Optional[int]=None):\n        # print(params.shape, temperature, nsamp)\n        if temperature != 1:\n            # return self.sample_n(params, temperature, nsamp=128)\n            if nsamp is not None:\n                return self.sample_n(params, temperature, nsamp=nsamp)\n            else:\n                return self.sample_components(params, temperature)\n        else:\n            mu, logsigma, logitpi = self.get_params(params)\n            # idx = torch.distributions.Categorical(\n                # logits=logitpi).sample()[...,None,None].expand(*mu.shape[:-1],1)\n            idx = categorical_sample_2d(logitpi, 1)[...,None].expand(\n                    mu.shape[0], mu.shape[1], mu.shape[2], 1)\n            mu = mu.gather(-1, idx).squeeze(-1)\n            logsigma = logsigma.gather(-1, idx).squeeze(-1)\n            return mu + logsigma.exp()*torch.randn_like(logsigma)\n            # return mu + temperature*logsigma.exp()*torch.randn_like(logsigma)\n\n    def sample_n(self, params:Tensor, temperature:float=1.0, nsamp:int=128):\n        \"\"\"\n        draw nsamp samples,\n        rerank and sample categorical with temperature\n        Args:\n            params: Tensor[batch, time, n_params]    \n        \"\"\"\n         # sample N \n        mu, logsigma, logitpi = self.get_params(params)\n        # logitpi = logitpi[...,None,:].expand(*logitpi.shape[:-1],nsamp,-1)\n        # logitpi = logitpi[...,None,:].expand(\n            # logitpi.shape[0],logitpi.shape[1],nsamp,-1)\n        # ..., nsamp, self.n\n        # print(f'{logitpi.shape=}')\n        # idx = torch.distributions.Categorical(logits=logitpi).sample()\n        # idx = torch.multinomial(\n            # logitpi.reshape(-1, logitpi.shape[-1]).exp(), nsamp, replacement=True\n            # ).reshape(logitpi.shape[0], logitpi.shape[1], nsamp)\n        idx = categorical_sample_2d(logitpi, nsamp)\n        # ..., nsamp\n        # print(f'{idx.shape=}')\n        # idx = idx[...,None,:].expand(*mu.shape[:-1], -1)\n        idx = idx[...,None,:].expand(mu.shape[0], mu.shape[1], mu.shape[2], -1)\n        # ..., latent, nsamp \n        # print(f'{idx.shape=}')\n\n        # mu is: ..., latent, self.n\n        mu = mu.gather(-1, idx)\n        logsigma = logsigma.gather(-1, idx)\n        # ..., latent, nsamp\n        # print(f'{mu.shape=}')\n\n        samps = (mu + torch.randn_like(mu)*logsigma.exp()).moveaxis(-1, 0)\n        # nsamp,...,latent\n        # print(f'{samps.shape=}')\n\n        # compute nll\n        # here there is a extra first dimension (nsamp)\n        # which broadcasts against the distribution params inside of self.forward,\n        # to compute the nll for each sample\n        nll = self(samps, params).moveaxis(0, -1)\n        # ...,nsamp\n        # print(f'{nll.shape=}')\n        # print(f'{nll=}')\n\n        # sample categorical with temperature\n        # idx = torch.distributions.Categorical(\n        #     logits=-nll/(temperature+1e-5)).sample()\n        # ...\n        idx = categorical_sample_2d(-nll/(temperature+1e-5), 1)\n        # print(f'{idx.shape=}')\n        # ...,1\n\n        # select\n        # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n        # 1,...,latent\n        idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n        # 1,...,latent\n        # print(f'{idx.shape=}')\n        samp = samps.gather(0, idx).squeeze(0)\n        # ...,latent\n        # print(f'{samp.shape=}')\n        # print(f'{samp=}')\n        return samp\n\n    def sample_components(self, params:Tensor, temperature:float=1.0):\n        \"\"\"\n        sample every mixture component with temperature,\n        rerank and sample categorical with temperature.\n        \"\"\"\n        # sample each component with temperature \n        mu, logsigma, _ = self.get_params(params)\n        samps = mu + torch.randn_like(mu)*logsigma.exp()*temperature**0.5\n        # ..., latent, self.n\n        samps = samps.moveaxis(-1, 0)\n        # self.n ...latent\n\n        # compute nll for each sample\n        # here there is an extra first dimension (nsamp)\n        # which broadcasts against the distribution params inside of self.forward\n        # to compute the nll for each sample\n        nll = self(samps, params).moveaxis(0, -1)\n        # ..., self.n\n\n        # sample categorical with temperature\n        if temperature &gt; 1e-5:\n            logits = nll.mul_(-1/temperature**0.5)\n            # idx = torch.distributions.Categorical(logits=logits).sample()\n            idx = categorical_sample_2d(logits, 1)\n        else:\n            idx = nll.argmin(-1)[...,None]\n        # print(f'{idx.shape=}')\n        # ...\n\n        # select\n        # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n        idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n\n        # 1,...,latent\n        # print(f'{idx.shape=}')\n        samp = samps.gather(0, idx).squeeze(0)\n        # ...,latent\n        # print(f'{samp.shape=}')\n        # print(samp.shape)\n        # print(f'{samp=}')\n        return samp\n\n    @torch.jit.ignore\n    def metrics(self, params:Tensor):\n        mu, logsigma, logitpi = self.get_params(params)\n        ent = torch.distributions.Categorical(logits=logitpi).entropy().detach()\n        return {\n            'logsigma_min': logsigma.min().detach(),\n            'logsigma_median': logsigma.median().detach(),\n            'logsigma_max': logsigma.max().detach(),\n            'pi_entropy_min': ent.min(),\n            'pi_entropy_median': ent.median(),\n            'pi_entropy_max': ent.max(),\n        } \n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.__init__","title":"<code>__init__(n=16)</code>","text":"<p>n: number of mixture components</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(self, n:int=16):\n    \"\"\"n: number of mixture components\"\"\"\n    super().__init__()\n    self.n = n\n    self.register_buffer(\n        'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.forward","title":"<code>forward(x, params, mode='normal')</code>","text":"<p>mixture of diagonal normals negative log likelihood. should broadcast any number of leading dimensions Args:     x: Tensor[batch, time, latent] or [..., latent]     params: Tensor[batch, time, n_params] or [..., n_params] Return:     negative log likelihood: Tensor[batch, time] or [...]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor, mode:str='normal'):\n    \"\"\"mixture of diagonal normals negative log likelihood.\n    should broadcast any number of leading dimensions\n    Args:\n        x: Tensor[batch, time, latent] or [..., latent]\n        params: Tensor[batch, time, n_params] or [..., n_params]\n    Return:\n        negative log likelihood: Tensor[batch, time] or [...]\n    \"\"\"\n    x = x[...,None] # broadcast against mixture component\n    mu, logsigma, logitpi = self.get_params(params)\n\n    if mode == 'fixed':\n        logsigma = torch.zeros_like(logsigma)\n    elif mode == 'posthoc':\n        logsigma = (x - mu).abs().log()\n    elif mode != 'normal':\n        raise ValueError(mode)\n\n    logsigma = logsigma.clip(-7, 5) # TODO: clip range?\n    # cmp_loglik = -0.5 * (\n    #     ((x - mu) / logsigma.exp()) ** 2\n    #     + log2pi\n    # ) - logsigma\n    # logpi = logitpi.log_softmax(-1)\n    # return -(cmp_loglik.sum(-2) + logpi).logsumexp(-1)\n    cmp_loglik = (\n        (x - mu) # materialize huge tensor when calling from sample_n\n        / logsigma.exp()) ** 2 \n\n    # reduce latent dim\n    # reduce before summing these components for memory reasons\n    cmp_loglik = -0.5 * (\n        (cmp_loglik.sum(-2) + self.log2pi*x.shape[-2]) \n        ) - logsigma.sum(-2)\n\n    logpi = logitpi.log_softmax(-1)\n    return -(cmp_loglik + logpi).logsumexp(-1)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.get_params","title":"<code>get_params(params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Tensor</code> <p>Tensor[batch, time, n_params] </p> required <p>Returns:     mu: Tensor[batch, time, n_latent, self.n]     logsigma: Tensor[batch, time, n_latent, self.n]     logitpi: Tensor[batch, time, self.n]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def get_params(self, params:Tensor):\n    \"\"\"\n    Args:\n        params: Tensor[batch, time, n_params] \n    Returns:\n        mu: Tensor[batch, time, n_latent, self.n]\n        logsigma: Tensor[batch, time, n_latent, self.n]\n        logitpi: Tensor[batch, time, self.n]\n    \"\"\"\n    #means, log stddevs, logit weights\n    locscale = params[...,:-self.n]\n    logitpi = params[...,-self.n:]\n\n    mu, logsigma = locscale.view(\n        params.shape[0], params.shape[1], -1, self.n, 2).unbind(-1)\n    # mu, logsigma = locscale.view(*params.shape[:-1], -1, self.n, 2).unbind(-1)\n    return mu, logsigma, logitpi\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.n_params","title":"<code>n_params(size)</code>","text":""},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.n_params--of-distribution-parameters-as-a-function-of-latent-variables","title":"of distribution parameters as a function of # latent variables","text":"Source code in <code>src/tungnaa/model.py</code> <pre><code>def n_params(self, size):\n    \"\"\"# of distribution parameters as a function of # latent variables\"\"\"\n    return (2 * size + 1) * self.n\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.sample_components","title":"<code>sample_components(params, temperature=1.0)</code>","text":"<p>sample every mixture component with temperature, rerank and sample categorical with temperature.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def sample_components(self, params:Tensor, temperature:float=1.0):\n    \"\"\"\n    sample every mixture component with temperature,\n    rerank and sample categorical with temperature.\n    \"\"\"\n    # sample each component with temperature \n    mu, logsigma, _ = self.get_params(params)\n    samps = mu + torch.randn_like(mu)*logsigma.exp()*temperature**0.5\n    # ..., latent, self.n\n    samps = samps.moveaxis(-1, 0)\n    # self.n ...latent\n\n    # compute nll for each sample\n    # here there is an extra first dimension (nsamp)\n    # which broadcasts against the distribution params inside of self.forward\n    # to compute the nll for each sample\n    nll = self(samps, params).moveaxis(0, -1)\n    # ..., self.n\n\n    # sample categorical with temperature\n    if temperature &gt; 1e-5:\n        logits = nll.mul_(-1/temperature**0.5)\n        # idx = torch.distributions.Categorical(logits=logits).sample()\n        idx = categorical_sample_2d(logits, 1)\n    else:\n        idx = nll.argmin(-1)[...,None]\n    # print(f'{idx.shape=}')\n    # ...\n\n    # select\n    # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n    idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n\n    # 1,...,latent\n    # print(f'{idx.shape=}')\n    samp = samps.gather(0, idx).squeeze(0)\n    # ...,latent\n    # print(f'{samp.shape=}')\n    # print(samp.shape)\n    # print(f'{samp=}')\n    return samp\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.DiagonalNormalMixture.sample_n","title":"<code>sample_n(params, temperature=1.0, nsamp=128)</code>","text":"<p>draw nsamp samples, rerank and sample categorical with temperature Args:     params: Tensor[batch, time, n_params]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def sample_n(self, params:Tensor, temperature:float=1.0, nsamp:int=128):\n    \"\"\"\n    draw nsamp samples,\n    rerank and sample categorical with temperature\n    Args:\n        params: Tensor[batch, time, n_params]    \n    \"\"\"\n     # sample N \n    mu, logsigma, logitpi = self.get_params(params)\n    # logitpi = logitpi[...,None,:].expand(*logitpi.shape[:-1],nsamp,-1)\n    # logitpi = logitpi[...,None,:].expand(\n        # logitpi.shape[0],logitpi.shape[1],nsamp,-1)\n    # ..., nsamp, self.n\n    # print(f'{logitpi.shape=}')\n    # idx = torch.distributions.Categorical(logits=logitpi).sample()\n    # idx = torch.multinomial(\n        # logitpi.reshape(-1, logitpi.shape[-1]).exp(), nsamp, replacement=True\n        # ).reshape(logitpi.shape[0], logitpi.shape[1], nsamp)\n    idx = categorical_sample_2d(logitpi, nsamp)\n    # ..., nsamp\n    # print(f'{idx.shape=}')\n    # idx = idx[...,None,:].expand(*mu.shape[:-1], -1)\n    idx = idx[...,None,:].expand(mu.shape[0], mu.shape[1], mu.shape[2], -1)\n    # ..., latent, nsamp \n    # print(f'{idx.shape=}')\n\n    # mu is: ..., latent, self.n\n    mu = mu.gather(-1, idx)\n    logsigma = logsigma.gather(-1, idx)\n    # ..., latent, nsamp\n    # print(f'{mu.shape=}')\n\n    samps = (mu + torch.randn_like(mu)*logsigma.exp()).moveaxis(-1, 0)\n    # nsamp,...,latent\n    # print(f'{samps.shape=}')\n\n    # compute nll\n    # here there is a extra first dimension (nsamp)\n    # which broadcasts against the distribution params inside of self.forward,\n    # to compute the nll for each sample\n    nll = self(samps, params).moveaxis(0, -1)\n    # ...,nsamp\n    # print(f'{nll.shape=}')\n    # print(f'{nll=}')\n\n    # sample categorical with temperature\n    # idx = torch.distributions.Categorical(\n    #     logits=-nll/(temperature+1e-5)).sample()\n    # ...\n    idx = categorical_sample_2d(-nll/(temperature+1e-5), 1)\n    # print(f'{idx.shape=}')\n    # ...,1\n\n    # select\n    # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n    # 1,...,latent\n    idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n    # 1,...,latent\n    # print(f'{idx.shape=}')\n    samp = samps.gather(0, idx).squeeze(0)\n    # ...,latent\n    # print(f'{samp.shape=}')\n    # print(f'{samp=}')\n    return samp\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.GED","title":"<code>GED</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generalized energy distance (pseudo) likelihood</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class GED(nn.Module):\n    \"\"\"Generalized energy distance (pseudo) likelihood\"\"\"\n    def __init__(self, \n            latent_size,\n            hidden_size=256,\n            hidden_layers=3,\n            dropout=None,\n            unfold=None,\n            multiply_params=False,\n            project_params=None,\n            glu=False,\n            ):\n        super().__init__()\n        self.latent_size = latent_size\n        self.hidden_size = hidden_size\n        self.unfold = unfold\n        self.multiply_params = multiply_params\n        self.project_params = project_params\n\n        layers = []\n        norm = nn.utils.parametrizations.weight_norm \n        if glu:\n            act = nn.GLU\n            for _ in range(hidden_layers):\n                block = [norm(nn.Linear(hidden_size, 2*hidden_size))]\n                block.append(act())\n                if dropout:\n                    block.append(nn.Dropout(dropout))\n                layers.append(Residual(*block))\n            layers.append(norm(nn.Linear(hidden_size, latent_size)))\n        else:\n            act = nn.GELU\n            for _ in range(hidden_layers):\n                block = [nn.Dropout(dropout)] if dropout else []\n                block.append(act())\n                block.append(norm(nn.Linear(hidden_size, hidden_size)))\n                layers.append(Residual(*block))\n            layers.extend([\n                act(),\n                norm(nn.Linear(hidden_size, latent_size))\n                ])\n\n        self.net = nn.Sequential(*layers)\n\n    def n_params(self, size):\n        assert size == self.latent_size\n        if self.project_params:\n            p = self.project_params\n            return self.hidden_size + p*(p-1)\n        elif self.multiply_params:\n            return self.hidden_size\n        else:\n            return self.hidden_size//2\n\n    @torch.jit.ignore\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"compute generalized energy distance\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]\n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        # bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n        # params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n        # x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n        params = torch.cat((params, params), 0)\n        y = self.net(self.add_noise(params))\n        if self.unfold:\n            zeros = y.new_zeros(*y.shape[:-2],self.unfold-1,y.shape[-1])\n            y = torch.cat((zeros, y), -2)\n            y = y.unfold(dimension=-2, size=self.unfold, step=1)\n            zeros = x.new_zeros(*x.shape[:-2],self.unfold-1,x.shape[-1])\n            x = torch.cat((zeros, x), -2)\n            x = x.unfold(dimension=-2, size=self.unfold, step=1)\n            dim = (-1,-2)\n        else:\n            dim = -1\n        y1, y2 = y.chunk(2, 0)\n        return (\n            torch.linalg.vector_norm(x-y1, dim=dim)\n            + torch.linalg.vector_norm(x-y2, dim=dim)\n            - torch.linalg.vector_norm(y2-y1, dim=dim)\n            )\n\n    # @torch.jit.ignore\n    def sample(self, params, temperature:float=1.0):\n        return self.net(self.add_noise(params, temperature))\n\n    def add_noise(self, params, temperature:float=1):\n        if self.project_params is not None:\n            p = self.project_params\n            w, params = params.split((p**2),-1)\n            noise = torch.randn(\n                *params.shape[:2],1,p, device=params.device, dtype=params.dtype)\n            w = w.reshape(*w.shape[:2],p,p)\n            noise = (noise @ w).squeeze(-2)\n        else:\n            if self.multiply_params:\n                params, w = params.chunk(2,-1)\n                noise = w*torch.randn_like(params)*temperature\n            else:\n                noise = torch.randn_like(params)*temperature\n\n        return torch.cat((params, noise), -1)\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.GED.forward","title":"<code>forward(x, params)</code>","text":"<p>compute generalized energy distance Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params] Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, x:Tensor, params:Tensor):\n    \"\"\"compute generalized energy distance\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]\n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    # bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n    # params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n    # x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n    params = torch.cat((params, params), 0)\n    y = self.net(self.add_noise(params))\n    if self.unfold:\n        zeros = y.new_zeros(*y.shape[:-2],self.unfold-1,y.shape[-1])\n        y = torch.cat((zeros, y), -2)\n        y = y.unfold(dimension=-2, size=self.unfold, step=1)\n        zeros = x.new_zeros(*x.shape[:-2],self.unfold-1,x.shape[-1])\n        x = torch.cat((zeros, x), -2)\n        x = x.unfold(dimension=-2, size=self.unfold, step=1)\n        dim = (-1,-2)\n    else:\n        dim = -1\n    y1, y2 = y.chunk(2, 0)\n    return (\n        torch.linalg.vector_norm(x-y1, dim=dim)\n        + torch.linalg.vector_norm(x-y2, dim=dim)\n        - torch.linalg.vector_norm(y2-y1, dim=dim)\n        )\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.GaussianAttention","title":"<code>GaussianAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>More restrictive version of dynamic convolutional attention. the convolution is always a difference of gaussians (diffusion / unsharp mask) with a nonnegative shift.</p> <p>Parameters:</p> Name Type Description Default <code>query_dim</code> <code>int</code> <p>number of channels in the query tensor.</p> required <code>embedding_dim</code> <code>int</code> <p>number of channels in the value tensor.</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>class GaussianAttention(nn.Module):\n    \"\"\"\n    More restrictive version of dynamic convolutional attention.\n    the convolution is always a difference of gaussians (diffusion / unsharp mask)\n    with a nonnegative shift.\n\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the value tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim,\n        filter_size:int=21,\n        min_sigma:float=0.2,\n        prior_tokens_per_frame:float=1.0,\n        wn=False,\n    ):\n        super().__init__()\n        self._mask_value = 1e-8\n\n        norm = nn.utils.parametrizations.weight_norm if wn else lambda x:x\n        self.proj = norm(nn.Linear(query_dim, 4))\n        self.filter_size = filter_size\n        self.min_sigma = min_sigma\n        self.prior_tokens_per_frame = prior_tokens_per_frame\n\n        self.register_buffer(\n            'k', torch.arange(filter_size)[:, None, None], persistent=False)\n\n    def align(self, \n            query, attention_weights, \n            inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n        \"\"\"\n        Args:\n            query: [B, D_attn_rnn]\n            attention_weights: [B, T_text]\n            inputs: [B, T_text, D_text]\n            mask: [B, T_text]\n        Returns:\n            attention_weights: [B, T_text]\n        \"\"\"\n        B = query.size(0)\n\n        mu, alpha, sigma = F.sigmoid(self.proj(query)).split((1,1,2),dim=1)\n        # mu, alpha, sigma = F.softplus(self.proj(query)).split((1,1,2),dim=1)\n        # mu = self.filter_size//2 - mu * self.prior_tokens_per_frame# shift\n        mu = self.filter_size//2 - mu * (2 * self.prior_tokens_per_frame)# shift\n        # alpha, beta = 1+alpha, -alpha # diffuse weight, unsharp weight\n        sigma = self.min_sigma + sigma.cumsum(-1) # second more dispersed than first\n\n        # NOTE: may be able to compute k using torch.special.ndtr + diff?\n\n        k = self.k\n        # k = torch.arange(\n        #     self.filter_size, \n        #     device=query.device, dtype=query.dtype\n        #     )[:, None, None] # [K, 1, 1]\n\n        # g = (-(k-mu)**2/(2*sigma**2)).exp()/sigma\n        g = (-((k-mu)/(2*sigma))**2).exp()/sigma\n\n        alpha = alpha.T\n        k = (1+alpha)*g[...,0] - alpha*g[...,1] # [K, B]\n        k = k/k.sum(0) # normalize\n        k = k.T # [B, K]\n\n        attention_weights = F.conv1d(\n            attention_weights[None], k[:,None], \n            groups=B, padding=self.filter_size//2\n        ).squeeze(0)\n\n        if mask is not None:\n            attention_weights = attention_weights.masked_fill(\n                ~mask, self._mask_value)\n\n        attention_weights = attention_weights.clip(self._mask_value)\n\n        attention_weights = (\n            attention_weights / attention_weights.sum(-1, keepdim=True))\n\n        return attention_weights\n\n    def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n        \"\"\"this is split out to implement attention painting\n        \"\"\"\n        # do weights really need to be masked here if it's post-softmax anyway?\n        # is it enough that the inputs are zero padded?\n        # print(attention_weights.shape, inputs.shape)\n        # apply masking\n        # if mask is not None:\n        #     if torch.is_grad_enabled():\n        #         attention_weights = attention_weights.masked_fill(\n        #             ~mask, self._mask_value)\n        #     else:\n        #         attention_weights.masked_fill_(~mask, self._mask_value)\n        context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n        return context\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        attention_weights = inputs.new_zeros(B, T)\n        attention_weights[:, 0] = 1.0\n        return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.GaussianAttention.align","title":"<code>align(query, attention_weights, inputs=None, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <p>[B, D_attn_rnn]</p> required <code>attention_weights</code> <p>[B, T_text]</p> required <code>inputs</code> <code>Optional[Tensor]</code> <p>[B, T_text, D_text]</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>[B, T_text]</p> <code>None</code> <p>Returns:     attention_weights: [B, T_text]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def align(self, \n        query, attention_weights, \n        inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n    \"\"\"\n    Args:\n        query: [B, D_attn_rnn]\n        attention_weights: [B, T_text]\n        inputs: [B, T_text, D_text]\n        mask: [B, T_text]\n    Returns:\n        attention_weights: [B, T_text]\n    \"\"\"\n    B = query.size(0)\n\n    mu, alpha, sigma = F.sigmoid(self.proj(query)).split((1,1,2),dim=1)\n    # mu, alpha, sigma = F.softplus(self.proj(query)).split((1,1,2),dim=1)\n    # mu = self.filter_size//2 - mu * self.prior_tokens_per_frame# shift\n    mu = self.filter_size//2 - mu * (2 * self.prior_tokens_per_frame)# shift\n    # alpha, beta = 1+alpha, -alpha # diffuse weight, unsharp weight\n    sigma = self.min_sigma + sigma.cumsum(-1) # second more dispersed than first\n\n    # NOTE: may be able to compute k using torch.special.ndtr + diff?\n\n    k = self.k\n    # k = torch.arange(\n    #     self.filter_size, \n    #     device=query.device, dtype=query.dtype\n    #     )[:, None, None] # [K, 1, 1]\n\n    # g = (-(k-mu)**2/(2*sigma**2)).exp()/sigma\n    g = (-((k-mu)/(2*sigma))**2).exp()/sigma\n\n    alpha = alpha.T\n    k = (1+alpha)*g[...,0] - alpha*g[...,1] # [K, B]\n    k = k/k.sum(0) # normalize\n    k = k.T # [B, K]\n\n    attention_weights = F.conv1d(\n        attention_weights[None], k[:,None], \n        groups=B, padding=self.filter_size//2\n    ).squeeze(0)\n\n    if mask is not None:\n        attention_weights = attention_weights.masked_fill(\n            ~mask, self._mask_value)\n\n    attention_weights = attention_weights.clip(self._mask_value)\n\n    attention_weights = (\n        attention_weights / attention_weights.sum(-1, keepdim=True))\n\n    return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.GaussianAttention.apply","title":"<code>apply(attention_weights, inputs, mask=None)</code>","text":"<p>this is split out to implement attention painting</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n    \"\"\"this is split out to implement attention painting\n    \"\"\"\n    # do weights really need to be masked here if it's post-softmax anyway?\n    # is it enough that the inputs are zero padded?\n    # print(attention_weights.shape, inputs.shape)\n    # apply masking\n    # if mask is not None:\n    #     if torch.is_grad_enabled():\n    #         attention_weights = attention_weights.masked_fill(\n    #             ~mask, self._mask_value)\n    #     else:\n    #         attention_weights.masked_fill_(~mask, self._mask_value)\n    context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n    return context\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.JSONDataset","title":"<code>JSONDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/tungnaa/util.py</code> <pre><code>class JSONDataset(torch.utils.data.Dataset):\n    \"\"\"\n    \"\"\"\n    def __init__(self, \n            manifest_file, csv_file, max_tokens, max_frames, \n            speaker_annotate=False, speaker_dataset=False, strip_quotes=False,\n            rave=None, text_encoder=None, return_path=False, rand_text_subs=None, style_annotate=0, replace_runs=False):\n        super().__init__()\n        \"\"\"\n        manifest_file: hifi-tts style json manifest file (see prep.py)\n        csv_file: csv file where first column is audio filename, \n            other columns contain additional metadata\n\n        rand_text_subs: list of (originals, replacement) token pairs\n        replace_runs: if True, always replace runs of the same token together\n        \"\"\"\n        self.root_dir = Path(manifest_file).parent\n        with open(manifest_file) as f:\n            self.index = [json.loads(line) for line in f]\n\n        self.csv_data = {}\n        if csv_file is not None:\n            with open(csv_file) as f:\n                for line in f:\n                    k,*vs = line.strip().split(',')\n                    self.csv_data[k] = vs\n            # print(self.csv_data)###DEBUG\n        self.max_frames = max_frames\n        self.max_tokens = max_tokens\n        self.speaker_annotate = speaker_annotate\n        self.speaker_dataset = speaker_dataset\n        self.style_annotate = style_annotate\n        self.strip_quotes = strip_quotes\n        self.rave = rave # needed for pitch models\n        self.text_encoder = text_encoder\n        self.return_path = return_path\n\n        if rand_text_subs is not None:\n            subs = defaultdict(list)\n            for ogs,t in rand_text_subs:\n                for c in ogs:\n                    subs[c].append(t)\n            self.rand_text_subs = dict(subs)\n        else:\n            self.rand_text_subs = None\n        self.replace_runs = replace_runs\n\n    def worker_init(self, i):\n        s = torch.initial_seed()\n        random.seed(s)\n\n    def __getitem__(self, i):\n        item = self.index[i]\n        if self.return_path:\n            return item['audio_path']\n\n        # pre-computed data augmentation\n        # if not exists, choose random\n        # TODO\n\n        if 'audio_std_path' in item:\n            # data augmentation if RAVE prep included posterior stddevs\n            audio, stddev = torch.load(choose_path(\n                self.root_dir / item['audio_std_path']))\n            stddev.mul_(torch.randn_like(stddev))\n            audio.add_(stddev)\n        else:\n            print(f'{item=}')\n            audio = torch.load(choose_path(\n                self.root_dir / item['audio_feature_path']))\n\n        if 'audio_pitch_path' in item:\n            pitch_probs = torch.load(choose_path(\n                self.root_dir / item['audio_pitch_path']))\n            pitch_bins = torch.distributions.Categorical(pitch_probs).sample()\n            pitch_hz = self.rave.pitch_encoder.bins_to_frequency(pitch_bins)\n            # pitch becomes first latent.\n            # print(pitch_hz.shape, audio.shape)\n            # slice off the last frame of audio if needed\n            # TODO: look into this discrepancy when input sizes aren't round...\n            # print(pitch_hz.shape, audio.shape)\n            audio = torch.cat((pitch_hz, audio[...,:pitch_hz.shape[-1]]), 1)\n\n        text = item['text']\n\n        if self.strip_quotes:\n            text = re.sub(_quotes_re, '', text)\n\n        if self.rand_text_subs is not None:\n            rate = random.random()**2\n            if self.replace_runs:\n                runs = []\n                for run in get_char_runs(text):\n                    c, n = run[0], len(run)\n                    if c in self.rand_text_subs and random.random() &lt; rate:\n                        toks = self.rand_text_subs[c]\n                        runs.append(random.choice(toks) * n)\n                    else:\n                        runs.append(run)\n                text = ''.join(runs)\n            else:\n                text = list(text)\n                for i in range(len(text)):\n                    if text[i] in self.rand_text_subs and random.random() &lt; rate:\n                        toks = self.rand_text_subs[text[i]]\n                        text[i] = random.choice(toks)\n                text = ''.join(text)\n            # print(text)\n\n        if self.speaker_annotate:\n            speaker = item['speaker']\n            if not self.speaker_dataset:\n                speaker = speaker.split(':')[1]\n            text = f'[{speaker}] {text}'\n\n        if self.style_annotate:\n            if random.random() &lt; self.style_annotate:\n                style = item['style']\n                # capitalize first letter of style\n                if style=='neutral':\n                    style = '|'\n                elif style=='aggressive':\n                    style = '+'\n                elif style=='happy':\n                    style = '^'\n                elif style=='worried':\n                    style = '&amp;'\n                else:\n                    print(f'warning: unrecognized style \"{style}\"')\n                    style = ''\n            else:\n                style = '%'\n            text = style + text\n\n\n        if len(self.csv_data):\n            k = item['audio_path']\n            # k = Path(k).name.replace('.flac', '.wav')\n            k = Path(k).name.split('.')[0]\n            # assert k in self.csv_data, k\n            if k not in self.csv_data:\n                print(f'WARNING: {k} not found in csv')\n            else:\n                text = f'{self.csv_data[k][0]}:{text}'\n            # print(text) ###DEBUG\n\n        r = {\n            'index': i,\n            'plain_text': text,\n            # batch x time x channel\n            'audio': audio.transpose(1,2),\n            'audio_path': item['audio_path'],\n        }\n        if 'text_feature_path' in item:\n            if self.speaker_annotate:\n                raise NotImplementedError\n            # batch x time x channel\n            r['emb_text'] = torch.load(\n                self.root_dir / item['text_feature_path'])\n        return r\n\n    def __len__(self):\n        return len(self.index)\n\n    def collate_fn(self):\n        \"\"\"\n        returns a function suitable for the collate_fn argument of a torch DataLoader\n        \"\"\"\n        def collate(batch):\n            \"\"\"\n            Args:\n                batch: list of dicts which are single data points\n\n            Return:\n                dict of batch tensors\n            \"\"\"\n            n_text = min(self.max_tokens, max(len(b['plain_text']) for b in batch))\n            n_audio = min(self.max_frames, max(b['audio'].shape[1] for b in batch))\n            text_idxs = []\n            text_embs = []\n            audio_ts = []\n            text_masks = []\n            audio_masks = []\n            # index_ts = []\n            for b in batch:\n                text_idx, *_ = self.text_encoder.tokenize(b['plain_text'])\n                # text_idx = torch.tensor(\n                    # [[ord(char) for char in b['plain_text']]])\n                if 'emb_text' in b:\n                    text_emb = b['emb_text']\n                    assert text_idx.shape[1] == text_emb.shape[1]\n                    text_embs.append(pad1(text_emb, n_text))\n                text_masks.append(torch.arange(n_text) &lt; text_idx.shape[1])\n                text_idxs.append(pad1(text_idx, n_text))\n                audio = b['audio']\n                audio_masks.append(torch.arange(n_audio) &lt; audio.shape[1])\n                audio_ts.append(pad1(audio, n_audio))\n                # index_ts.append(torch.LongTensor((b['index'],)))\n            return {\n                # 'index': torch.cat(index_ts, 0),\n                'plain_text': [b['plain_text'] for b in batch],\n                'audio_path': [b['audio_path'] for b in batch],\n                'text': torch.cat(text_idxs, 0),\n                'text_emb': torch.cat(text_embs, 0) if len(text_embs) else None,\n                'audio': torch.cat(audio_ts, 0),\n                'text_mask': torch.stack(text_masks, 0),\n                'audio_mask': torch.stack(audio_masks, 0),\n            }\n\n        return collate\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.JSONDataset.collate_fn","title":"<code>collate_fn()</code>","text":"<p>returns a function suitable for the collate_fn argument of a torch DataLoader</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def collate_fn(self):\n    \"\"\"\n    returns a function suitable for the collate_fn argument of a torch DataLoader\n    \"\"\"\n    def collate(batch):\n        \"\"\"\n        Args:\n            batch: list of dicts which are single data points\n\n        Return:\n            dict of batch tensors\n        \"\"\"\n        n_text = min(self.max_tokens, max(len(b['plain_text']) for b in batch))\n        n_audio = min(self.max_frames, max(b['audio'].shape[1] for b in batch))\n        text_idxs = []\n        text_embs = []\n        audio_ts = []\n        text_masks = []\n        audio_masks = []\n        # index_ts = []\n        for b in batch:\n            text_idx, *_ = self.text_encoder.tokenize(b['plain_text'])\n            # text_idx = torch.tensor(\n                # [[ord(char) for char in b['plain_text']]])\n            if 'emb_text' in b:\n                text_emb = b['emb_text']\n                assert text_idx.shape[1] == text_emb.shape[1]\n                text_embs.append(pad1(text_emb, n_text))\n            text_masks.append(torch.arange(n_text) &lt; text_idx.shape[1])\n            text_idxs.append(pad1(text_idx, n_text))\n            audio = b['audio']\n            audio_masks.append(torch.arange(n_audio) &lt; audio.shape[1])\n            audio_ts.append(pad1(audio, n_audio))\n            # index_ts.append(torch.LongTensor((b['index'],)))\n        return {\n            # 'index': torch.cat(index_ts, 0),\n            'plain_text': [b['plain_text'] for b in batch],\n            'audio_path': [b['audio_path'] for b in batch],\n            'text': torch.cat(text_idxs, 0),\n            'text_emb': torch.cat(text_embs, 0) if len(text_embs) else None,\n            'audio': torch.cat(audio_ts, 0),\n            'text_mask': torch.stack(text_masks, 0),\n            'audio_mask': torch.stack(audio_masks, 0),\n        }\n\n    return collate\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.MLPDecoder","title":"<code>MLPDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class MLPDecoder(nn.Module):\n    def __init__(self, in_size, size, num_layers=4, \n            bias=True, batch_first=True, \n            dropout=0.1):\n        super().__init__()\n        assert batch_first\n\n        norm = nn.utils.parametrizations.weight_norm \n        net = [norm(nn.Linear(in_size, size, bias))]\n        for _ in range(num_layers):\n            block = []\n            if dropout:\n                block.append(nn.Dropout(dropout))\n            block.append(nn.LeakyReLU(0.2))\n            block.append(norm(nn.Linear(size, size, bias)))\n            net.append(Residual(*block))\n\n        net.append(nn.LeakyReLU(0.2))\n        self.net = nn.Sequential(*net)\n        # self.net = torch.jit.script(nn.Sequential(*net))\n\n    def forward(self, x, states):\n        \"\"\"pseudo-RNN forward\"\"\"\n        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n            h = x.data\n        else:\n            h = x\n        h = self.net(h)\n        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n            x = torch.nn.utils.rnn.PackedSequence(h, *x[1:])\n        else:\n            x = h\n        return x, states\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.MLPDecoder.forward","title":"<code>forward(x, states)</code>","text":"<p>pseudo-RNN forward</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x, states):\n    \"\"\"pseudo-RNN forward\"\"\"\n    if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n        h = x.data\n    else:\n        h = x\n    h = self.net(h)\n    if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n        x = torch.nn.utils.rnn.PackedSequence(h, *x[1:])\n    else:\n        x = h\n    return x, states\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.MonotonicDynamicConvolutionAttention","title":"<code>MonotonicDynamicConvolutionAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Dynamic convolution attention from https://arxiv.org/pdf/1910.10288.pdf</p> <p>This is an altered version where the static filter is replaced by the bias  of the linear layer leading to the dynamic filter</p> <p>original docstring follows:</p> <p>query -&gt; linear -&gt; tanh -&gt; linear -&gt;|                                     |                                            mask values                                     v                                              |    |            atten_w(t-1) -|-&gt; conv1d_dynamic -&gt; linear -|-&gt; tanh -&gt; + -&gt; softmax -&gt; * -&gt; * -&gt; context                          |-&gt; conv1d_static  -&gt; linear -|           |                          |-&gt; conv1d_prior   -&gt; log ----------------| query: attention rnn output. Note:     Dynamic convolution attention is an alternation of the location senstive attention with dynamically computed convolution filters from the previous attention scores and a set of constraints to keep the attention alignment diagonal.     DCA is sensitive to mixed precision training and might cause instable training. Args:     query_dim (int): number of channels in the query tensor.     embedding_dim (int): number of channels in the value tensor.     static_filter_dim (int): number of channels in the convolution layer computing the static filters.     static_kernel_size (int): kernel size for the convolution layer computing the static filters.     dynamic_filter_dim (int): number of channels in the convolution layer computing the dynamic filters.     dynamic_kernel_size (int): kernel size for the convolution layer computing the dynamic filters.     prior_filter_len (int, optional): [description]. Defaults to 11 from the paper.     alpha (float, optional): [description]. Defaults to 0.1 from the paper.     beta (float, optional): [description]. Defaults to 0.9 from the paper.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class MonotonicDynamicConvolutionAttention(nn.Module):\n    \"\"\"Dynamic convolution attention from\n    https://arxiv.org/pdf/1910.10288.pdf\n\n    This is an altered version where the static filter is replaced by the bias \n    of the linear layer leading to the dynamic filter\n\n    original docstring follows:\n\n    query -&gt; linear -&gt; tanh -&gt; linear -&gt;|\n                                        |                                            mask values\n                                        v                                              |    |\n               atten_w(t-1) -|-&gt; conv1d_dynamic -&gt; linear -|-&gt; tanh -&gt; + -&gt; softmax -&gt; * -&gt; * -&gt; context\n                             |-&gt; conv1d_static  -&gt; linear -|           |\n                             |-&gt; conv1d_prior   -&gt; log ----------------|\n    query: attention rnn output.\n    Note:\n        Dynamic convolution attention is an alternation of the location senstive attention with\n    dynamically computed convolution filters from the previous attention scores and a set of\n    constraints to keep the attention alignment diagonal.\n        DCA is sensitive to mixed precision training and might cause instable training.\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the value tensor.\n        static_filter_dim (int): number of channels in the convolution layer computing the static filters.\n        static_kernel_size (int): kernel size for the convolution layer computing the static filters.\n        dynamic_filter_dim (int): number of channels in the convolution layer computing the dynamic filters.\n        dynamic_kernel_size (int): kernel size for the convolution layer computing the dynamic filters.\n        prior_filter_len (int, optional): [description]. Defaults to 11 from the paper.\n        alpha (float, optional): [description]. Defaults to 0.1 from the paper.\n        beta (float, optional): [description]. Defaults to 0.9 from the paper.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim,\n        attention_dim,\n        static_filter_dim=8, # unused\n        static_kernel_size=21, # unused\n        dynamic_filter_dim=8,\n        dynamic_kernel_size=21,\n        prior_filter_len=11,\n        alpha=0.1,\n        beta=0.9,\n    ):\n        super().__init__()\n        self._mask_value = 1e-8\n        self.dynamic_filter_dim = dynamic_filter_dim\n        self.dynamic_kernel_size = dynamic_kernel_size\n        self.prior_filter_len = prior_filter_len\n        # setup key and query layers\n        dynamic_weight_dim = dynamic_filter_dim * dynamic_kernel_size\n        # self.filter_mlp = torch.jit.script(nn.Sequential(\n        self.filter_mlp = (nn.Sequential(\n            nn.Linear(query_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, dynamic_weight_dim)#, bias=False)\n        ))\n\n        # self.post_mlp = torch.jit.script(nn.Sequential(\n        self.post_mlp = (nn.Sequential(\n            nn.Linear(dynamic_filter_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1, bias=False)\n        ))\n\n        self.register_buffer(\n            \"prior\", prior_coefs(prior_filter_len, alpha, beta), \n            persistent=False)\n\n    def align(self, \n            query, attention_weights, \n            inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n        \"\"\"\n        Args:\n            query: [B, D_attn_rnn]\n            attention_weights: [B, T_text]\n            inputs: [B, T_text, D_text]\n            mask: [B, T_text]\n        Returns:\n            attention_weights: [B, T_text]\n        \"\"\"\n        B = query.shape[0]\n\n        prior_filter = do_prior_filter(\n            attention_weights, self.prior, self.prior_filter_len-1)\n\n        G = self.filter_mlp(query)\n        # compute dynamic filters\n        pad = (self.dynamic_kernel_size - 1) // 2\n        dynamic_filter = F.conv1d(\n            attention_weights[None],\n            G.view(-1, 1, self.dynamic_kernel_size),\n            padding=pad,\n            groups=B,\n        )\n        dynamic_filter = dynamic_filter.view(\n            B, self.dynamic_filter_dim, -1).transpose(1, 2)\n\n        attention_weights = addsoftmax(\n            self.post_mlp(dynamic_filter).squeeze(-1), prior_filter)\n\n        return attention_weights\n\n    def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n        \"\"\"this is split out to implement attention painting\n\n        do weights really need to be masked here if it's post-softmax anyway?\n        is it enough that the inputs are zero padded?\n        \"\"\"\n        # print(attention_weights.shape, inputs.shape)\n        # apply masking\n        if mask is not None:\n            if torch.is_grad_enabled():\n                attention_weights = attention_weights.masked_fill(\n                    ~mask, self._mask_value)\n            else:\n                attention_weights.masked_fill_(~mask, self._mask_value)\n        context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n        return context\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        attention_weights = inputs.new_zeros(B, T)\n        attention_weights[:, 0] = 1.0\n        return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.MonotonicDynamicConvolutionAttention.align","title":"<code>align(query, attention_weights, inputs=None, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <p>[B, D_attn_rnn]</p> required <code>attention_weights</code> <p>[B, T_text]</p> required <code>inputs</code> <code>Optional[Tensor]</code> <p>[B, T_text, D_text]</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>[B, T_text]</p> <code>None</code> <p>Returns:     attention_weights: [B, T_text]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def align(self, \n        query, attention_weights, \n        inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n    \"\"\"\n    Args:\n        query: [B, D_attn_rnn]\n        attention_weights: [B, T_text]\n        inputs: [B, T_text, D_text]\n        mask: [B, T_text]\n    Returns:\n        attention_weights: [B, T_text]\n    \"\"\"\n    B = query.shape[0]\n\n    prior_filter = do_prior_filter(\n        attention_weights, self.prior, self.prior_filter_len-1)\n\n    G = self.filter_mlp(query)\n    # compute dynamic filters\n    pad = (self.dynamic_kernel_size - 1) // 2\n    dynamic_filter = F.conv1d(\n        attention_weights[None],\n        G.view(-1, 1, self.dynamic_kernel_size),\n        padding=pad,\n        groups=B,\n    )\n    dynamic_filter = dynamic_filter.view(\n        B, self.dynamic_filter_dim, -1).transpose(1, 2)\n\n    attention_weights = addsoftmax(\n        self.post_mlp(dynamic_filter).squeeze(-1), prior_filter)\n\n    return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.MonotonicDynamicConvolutionAttention.apply","title":"<code>apply(attention_weights, inputs, mask=None)</code>","text":"<p>this is split out to implement attention painting</p> <p>do weights really need to be masked here if it's post-softmax anyway? is it enough that the inputs are zero padded?</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n    \"\"\"this is split out to implement attention painting\n\n    do weights really need to be masked here if it's post-softmax anyway?\n    is it enough that the inputs are zero padded?\n    \"\"\"\n    # print(attention_weights.shape, inputs.shape)\n    # apply masking\n    if mask is not None:\n        if torch.is_grad_enabled():\n            attention_weights = attention_weights.masked_fill(\n                ~mask, self._mask_value)\n        else:\n            attention_weights.masked_fill_(~mask, self._mask_value)\n    context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n    return context\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.NSF","title":"<code>NSF</code>","text":"<p>               Bases: <code>Module</code></p> <p>Neural Spline Flow likelihood</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class NSF(nn.Module):\n    \"\"\"Neural Spline Flow likelihood\"\"\"\n    def __init__(self, \n            latent_size,\n            context_size=256,\n            hidden_size=256,\n            hidden_layers=2,\n            blocks=8,\n            bins=8,\n            dropout=None,\n            ):\n        super().__init__()\n        self.context_size = context_size\n        self.latent_size = latent_size\n\n        flows = []\n        for _ in range(blocks):\n            flows.append(nf.flows.CoupledRationalQuadraticSpline(\n                latent_size, hidden_layers, hidden_size, context_size,\n                num_bins=bins, dropout_probability=dropout))\n            flows.append(nf.flows.LULinearPermute(latent_size))\n        # base distribution\n        # q0 = nf.distributions.DiagGaussian(latent_size, trainable=False)\n        q0 = ConditionalDiagGaussian(\n            latent_size, context_encoder=nn.Linear(context_size, 2*latent_size))\n        # flow model\n        self.net = nf.ConditionalNormalizingFlow(q0=q0, flows=flows)\n        # self.net.q0.forward = torch.jit.ignore(self.net.q0.forward)\n\n    def n_params(self, size):\n        assert size == self.latent_size\n        return self.context_size\n\n    @torch.jit.ignore\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n        params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n        x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n        loglik = self.net.log_prob(x, context=params)\n        return -loglik.reshape(*bs)\n\n    @torch.jit.ignore\n    def sample(self, params:Tensor, temperature:float=1.0):\n        bs = params.shape[:-1]\n        params = params.reshape(-1, params.shape[-1])\n        # if temperature!=1:\n        #     print('warning: temperature not implemented in NSF')\n        # samp, logprob = self.net.sample(params.shape[0], context=params)\n        mu, logsigma = self.net.q0.context_encoder(params).chunk(2,-1)\n        base_samp = mu + torch.randn_like(mu)*logsigma.exp()*temperature\n        samp = self.net(base_samp, context=params)\n        samp = samp.reshape(*bs, -1)\n        return samp\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.NSF.forward","title":"<code>forward(x, params)</code>","text":"<p>negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, x:Tensor, params:Tensor):\n    \"\"\"negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n    params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n    x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n    loglik = self.net.log_prob(x, context=params)\n    return -loglik.reshape(*bs)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.Prenet","title":"<code>Prenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tacotron specific Prenet with an optional Batch Normalization. Note:     Prenet with BN improves the model performance significantly especially if it is enabled after learning a diagonal attention alignment with the original prenet. However, if the target dataset is high quality then it also works from the start. It is also suggested to disable dropout if BN is in use.     prenet_type == \"original\"         x -&gt; [linear -&gt; ReLU -&gt; Dropout]xN -&gt; o     prenet_type == \"bn\"         x -&gt; [linear -&gt; BN -&gt; ReLU -&gt; Dropout]xN -&gt; o Args:     in_features (int): number of channels in the input tensor and the inner layers.     prenet_type (str, optional): prenet type \"original\" or \"bn\". Defaults to \"original\".     prenet_dropout (bool, optional): dropout rate. Defaults to True.     dropout_at_inference (bool, optional): use dropout at inference. It leads to a better quality for some models.     out_features (list, optional): List of output channels for each prenet block.         It also defines number of the prenet blocks based on the length of argument list.         Defaults to [256, 256].     bias (bool, optional): enable/disable bias in prenet linear layers. Defaults to True.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class Prenet(nn.Module):\n    \"\"\"Tacotron specific Prenet with an optional Batch Normalization.\n    Note:\n        Prenet with BN improves the model performance significantly especially\n    if it is enabled after learning a diagonal attention alignment with the original\n    prenet. However, if the target dataset is high quality then it also works from\n    the start. It is also suggested to disable dropout if BN is in use.\n        prenet_type == \"original\"\n            x -&gt; [linear -&gt; ReLU -&gt; Dropout]xN -&gt; o\n        prenet_type == \"bn\"\n            x -&gt; [linear -&gt; BN -&gt; ReLU -&gt; Dropout]xN -&gt; o\n    Args:\n        in_features (int): number of channels in the input tensor and the inner layers.\n        prenet_type (str, optional): prenet type \"original\" or \"bn\". Defaults to \"original\".\n        prenet_dropout (bool, optional): dropout rate. Defaults to True.\n        dropout_at_inference (bool, optional): use dropout at inference. It leads to a better quality for some models.\n        out_features (list, optional): List of output channels for each prenet block.\n            It also defines number of the prenet blocks based on the length of argument list.\n            Defaults to [256, 256].\n        bias (bool, optional): enable/disable bias in prenet linear layers. Defaults to True.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_features,\n        prenet_type=\"original\",\n        dropout_at_inference=False,\n        out_features=[256, 256],\n        bias=True,\n        weight_norm=False\n    ):\n        super().__init__()\n        self.prenet_type = prenet_type\n        self.dropout_at_inference = dropout_at_inference\n        in_features = [in_features] + out_features[:-1]\n        # if prenet_type == \"bn\":\n        #     self.linear_layers = nn.ModuleList(\n        #         [LinearBN(in_size, out_size, bias=bias) for (in_size, out_size) in zip(in_features, out_features)]\n        #     )\n        # elif prenet_type == \"original\":\n        norm = (\n            nn.utils.parametrizations.weight_norm \n            if weight_norm else lambda x:x)\n        self.linear_layers = nn.ModuleList([\n            norm(nn.Linear(in_size, out_size, bias=bias))\n            for (in_size, out_size) in zip(in_features, out_features)\n        ])\n\n    def forward(self, x, dropout:float=0.5):\n        for linear in self.linear_layers:\n            if dropout:\n                x = F.dropout(\n                    F.relu(linear(x)), p=dropout, \n                    training=self.training or self.dropout_at_inference)\n            else:\n                x = F.relu(linear(x))\n        return x\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.StandardNormal","title":"<code>StandardNormal</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class StandardNormal(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        return size\n\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"standard normal negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        mu = params\n        loglik = 0.5 * (\n            (x - mu) ** 2 + self.log2pi\n        )\n        return loglik.sum(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0):\n        return params + temperature*torch.randn_like(params)\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.StandardNormal.forward","title":"<code>forward(x, params)</code>","text":"<p>standard normal negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor):\n    \"\"\"standard normal negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    mu = params\n    loglik = 0.5 * (\n        (x - mu) ** 2 + self.log2pi\n    )\n    return loglik.sum(-1)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore","title":"<code>TacotronCore</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class TacotronCore(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        frame_channels=None, # RAVE latent dim\n        dropout=0.1,\n        dropout_type='dropout', #'zoneout'\n        prenet_type='original', # disabled\n        prenet_dropout=0.2,\n        prenet_layers=2,\n        prenet_size=256,\n        prenet_wn=False,\n        hidden_dropout=0,\n        separate_stopnet=True, # disabled\n        rnn_size=1200,\n        rnn_bias=True,\n        rnn_layers=1,\n        noise_channels=0,\n        decoder_type='lstm',\n        decoder_layers=1,\n        decoder_size=None,\n        hidden_to_decoder=True,\n        memory_to_decoder=False,\n        init_proj=1,\n        proj_wn=False,\n        attn_wn=False,\n        learn_go_frame=False,\n        pitch_xform=False,\n        block_size=2048,\n        max_batch=8,\n        max_tokens=1024,\n        prior_filter_len=11,\n        tokens_per_frame=1.0,\n        attention_type='dca',\n        length_reparam=False\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input (text feature) channels.\n            out_channels (int): number of output (likelihood parameter) channels.\n            frame_channels (int): number of audio latent channels.\n            dropout (float): dropout rate (except prenet).\n            prenet_dropout (float): prenet dropout rate.\n            max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n        \"\"\"\n        super().__init__()\n        assert frame_channels is not None\n\n        self.B = max_batch\n        self.T = max_tokens\n\n        self.length_reparam = length_reparam\n        self.hidden_to_decoder = hidden_to_decoder\n        self.memory_to_decoder = memory_to_decoder\n\n        self.block_size = block_size\n        self.frame_channels = frame_channels\n        self.pitch_xform = pitch_xform\n        # self.r_init = r\n        # self.r = r\n        self.encoder_embedding_dim = in_channels\n        self.separate_stopnet = separate_stopnet\n\n        decoder_size = decoder_size or rnn_size\n\n        # model dimensions\n        self.rnn_layers = rnn_layers\n        self.decoder_layers = decoder_layers\n        self.attention_hidden_dim = rnn_size\n        self.decoder_rnn_dim = decoder_size\n        self.prenet_dim = prenet_size\n        self.noise_channels = noise_channels\n        # self.attn_dim = 128\n        self.p_attention_dropout = dropout\n        self.p_decoder_dropout = dropout\n        self.dropout_type = dropout_type\n\n        # memory -&gt; |Prenet| -&gt; processed_memory\n        prenet_dim = self.frame_channels\n        self.prenet_dropout = prenet_dropout\n        self.prenet = Prenet(\n            prenet_dim, prenet_type, \n            out_features=[self.prenet_dim]*prenet_layers, \n            bias=False, weight_norm=prenet_wn\n        )\n\n        self.hidden_dropout = hidden_dropout\n\n        # self.attention_rnn = DropoutRNN(nn.LSTMCell(\n            # self.prenet_dim + in_channels, self.attention_hidden_dim, bias=rnn_bias))\n\n        # interlayer dropout is given here, recurrent dropout in forward\n        # not for a good reason but it's fine\n        self.attention_rnn = ResidualRNN( \n            lambda i,h,**kw: DropoutRNN(nn.LSTMCell(i,h,bias=rnn_bias),**kw),\n            self.prenet_dim + in_channels + noise_channels, \n            self.attention_hidden_dim, \n            layers=rnn_layers, dropout=dropout\n        )\n\n        if attention_type=='dca':\n            prior_alpha = tokens_per_frame / (prior_filter_len-1)\n            prior_beta = 1-prior_alpha\n            self.attention = MonotonicDynamicConvolutionAttention(\n                query_dim=self.attention_hidden_dim,\n                attention_dim=128,\n                prior_filter_len=prior_filter_len,\n                alpha=prior_alpha,\n                beta=prior_beta\n            )\n        elif attention_type=='gauss':\n            self.attention = GaussianAttention(\n                query_dim=self.attention_hidden_dim,\n                prior_tokens_per_frame=tokens_per_frame,\n                wn=attn_wn\n            )\n\n        # a decoder network further processes the text glimpse and main RNN state,\n        # which can increase depth, while using optimized RNN kernels on GPU,\n        # which the main RNN can't because it is interleaved with attention.\n        # so moving parameters from attention RNN to decoder RNN may make training\n        # faster (though not inference)\n        # it can also be skipped entirely.\n        if decoder_type is None:\n            self.decoder_rnn = None\n        else:\n            if decoder_type=='lstm':\n                decoder_type = nn.LSTM\n            elif decoder_type=='mlp':\n                decoder_type = MLPDecoder\n            else:\n                raise ValueError(decoder_type)\n            decoder_in = in_channels\n            if hidden_to_decoder:\n                decoder_in += self.attention_hidden_dim \n            if memory_to_decoder: \n                decoder_in += self.prenet_dim + noise_channels\n            self.decoder_rnn = decoder_type(\n                decoder_in, self.decoder_rnn_dim, \n                num_layers=self.decoder_layers,\n                bias=rnn_bias, batch_first=True, \n                dropout=0 if decoder_layers==1 else dropout)\n\n        # this linear projection matches the sizes of the hidden state and\n        # the likelihood \n        hidden_size = (\n            self.attention_hidden_dim \n            if self.decoder_rnn is None \n            else self.decoder_rnn_dim)\n        linear_projection = nn.Linear(\n            hidden_size + in_channels, \n            out_channels)\n\n        with torch.no_grad():\n            linear_projection.weight.mul_(init_proj)\n\n        if proj_wn:\n            linear_projection = nn.utils.parametrizations.weight_norm(\n                linear_projection)\n        self.linear_projection = linear_projection\n\n\n        # the stopnet predicts whether the utterance has ended\n        self.stopnet = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(\n                # self.decoder_rnn_dim + self.frame_channels * self.r_init, 1, \n                hidden_size + self.frame_channels, 1, \n                # bias=True, init_gain=\"sigmoid\"\n                ),\n        )\n\n        # TODO: need to clone these or no?\n        # does register_buffer copy?\n        # does Parameter copy?\n        t_mem = torch.zeros(1, self.frame_channels)\n        t_attn = torch.zeros(self.rnn_layers, 1, self.attention_hidden_dim)\n        t_dec = torch.zeros(self.decoder_layers, 1, self.decoder_rnn_dim)\n        t_ctx = torch.zeros(1, self.encoder_embedding_dim)\n\n        # initial state parameters\n        self.learn_go_frame = learn_go_frame\n        # if self.learn_go_frame:\n        self.go_frame = nn.Parameter(t_mem.clone())\n        self.go_attention_hidden = nn.Parameter(t_attn.clone())\n        self.go_attention_cell = nn.Parameter(t_attn.clone())\n        self.go_decoder_hidden = nn.Parameter(t_dec.clone())\n        self.go_decoder_cell = nn.Parameter(t_dec.clone())\n        self.go_context = nn.Parameter(t_ctx.clone())\n\n        # print(id(self.go_attention_hidden.data))\n        # print(id(self.go_attention_cell.data))\n\n        # state buffers for inference\n        self.register_buffer(\n            \"memory\", t_mem.expand(max_batch, -1).clone()) \n        self.register_buffer(\n            \"context\", t_ctx.expand(max_batch, -1).clone())\n        self.register_buffer(\n            \"attention_hidden\", t_attn.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"attention_cell\", t_attn.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"decoder_hidden\", t_dec.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"decoder_cell\", t_dec.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"alignment\", torch.zeros(max_batch, max_tokens))\n        self.register_buffer(\n            \"inputs\", torch.zeros(max_batch, max_tokens, in_channels))\n\n    def init_states(self, inputs):#, keep_states=False):\n        \"\"\"\n        return initial states\n        \"\"\"\n        B = inputs.size(0)\n        # if not keep_states:\n        if self.learn_go_frame:\n            attention_hidden = self.go_attention_hidden.expand(B, -1)\n            attention_cell = self.go_attention_cell.expand(-1, B, -1)\n            decoder_hidden = self.go_decoder_hidden.expand(-1, B, -1)\n            decoder_cell = self.go_decoder_cell.expand(-1, B, -1)\n            context = self.go_context.expand(B, -1)\n            memory = self.go_frame.expand(B,-1).clone()\n        else:\n            attention_hidden = inputs.new_zeros(\n                self.rnn_layers, B, self.attention_hidden_dim)\n            attention_cell = torch.zeros_like(attention_hidden)\n            decoder_hidden = inputs.new_zeros(\n                self.decoder_layers, B, self.decoder_rnn_dim)\n            decoder_cell = torch.zeros_like(decoder_hidden)\n            context = inputs.new_zeros(B, self.encoder_embedding_dim)\n            memory = inputs.new_zeros(B, self.frame_channels)           \n\n        alignment = self.attention.init_states(inputs)\n\n        # for t in (memory, context, alignment,\n        #     attention_hidden, attention_cell, \n        #     decoder_hidden, decoder_cell):\n        #     print(t.shape)\n\n        return (\n            memory, context, alignment,\n            attention_hidden, attention_cell, \n            decoder_hidden, decoder_cell)\n\n    @torch.jit.export\n    def reset(self, inputs):\n        r\"\"\"\n        populates buffers with initial states and text inputs\n\n        call with encoded text, before using `step`\n\n        Args:\n            inputs: (B, T_text, D_text)\n\n        \"\"\"\n        assert inputs.ndim==3, str(inputs.shape)#f'{inputs.shape=}'\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        assert B&lt;=self.inputs.shape[0], 'max batch size exceeded'\n        assert T&lt;=self.inputs.shape[1], 'max tokens exceeded'\n\n        (\n            self.memory[:B], self.context[:B], self.alignment[:B, :T],\n            self.attention_hidden[:,:B], self.attention_cell[:,:B], \n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n        ) = self.init_states(inputs)#, keep_states=False)\n\n        self.inputs[:B, :T] = inputs\n\n        self.B = B\n        self.T = T\n\n    # TODO: should there be option to include acoustic memory here?\n    @torch.jit.export\n    def get_state(self):\n        return {\n            'rnn_states': (\n                self.attention_hidden.clone(),\n                self.attention_cell.clone(),\n                self.decoder_hidden.clone(),\n                self.decoder_cell.clone()\n            ),\n            'context': self.context.clone(),\n            'attention_states': self.alignment.clone(),\n        }\n\n    @torch.jit.export\n    def set_state(self, state:Dict[str,Tensor|Tuple[Tensor, Tensor, Tensor, Tensor]]):\n        rnn_states = state['rnn_states']\n        if torch.jit.is_scripting():\n            # weirdly torschript seems to demand this while Python chokes on \n            # the use of generic types with isintance\n            assert isinstance(rnn_states, Tuple[Tensor, Tensor, Tensor, Tensor])\n        (\n            self.attention_hidden[:],\n            self.attention_cell[:],\n            self.decoder_hidden[:],\n            self.decoder_cell[:]\n        ) = rnn_states\n        context = state['context']\n        alignment = state['attention_states']\n        assert isinstance(context, Tensor)\n        assert isinstance(alignment, Tensor)\n        self.context[:] = context\n        self.alignment[:] = alignment\n\n    # @torch.jit.export\n    def forward(self, \n            inputs,\n            context, memory, alignment,\n            attention_hidden, attention_cell,\n            set_alignment:bool=False,\n            mask:Optional[Tensor]=None):\n        \"\"\"run step of attention loop\n        Args:\n            inputs: [B, T_text, D_text] encoded text\n            context: [B, D_text] combined text encoding from previous alignment\n            memory: [B, D_audio] acoustic memory of last output\n            alignment: [B, T_text]\n            attention_hidden: [B, attention_hidden_dim]\n            attention_cell: [B, attention_hidden_dim]\n            set_alignment: bool\n                if True, `alignment` is the next alignment to text\n                if False, `alignment` is the previous alignment, \n                    and the attention module computes next\n        Returns:\n            context: as above\n            alignment: as above\n            attention_hidden: as above\n            attention_cell: as above\n        \"\"\"\n        # feed the latest text and audio encodings into the RNN\n        query_input = [memory, context]\n        if self.noise_channels:\n            query_input.append(torch.randn(\n                context.shape[0], self.noise_channels, \n                device=context.device, dtype=context.dtype))\n        query_input = torch.cat(query_input, -1)\n        attention_hidden, attention_cell = self.attention_rnn(\n            query_input, (attention_hidden, attention_cell),\n            training=self.training, \n            dropout_p=self.p_attention_dropout, \n            dropout_type=self.dropout_type)\n\n        if not set_alignment:\n            # compute next alignment from the RNN state\n            alignment = self.attention.align(\n                attention_hidden[-1], alignment, inputs, mask)\n\n        # combine text encodings according to the new alignment\n        context = self.attention.apply(alignment, inputs, mask)\n\n        return (\n            context,\n            alignment,\n            attention_hidden, attention_cell\n        )\n\n    @torch.jit.export\n    def decode_post(self, \n            hidden, context, memory,\n            decoder_hidden, decoder_cell,\n            lengths:Optional[Tensor]=None\n        ):\n        \"\"\"run post-decoder (step or full time dimension)\n\n        Args:\n            hidden: B x T_audio x channel (hidden state after attention net)\n            context: B x T_audio x D_text (audio-aligned text features)\n            lengths: if not None, pack the inputs\n        Returns:\n            hidden: B x T_audio x channel (hidden state after decoder net)\n            output_params: B x T_audio x channel (likelihood parameters)\n        \"\"\"\n        if self.decoder_rnn is not None:\n            decoder_rnn_input = []\n            if self.hidden_to_decoder:\n                decoder_rnn_input.append(hidden)\n            if self.memory_to_decoder:\n                decoder_rnn_input.append(memory)\n            decoder_rnn_input.append(context)\n            decoder_rnn_input = torch.cat(decoder_rnn_input, -1)\n            if lengths is not None:\n                decoder_rnn_input_packed = nn.utils.rnn.pack_padded_sequence(\n                    decoder_rnn_input, lengths, \n                    batch_first=True, enforce_sorted=False)\n\n                # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n                hidden_packed, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                    decoder_rnn_input_packed, (decoder_hidden, decoder_cell))\n\n                hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(\n                    hidden_packed, batch_first=True)\n\n                # in case pad_packed messes up extra padding\n                hidden = torch.cat((hidden, hidden.new_zeros(\n                    hidden.shape[0], \n                    context.shape[1]-hidden.shape[1], \n                    hidden.shape[2])\n                    ), 1)\n            else:\n                # TODO why contiguous needed here when decoder_layers &gt; 1?\n                hidden, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                    decoder_rnn_input, (decoder_hidden.contiguous(), decoder_cell.contiguous()))\n\n        if self.hidden_dropout:\n            hidden = F.dropout(hidden, float(self.hidden_dropout), self.training)\n        # # B x T x (D_decoder_rnn + D_text)\n        decoder_hidden_context = torch.cat((hidden, context), dim=-1)\n        # B x T x self.frame_channels\n        output_params = self.linear_projection(decoder_hidden_context)\n        return hidden, output_params, decoder_hidden, decoder_cell\n\n    @torch.jit.export\n    def predict_stop(self, decoder_state, output):\n        # B x (D_decoder_rnn + (self.r * self.frame_channels))\n        stopnet_input = torch.cat((decoder_state, output), dim=-1)\n        if self.separate_stopnet:\n            stopnet_input = stopnet_input.detach()\n        stop_token = self.stopnet(stopnet_input)\n        return stop_token\n\n    @torch.jit.export\n    def latent_map(self, z):\n        if self.pitch_xform:\n            z = torch.cat((\n                hz_to_z(z[...,:1]),\n                z[...,1:]\n            ), -1) \n\n        if self.length_reparam:\n            m = torch.linalg.vector_norm(z, dim=-1, keepdim=True)\n            z = torch.cat((m, z), -1)\n        return z\n\n    @torch.jit.export\n    def latent_unmap(self, z):\n        if self.length_reparam:\n            m, z = z.split((1,z.shape[-1]-1), dim=-1)\n            z = z * (\n                m / (torch.linalg.vector_norm(z, dim=-1, keepdim=True)+1e-7))\n\n        if self.pitch_xform:\n            z = torch.cat((\n                z_to_hz(z[...,:1]),\n                z[...,1:]\n            ), -1) \n        return z\n\n    def decode_loop(self, \n            inputs, context, memories, alignment, att_hidden, att_cell, mask):\n        \"\"\"loop over training data frames, align to text\"\"\"\n        hidden, contexts, alignments = [], [], []\n        for memory in memories[:-1]:\n            context, alignment, att_hidden, att_cell = self(\n                inputs, context, memory, alignment,\n                att_hidden, att_cell, set_alignment=False,\n                mask=mask)\n            hidden.append(att_hidden[-1])\n            contexts.append(context)\n            alignments.append(alignment)\n        hidden = torch.stack(hidden, 1)\n        contexts = torch.stack(contexts, 1)\n        alignments = torch.stack(alignments, 1)\n        return hidden, contexts, alignments\n\n        # @torch.jit.ignore\n    @torch.jit.export\n    def step_pre(self, \n            alignment:Optional[Tensor]=None,\n            audio_frame:Optional[Tensor]=None\n        ):\n        B, T = self.B, self.T\n        if alignment is None:\n            alignment = self.alignment[:B, :T]\n            set_alignment = False\n        else:\n            assert alignment.ndim==2, str(alignment.shape)\n            set_alignment = True\n\n        if audio_frame is not None:\n            self.memory[:B] = self.latent_map(audio_frame)\n\n        # print(self.memory[:B])\n\n        memory = self.prenet(\n            self.memory[:B], dropout=float(self.prenet_dropout))\n        # if alignment is not None: print(f'DEBUG: {alignment.shape=}, {memory.shape=}')\n        (\n            self.context[:B], \n            alignment,\n            self.attention_hidden[:,:B], self.attention_cell[:,:B]\n        ) = self(\n            self.inputs[:B, :T], \n            self.context[:B], memory, alignment,\n            self.attention_hidden[:,:B], self.attention_cell[:,:B], \n            set_alignment=set_alignment,\n            mask=None)\n        (\n            _, output_params, \n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n        ) = self.decode_post(\n            self.attention_hidden[-1,:B,None], self.context[:B,None], \n            memory[:B,None],\n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B],\n            lengths=None)\n\n        return alignment, output_params\n\n    @torch.jit.export\n    def step_post(self, alignment, decoder_output):\n        self.memory[:self.B] = decoder_output\n        self.alignment[:self.B,:self.T] = alignment\n\n        decoder_output = self.latent_unmap(decoder_output)\n        return decoder_output\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.__init__","title":"<code>__init__(in_channels, out_channels, frame_channels=None, dropout=0.1, dropout_type='dropout', prenet_type='original', prenet_dropout=0.2, prenet_layers=2, prenet_size=256, prenet_wn=False, hidden_dropout=0, separate_stopnet=True, rnn_size=1200, rnn_bias=True, rnn_layers=1, noise_channels=0, decoder_type='lstm', decoder_layers=1, decoder_size=None, hidden_to_decoder=True, memory_to_decoder=False, init_proj=1, proj_wn=False, attn_wn=False, learn_go_frame=False, pitch_xform=False, block_size=2048, max_batch=8, max_tokens=1024, prior_filter_len=11, tokens_per_frame=1.0, attention_type='dca', length_reparam=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input (text feature) channels.</p> required <code>out_channels</code> <code>int</code> <p>number of output (likelihood parameter) channels.</p> required <code>frame_channels</code> <code>int</code> <p>number of audio latent channels.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>dropout rate (except prenet).</p> <code>0.1</code> <code>prenet_dropout</code> <code>float</code> <p>prenet dropout rate.</p> <code>0.2</code> <code>max_decoder_steps</code> <code>int</code> <p>Maximum number of steps allowed for the decoder. Defaults to 10000.</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    frame_channels=None, # RAVE latent dim\n    dropout=0.1,\n    dropout_type='dropout', #'zoneout'\n    prenet_type='original', # disabled\n    prenet_dropout=0.2,\n    prenet_layers=2,\n    prenet_size=256,\n    prenet_wn=False,\n    hidden_dropout=0,\n    separate_stopnet=True, # disabled\n    rnn_size=1200,\n    rnn_bias=True,\n    rnn_layers=1,\n    noise_channels=0,\n    decoder_type='lstm',\n    decoder_layers=1,\n    decoder_size=None,\n    hidden_to_decoder=True,\n    memory_to_decoder=False,\n    init_proj=1,\n    proj_wn=False,\n    attn_wn=False,\n    learn_go_frame=False,\n    pitch_xform=False,\n    block_size=2048,\n    max_batch=8,\n    max_tokens=1024,\n    prior_filter_len=11,\n    tokens_per_frame=1.0,\n    attention_type='dca',\n    length_reparam=False\n):\n    \"\"\"\n    Args:\n        in_channels (int): number of input (text feature) channels.\n        out_channels (int): number of output (likelihood parameter) channels.\n        frame_channels (int): number of audio latent channels.\n        dropout (float): dropout rate (except prenet).\n        prenet_dropout (float): prenet dropout rate.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n    \"\"\"\n    super().__init__()\n    assert frame_channels is not None\n\n    self.B = max_batch\n    self.T = max_tokens\n\n    self.length_reparam = length_reparam\n    self.hidden_to_decoder = hidden_to_decoder\n    self.memory_to_decoder = memory_to_decoder\n\n    self.block_size = block_size\n    self.frame_channels = frame_channels\n    self.pitch_xform = pitch_xform\n    # self.r_init = r\n    # self.r = r\n    self.encoder_embedding_dim = in_channels\n    self.separate_stopnet = separate_stopnet\n\n    decoder_size = decoder_size or rnn_size\n\n    # model dimensions\n    self.rnn_layers = rnn_layers\n    self.decoder_layers = decoder_layers\n    self.attention_hidden_dim = rnn_size\n    self.decoder_rnn_dim = decoder_size\n    self.prenet_dim = prenet_size\n    self.noise_channels = noise_channels\n    # self.attn_dim = 128\n    self.p_attention_dropout = dropout\n    self.p_decoder_dropout = dropout\n    self.dropout_type = dropout_type\n\n    # memory -&gt; |Prenet| -&gt; processed_memory\n    prenet_dim = self.frame_channels\n    self.prenet_dropout = prenet_dropout\n    self.prenet = Prenet(\n        prenet_dim, prenet_type, \n        out_features=[self.prenet_dim]*prenet_layers, \n        bias=False, weight_norm=prenet_wn\n    )\n\n    self.hidden_dropout = hidden_dropout\n\n    # self.attention_rnn = DropoutRNN(nn.LSTMCell(\n        # self.prenet_dim + in_channels, self.attention_hidden_dim, bias=rnn_bias))\n\n    # interlayer dropout is given here, recurrent dropout in forward\n    # not for a good reason but it's fine\n    self.attention_rnn = ResidualRNN( \n        lambda i,h,**kw: DropoutRNN(nn.LSTMCell(i,h,bias=rnn_bias),**kw),\n        self.prenet_dim + in_channels + noise_channels, \n        self.attention_hidden_dim, \n        layers=rnn_layers, dropout=dropout\n    )\n\n    if attention_type=='dca':\n        prior_alpha = tokens_per_frame / (prior_filter_len-1)\n        prior_beta = 1-prior_alpha\n        self.attention = MonotonicDynamicConvolutionAttention(\n            query_dim=self.attention_hidden_dim,\n            attention_dim=128,\n            prior_filter_len=prior_filter_len,\n            alpha=prior_alpha,\n            beta=prior_beta\n        )\n    elif attention_type=='gauss':\n        self.attention = GaussianAttention(\n            query_dim=self.attention_hidden_dim,\n            prior_tokens_per_frame=tokens_per_frame,\n            wn=attn_wn\n        )\n\n    # a decoder network further processes the text glimpse and main RNN state,\n    # which can increase depth, while using optimized RNN kernels on GPU,\n    # which the main RNN can't because it is interleaved with attention.\n    # so moving parameters from attention RNN to decoder RNN may make training\n    # faster (though not inference)\n    # it can also be skipped entirely.\n    if decoder_type is None:\n        self.decoder_rnn = None\n    else:\n        if decoder_type=='lstm':\n            decoder_type = nn.LSTM\n        elif decoder_type=='mlp':\n            decoder_type = MLPDecoder\n        else:\n            raise ValueError(decoder_type)\n        decoder_in = in_channels\n        if hidden_to_decoder:\n            decoder_in += self.attention_hidden_dim \n        if memory_to_decoder: \n            decoder_in += self.prenet_dim + noise_channels\n        self.decoder_rnn = decoder_type(\n            decoder_in, self.decoder_rnn_dim, \n            num_layers=self.decoder_layers,\n            bias=rnn_bias, batch_first=True, \n            dropout=0 if decoder_layers==1 else dropout)\n\n    # this linear projection matches the sizes of the hidden state and\n    # the likelihood \n    hidden_size = (\n        self.attention_hidden_dim \n        if self.decoder_rnn is None \n        else self.decoder_rnn_dim)\n    linear_projection = nn.Linear(\n        hidden_size + in_channels, \n        out_channels)\n\n    with torch.no_grad():\n        linear_projection.weight.mul_(init_proj)\n\n    if proj_wn:\n        linear_projection = nn.utils.parametrizations.weight_norm(\n            linear_projection)\n    self.linear_projection = linear_projection\n\n\n    # the stopnet predicts whether the utterance has ended\n    self.stopnet = nn.Sequential(\n        nn.Dropout(0.1),\n        nn.Linear(\n            # self.decoder_rnn_dim + self.frame_channels * self.r_init, 1, \n            hidden_size + self.frame_channels, 1, \n            # bias=True, init_gain=\"sigmoid\"\n            ),\n    )\n\n    # TODO: need to clone these or no?\n    # does register_buffer copy?\n    # does Parameter copy?\n    t_mem = torch.zeros(1, self.frame_channels)\n    t_attn = torch.zeros(self.rnn_layers, 1, self.attention_hidden_dim)\n    t_dec = torch.zeros(self.decoder_layers, 1, self.decoder_rnn_dim)\n    t_ctx = torch.zeros(1, self.encoder_embedding_dim)\n\n    # initial state parameters\n    self.learn_go_frame = learn_go_frame\n    # if self.learn_go_frame:\n    self.go_frame = nn.Parameter(t_mem.clone())\n    self.go_attention_hidden = nn.Parameter(t_attn.clone())\n    self.go_attention_cell = nn.Parameter(t_attn.clone())\n    self.go_decoder_hidden = nn.Parameter(t_dec.clone())\n    self.go_decoder_cell = nn.Parameter(t_dec.clone())\n    self.go_context = nn.Parameter(t_ctx.clone())\n\n    # print(id(self.go_attention_hidden.data))\n    # print(id(self.go_attention_cell.data))\n\n    # state buffers for inference\n    self.register_buffer(\n        \"memory\", t_mem.expand(max_batch, -1).clone()) \n    self.register_buffer(\n        \"context\", t_ctx.expand(max_batch, -1).clone())\n    self.register_buffer(\n        \"attention_hidden\", t_attn.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"attention_cell\", t_attn.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"decoder_hidden\", t_dec.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"decoder_cell\", t_dec.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"alignment\", torch.zeros(max_batch, max_tokens))\n    self.register_buffer(\n        \"inputs\", torch.zeros(max_batch, max_tokens, in_channels))\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.decode_loop","title":"<code>decode_loop(inputs, context, memories, alignment, att_hidden, att_cell, mask)</code>","text":"<p>loop over training data frames, align to text</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def decode_loop(self, \n        inputs, context, memories, alignment, att_hidden, att_cell, mask):\n    \"\"\"loop over training data frames, align to text\"\"\"\n    hidden, contexts, alignments = [], [], []\n    for memory in memories[:-1]:\n        context, alignment, att_hidden, att_cell = self(\n            inputs, context, memory, alignment,\n            att_hidden, att_cell, set_alignment=False,\n            mask=mask)\n        hidden.append(att_hidden[-1])\n        contexts.append(context)\n        alignments.append(alignment)\n    hidden = torch.stack(hidden, 1)\n    contexts = torch.stack(contexts, 1)\n    alignments = torch.stack(alignments, 1)\n    return hidden, contexts, alignments\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.decode_post","title":"<code>decode_post(hidden, context, memory, decoder_hidden, decoder_cell, lengths=None)</code>","text":"<p>run post-decoder (step or full time dimension)</p> <p>Parameters:</p> Name Type Description Default <code>hidden</code> <p>B x T_audio x channel (hidden state after attention net)</p> required <code>context</code> <p>B x T_audio x D_text (audio-aligned text features)</p> required <code>lengths</code> <code>Optional[Tensor]</code> <p>if not None, pack the inputs</p> <code>None</code> <p>Returns:     hidden: B x T_audio x channel (hidden state after decoder net)     output_params: B x T_audio x channel (likelihood parameters)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef decode_post(self, \n        hidden, context, memory,\n        decoder_hidden, decoder_cell,\n        lengths:Optional[Tensor]=None\n    ):\n    \"\"\"run post-decoder (step or full time dimension)\n\n    Args:\n        hidden: B x T_audio x channel (hidden state after attention net)\n        context: B x T_audio x D_text (audio-aligned text features)\n        lengths: if not None, pack the inputs\n    Returns:\n        hidden: B x T_audio x channel (hidden state after decoder net)\n        output_params: B x T_audio x channel (likelihood parameters)\n    \"\"\"\n    if self.decoder_rnn is not None:\n        decoder_rnn_input = []\n        if self.hidden_to_decoder:\n            decoder_rnn_input.append(hidden)\n        if self.memory_to_decoder:\n            decoder_rnn_input.append(memory)\n        decoder_rnn_input.append(context)\n        decoder_rnn_input = torch.cat(decoder_rnn_input, -1)\n        if lengths is not None:\n            decoder_rnn_input_packed = nn.utils.rnn.pack_padded_sequence(\n                decoder_rnn_input, lengths, \n                batch_first=True, enforce_sorted=False)\n\n            # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n            hidden_packed, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                decoder_rnn_input_packed, (decoder_hidden, decoder_cell))\n\n            hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(\n                hidden_packed, batch_first=True)\n\n            # in case pad_packed messes up extra padding\n            hidden = torch.cat((hidden, hidden.new_zeros(\n                hidden.shape[0], \n                context.shape[1]-hidden.shape[1], \n                hidden.shape[2])\n                ), 1)\n        else:\n            # TODO why contiguous needed here when decoder_layers &gt; 1?\n            hidden, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                decoder_rnn_input, (decoder_hidden.contiguous(), decoder_cell.contiguous()))\n\n    if self.hidden_dropout:\n        hidden = F.dropout(hidden, float(self.hidden_dropout), self.training)\n    # # B x T x (D_decoder_rnn + D_text)\n    decoder_hidden_context = torch.cat((hidden, context), dim=-1)\n    # B x T x self.frame_channels\n    output_params = self.linear_projection(decoder_hidden_context)\n    return hidden, output_params, decoder_hidden, decoder_cell\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.forward","title":"<code>forward(inputs, context, memory, alignment, attention_hidden, attention_cell, set_alignment=False, mask=None)</code>","text":"<p>run step of attention loop Args:     inputs: [B, T_text, D_text] encoded text     context: [B, D_text] combined text encoding from previous alignment     memory: [B, D_audio] acoustic memory of last output     alignment: [B, T_text]     attention_hidden: [B, attention_hidden_dim]     attention_cell: [B, attention_hidden_dim]     set_alignment: bool         if True, <code>alignment</code> is the next alignment to text         if False, <code>alignment</code> is the previous alignment,              and the attention module computes next Returns:     context: as above     alignment: as above     attention_hidden: as above     attention_cell: as above</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, \n        inputs,\n        context, memory, alignment,\n        attention_hidden, attention_cell,\n        set_alignment:bool=False,\n        mask:Optional[Tensor]=None):\n    \"\"\"run step of attention loop\n    Args:\n        inputs: [B, T_text, D_text] encoded text\n        context: [B, D_text] combined text encoding from previous alignment\n        memory: [B, D_audio] acoustic memory of last output\n        alignment: [B, T_text]\n        attention_hidden: [B, attention_hidden_dim]\n        attention_cell: [B, attention_hidden_dim]\n        set_alignment: bool\n            if True, `alignment` is the next alignment to text\n            if False, `alignment` is the previous alignment, \n                and the attention module computes next\n    Returns:\n        context: as above\n        alignment: as above\n        attention_hidden: as above\n        attention_cell: as above\n    \"\"\"\n    # feed the latest text and audio encodings into the RNN\n    query_input = [memory, context]\n    if self.noise_channels:\n        query_input.append(torch.randn(\n            context.shape[0], self.noise_channels, \n            device=context.device, dtype=context.dtype))\n    query_input = torch.cat(query_input, -1)\n    attention_hidden, attention_cell = self.attention_rnn(\n        query_input, (attention_hidden, attention_cell),\n        training=self.training, \n        dropout_p=self.p_attention_dropout, \n        dropout_type=self.dropout_type)\n\n    if not set_alignment:\n        # compute next alignment from the RNN state\n        alignment = self.attention.align(\n            attention_hidden[-1], alignment, inputs, mask)\n\n    # combine text encodings according to the new alignment\n    context = self.attention.apply(alignment, inputs, mask)\n\n    return (\n        context,\n        alignment,\n        attention_hidden, attention_cell\n    )\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.init_states","title":"<code>init_states(inputs)</code>","text":"<p>return initial states</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def init_states(self, inputs):#, keep_states=False):\n    \"\"\"\n    return initial states\n    \"\"\"\n    B = inputs.size(0)\n    # if not keep_states:\n    if self.learn_go_frame:\n        attention_hidden = self.go_attention_hidden.expand(B, -1)\n        attention_cell = self.go_attention_cell.expand(-1, B, -1)\n        decoder_hidden = self.go_decoder_hidden.expand(-1, B, -1)\n        decoder_cell = self.go_decoder_cell.expand(-1, B, -1)\n        context = self.go_context.expand(B, -1)\n        memory = self.go_frame.expand(B,-1).clone()\n    else:\n        attention_hidden = inputs.new_zeros(\n            self.rnn_layers, B, self.attention_hidden_dim)\n        attention_cell = torch.zeros_like(attention_hidden)\n        decoder_hidden = inputs.new_zeros(\n            self.decoder_layers, B, self.decoder_rnn_dim)\n        decoder_cell = torch.zeros_like(decoder_hidden)\n        context = inputs.new_zeros(B, self.encoder_embedding_dim)\n        memory = inputs.new_zeros(B, self.frame_channels)           \n\n    alignment = self.attention.init_states(inputs)\n\n    # for t in (memory, context, alignment,\n    #     attention_hidden, attention_cell, \n    #     decoder_hidden, decoder_cell):\n    #     print(t.shape)\n\n    return (\n        memory, context, alignment,\n        attention_hidden, attention_cell, \n        decoder_hidden, decoder_cell)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronCore.reset","title":"<code>reset(inputs)</code>","text":"<p>populates buffers with initial states and text inputs</p> <p>call with encoded text, before using <code>step</code></p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>(B, T_text, D_text)</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef reset(self, inputs):\n    r\"\"\"\n    populates buffers with initial states and text inputs\n\n    call with encoded text, before using `step`\n\n    Args:\n        inputs: (B, T_text, D_text)\n\n    \"\"\"\n    assert inputs.ndim==3, str(inputs.shape)#f'{inputs.shape=}'\n    B = inputs.shape[0]\n    T = inputs.shape[1]\n    assert B&lt;=self.inputs.shape[0], 'max batch size exceeded'\n    assert T&lt;=self.inputs.shape[1], 'max tokens exceeded'\n\n    (\n        self.memory[:B], self.context[:B], self.alignment[:B, :T],\n        self.attention_hidden[:,:B], self.attention_cell[:,:B], \n        self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n    ) = self.init_states(inputs)#, keep_states=False)\n\n    self.inputs[:B, :T] = inputs\n\n    self.B = B\n    self.T = T\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder","title":"<code>TacotronDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class TacotronDecoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=None, # text embedding dim\n        frame_channels=None, # RAVE latent dim\n        dropout=0.1,\n        likelihood_type='nsf',#'normal'#'mixture'#'ged'\n        mixture_n=16,\n        flow_context=256,\n        flow_hidden=256,\n        flow_layers=2,\n        flow_blocks=2,\n        nsf_bins=16,\n        ged_hidden=256,\n        ged_layers=4,\n        ged_unfold=None,\n        ged_multiply_params=False,\n        ged_project_params=None,\n        ged_glu=False,\n        ged_dropout=None, #None follows main dropout, 0 turns off\n        dropout_type='dropout', #'zoneout'\n        prenet_type='original', # disabled\n        prenet_dropout=0.2,\n        prenet_layers=2,\n        prenet_size=256,\n        prenet_wn=False,\n        hidden_dropout=0,\n        separate_stopnet=True, # disabled\n        max_decoder_steps=10000,\n        text_encoder:Dict=None,\n        rnn_size=1200,\n        rnn_bias=True,\n        rnn_layers=1,\n        noise_channels=0,\n        decoder_type='lstm',\n        decoder_layers=1,\n        decoder_size=None,\n        hidden_to_decoder=True,\n        memory_to_decoder=False,\n        init_proj=1.0,\n        proj_wn=False,\n        attn_wn=False,\n        learn_go_frame=False,\n        pitch_xform=False,\n        length_reparam=False,\n        text_encoder_type='canine',\n        block_size=2048,\n        max_batch=8,\n        max_tokens=1024,\n        prior_filter_len=11,\n        tokens_per_frame=1.0,\n        attention_type='dca',\n        script=False\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input channels.\n            frame_channels (int): number of feature frame channels.\n            dropout (float): dropout rate (except prenet).\n            prenet_dropout (float): prenet dropout rate.\n            max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n            text_encoder: dict of text encoder kwargs\n        \"\"\"\n        super().__init__()\n        assert frame_channels is not None\n\n        if length_reparam:\n            frame_channels = frame_channels + 1\n\n        self.B = max_batch\n        self.T = max_tokens\n\n        if text_encoder_type not in [None, 'none']:\n            if text_encoder is None: text_encoder = {}\n            if text_encoder_type=='zero':\n                self.text_encoder = ZeroEncoder(**text_encoder)\n            elif text_encoder_type=='baseline':\n                self.text_encoder = TacotronEncoder(**text_encoder)\n            elif text_encoder_type=='canine':\n                self.text_encoder = CanineEncoder(**text_encoder)\n            elif text_encoder_type=='canine_embedding':\n                self.text_encoder = CanineEmbeddings(**text_encoder)\n            else:\n                raise ValueError(text_encoder_type)\n            if in_channels is None:\n                in_channels = self.text_encoder.channels\n            elif in_channels != self.text_encoder.channels:\n                raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n        else:\n            self.text_encoder = None\n            assert in_channels is not None\n\n        self.max_decoder_steps = max_decoder_steps\n        self.block_size = block_size\n        self.frame_channels = frame_channels\n        # self.pitch_xform = pitch_xform\n        # # self.r_init = r\n        # # self.r = r\n        # self.encoder_embedding_dim = in_channels\n        # self.separate_stopnet = separate_stopnet\n        # self.max_decoder_steps = max_decoder_steps\n        self.stop_threshold = 0.5\n\n        # decoder_size = decoder_size or rnn_size\n\n        # # model dimensions\n        # self.decoder_layers = decoder_layers\n        # self.attention_hidden_dim = rnn_size\n        # self.decoder_rnn_dim = decoder_size\n        # self.prenet_dim = prenet_size\n        # # self.attn_dim = 128\n        # self.p_attention_dropout = dropout\n        # self.p_decoder_dropout = dropout\n        # self.dropout_type = dropout_type\n\n        self.prenet_dropout = prenet_dropout\n\n        # the likelihood converts a hidden state to a probability distribution\n        # over each vocoder frame.\n        # in the simpler cases this is just unpacking location and scale from\n        # the hidden state.\n        # in other cases the likelihood can be a normalizing flow with its own\n        # trainable parameters.\n        if likelihood_type=='normal':\n            self.likelihood = StandardNormal()\n        elif likelihood_type=='diagonal':\n            self.likelihood = DiagonalNormal()\n        elif likelihood_type=='mixture':\n            self.likelihood = DiagonalNormalMixture(mixture_n)\n        elif likelihood_type=='ged':\n            ged_dropout = dropout if ged_dropout is None else ged_dropout\n            self.likelihood = GED(\n                self.frame_channels,\n                hidden_size=ged_hidden, hidden_layers=ged_layers, \n                dropout=ged_dropout, unfold=ged_unfold, \n                multiply_params=ged_multiply_params,\n                project_params=ged_project_params,\n                glu=ged_glu\n                )\n        elif likelihood_type=='nsf':\n            self.likelihood = NSF(\n                self.frame_channels, context_size=flow_context,\n                hidden_size=flow_hidden, hidden_layers=flow_layers,\n                blocks=flow_blocks, bins=nsf_bins,\n                dropout=dropout)\n        else:\n            raise ValueError(likelihood_type)\n\n        if script and likelihood_type!='nsf':\n            self.likelihood = torch.jit.script(self.likelihood)\n\n        self.core = TacotronCore(\n            in_channels=in_channels, # text embedding dim\n            out_channels=self.likelihood.n_params(self.frame_channels),\n            frame_channels=frame_channels, # RAVE latent dim\n            dropout=dropout,\n            dropout_type=dropout_type, #'zoneout'\n            prenet_type=prenet_type, # disabled\n            prenet_dropout=prenet_dropout,\n            prenet_layers=prenet_layers,\n            prenet_size=prenet_size,\n            prenet_wn=prenet_wn,\n            hidden_dropout=hidden_dropout,\n            separate_stopnet=separate_stopnet, # disabled\n            rnn_size=rnn_size,\n            rnn_bias=rnn_bias,\n            rnn_layers=rnn_layers,\n            noise_channels=noise_channels,\n            decoder_type=decoder_type,\n            decoder_layers=decoder_layers,\n            decoder_size=decoder_size,\n            hidden_to_decoder=hidden_to_decoder,\n            memory_to_decoder=memory_to_decoder,\n            init_proj=init_proj,\n            proj_wn=proj_wn,\n            attn_wn=attn_wn,\n            learn_go_frame=learn_go_frame,\n            pitch_xform=pitch_xform,\n            block_size=block_size,\n            max_batch=max_batch,\n            max_tokens=max_tokens,\n            prior_filter_len=prior_filter_len,\n            tokens_per_frame=tokens_per_frame,\n            attention_type=attention_type,\n            length_reparam=length_reparam\n        )\n\n        if script:\n            self.core = torch.jit.script(self.core)\n\n    @property\n    def memory(self):\n        return self.core.memory\n\n    @classmethod\n    def from_checkpoint(cls, path_or_dict):\n        if isinstance(path_or_dict, dict):\n            ckpt = path_or_dict\n        else:\n            ckpt = torch.load(\n                path_or_dict, map_location='cpu', weights_only=False)\n\n        kw = ckpt['kw']\n        model_kw = cls.update_kw_dict(kw['model'])\n\n        model = cls(**model_kw)\n        try:\n            model.load_state_dict(ckpt['model_state'], strict=True)\n        except Exception as e:\n            print(e.__traceback__)\n            model.load_state_dict(ckpt['model_state'], strict=False)\n\n\n        return model\n\n    @classmethod\n    def update_kw_dict(cls, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        b = d.pop('text_bottleneck', None)\n        if b is not None:\n            if d['text_encoder'] is None:\n                d['text_encoder'] = {}\n            d['text_encoder']['bottleneck'] = b\n        return d\n\n    @torch.jit.ignore\n    def update_state_dict(self, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        # TODO: core.\n        def replace(old, new):\n            t = d.pop(old, None)\n            if t is not None:\n                d[new] = t\n        replace('go_attention_rnn_cell_state', 'go_attention_cell')\n        replace('go_query', 'go_attention_hidden')\n        # rnncell -&gt; dropoutrnn\n        replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n        replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n        replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n        replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n        # dropoutrnn -&gt; residualrnn\n        replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n        replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n        replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n        replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n        for name in (\n            'go_attention_hidden', 'go_attention_cell',\n            'attention_hidden', 'attention_cell'\n            ):\n            if name in d and d[name].ndim==2:\n                d[name] = d[name][None]\n        # move into core\n        for name in list(d):\n            if any(name.startswith(s) for s in (\n                \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n                replace(name, f'core.{name}')\n\n        # text bottleneck -&gt; text encoder\n        replace('core.text_proj.weight', 'text_encoder.proj.weight')\n        replace('core.text_proj.bias', 'text_encoder.proj.bias')\n        return d\n\n    @torch.jit.ignore\n    def load_state_dict(self, d, **kw):\n        super().load_state_dict(self.update_state_dict(d), **kw)\n\n    @torch.jit.export\n    def reset(self, inputs):\n        r\"\"\"\n        populates buffers with initial states and text inputs\n\n        call with encoded text, before using `step`\n\n        Args:\n            inputs: (B, T_text, D_text)\n\n        \"\"\"\n        self.core.reset(inputs)\n\n    # TODO: should there be option to include acoustic memory here?\n    @torch.jit.export\n    def get_state(self):\n        return self.core.get_state()\n\n    @torch.jit.export\n    def set_state(self, \n            state:Dict[str,Tensor|Tuple[Tensor, Tensor, Tensor, Tensor]]):\n        self.core.set_state(state)\n\n    def latent_map(self, z):\n        return self.core.latent_map(z)\n\n    def latent_unmap(self, z):\n        return self.core.latent_unmap(z)\n\n    def chunk_pad(self, inputs, mask, c=128):\n        b, t = mask.shape\n        p = math.ceil(t / c) * c - t\n        if p&gt;0:\n            inputs = torch.cat(\n                (inputs, inputs.new_zeros(b, p, *inputs.shape[2:])), 1)\n            mask = torch.cat(\n                (mask, mask.new_zeros(b, p)), 1)\n        return inputs, mask\n\n\n    @torch.jit.ignore\n    def forward(self, inputs, audio, mask, audio_mask,\n            audio_lengths:Optional[Tensor]=None,\n            prenet_dropout:Optional[float]=None,\n            chunk_pad_text:int|None=None,\n            chunk_pad_audio:int|None=None,\n            temperature:float=1\n            ):\n        r\"\"\"Train Decoder with teacher forcing.\n        Args:\n            inputs: raw or encoded text.\n            audio: audio frames for teacher-forcing.\n            mask: text mask for sequence padding.\n            audio_mask: audio mask for loss computation.\n            prenet_dropout: if not None, override original value\n                (to implement e.g. annealing)\n            temperature: no effect on training, only on returned output/MSE\n        Shapes:\n            - inputs: \n                FloatTensor (B, T_text, D_text)\n                or LongTensor (B, T_text)\n            - audio: (B, T_audio, D_audio)\n            - mask: (B, T_text)\n            - audio_mask: (B, T_audio)\n            - stop_target TODO\n\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_audio, T_text)\n            - stop_tokens: (B, T_audio)\n\n        \"\"\"\n        if chunk_pad_audio:\n            audio, audio_mask = self.chunk_pad(\n                audio, audio_mask, chunk_pad_audio)\n\n        if audio_lengths is None:\n            audio_lengths = audio_mask.sum(-1).cpu()\n        ground_truth = audio\n        if prenet_dropout is None:\n            prenet_dropout = self.prenet_dropout\n\n        # print(f'{audio[...,0].min()=}')\n        audio = self.latent_map(audio)\n        # print(f'{audio[...,0].min()=}')\n\n        if inputs.dtype==torch.long:\n            assert self.text_encoder is not None\n            assert inputs.ndim==2\n            inputs = self.text_encoder.encode(inputs, mask)\n        if chunk_pad_text:\n            inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n        (\n            memory, context, alignment,\n            attention_hidden, attention_cell, \n            decoder_hidden, decoder_cell \n        ) = self.core.init_states(inputs)  \n\n        # concat the initial audio frame with training data\n        memories = torch.cat((memory[None], audio.transpose(0, 1)))\n        memories = self.core.prenet(memories, prenet_dropout)\n\n        # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n        hidden, contexts, alignments = self.core.decode_loop(\n            inputs, context, memories, alignment,\n            attention_hidden, attention_cell,\n            mask)\n\n        # compute the additional decoder layers \n        hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n            hidden, contexts, memories[:-1].transpose(0,1),\n            decoder_hidden, decoder_cell, \n            audio_lengths)\n\n        r, outputs = self.run_likelihood(\n            audio, audio_mask, output_params, temperature=temperature)\n\n        stop_loss = None\n        # TODO\n        # stop_tokens = self.predict_stop(hidden, outputs)\n        # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n        outputs = self.latent_unmap(outputs)\n\n        r.update({\n            'text': inputs,\n            # 'stop_loss': stop_loss,\n            'predicted': outputs,\n            'ground_truth': ground_truth,\n            'alignment': alignments,\n            'params': output_params,\n            # 'stop': stop_tokens,\n            'audio_mask': audio_mask,\n            'text_mask': mask,\n            **self.likelihood.metrics(output_params),\n            **self.alignment_metrics(alignments, mask, audio_mask)\n        })\n\n        return r\n\n    def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n        \"\"\"\n        alignments: (B, T_audio, T_text)\n        mask: (B, T_text)\n        \"\"\"\n        # TODO: could normalize concentration by logT and subtract from 1,\n        # so it represents a proportion of the entropy 'unused'\n        # then could have a cutoff parameter\n        # alignment should hit every token: max. entropy of mean token probs\n        # alignment should be sharp: min. mean of token entropy\n        concentration = []\n        concentration_norm = []\n        dispersion = []\n        # alternatively:\n        # max average length of token prob vectors, and of time-curve vectors\n        # if using L2, this enourages token energy to concentrate in few token\n        # dimensions per vector, but to spread across multiple time steps \n        # concentration_l2 = []\n        # dispersion_l2 = []\n        for a,mt,ma in zip(alignments, mask, audio_mask):\n            a = a[ma][:,mt]\n            mean_probs = a.mean(0)\n            ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n            # ent_mean = torch.special.entr(mean_probs).sum()\n            concentration.append(ent_mean)\n            concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n            dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n            # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n            # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n            # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n        return {\n            'concentration': torch.stack(concentration),#.mean(),\n            'concentration_norm': torch.stack(concentration_norm),#.mean(),\n            'dispersion': torch.stack(dispersion),#.mean(),\n            # 'concentration_l2': torch.stack(concentration_l2).mean(),\n            # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n        }\n\n    @torch.jit.ignore\n    def run_likelihood(self, \n            audio, audio_mask, output_params, temperature:float=1):\n        r = {}\n        m = audio_mask[...,None]\n        audio_m = audio*m\n        params_m = output_params*m\n        # nll = self.likelihood(audio*m, output_params*m)\n        # nll = nll.masked_select(audio_mask).mean()\n\n        # NOTE: could improve training performance here?\n        #   use audio_mask before likelihood instead of after\n        r['nll'] = (\n            self.likelihood(audio_m, params_m)\n            .masked_select(audio_mask).mean())\n\n        if isinstance(self.likelihood, DiagonalNormalMixture):\n            r['nll_fixed'] = (\n                self.likelihood(audio_m, params_m, mode='fixed')\n                .masked_select(audio_mask).mean())\n            r['nll_posthoc'] = (\n                self.likelihood(audio_m, params_m, mode='posthoc')\n                .masked_select(audio_mask).mean())\n\n        with torch.no_grad():\n            # return self.likelihood.sample(output_params, temperature=0)\n            outputs = self.likelihood.sample(\n                output_params, temperature=temperature) # low memory\n\n        d = audio_m - outputs*m\n        r['mse'] = (d*d).mean() * m.numel() / m.float().sum()\n        return r, outputs\n\n    # @torch.jit.ignore\n    @torch.jit.export\n    def step(self, \n            alignment:Optional[Tensor]=None,\n            audio_frame:Optional[Tensor]=None, \n            temperature:float=1.0\n        ):\n        r\"\"\"\n        single step of inference.\n\n        optionally supply `alignment` to force the alignments.\n        optionally supply `audio_frame` to set the previous frame.\n\n        Args:\n            alignment: B x T_text\n            audio_frame: B x D_audio\n            temperature: optional sampling temperature\n        Returns:\n            output: B x D_audio\n            alignment: B x T_text\n            stop_token: None (not implemented)\n        \"\"\"\n        alignment, output_params = self.core.step_pre(alignment, audio_frame)\n        decoder_output = self.likelihood.sample(\n            output_params, temperature=temperature).squeeze(1)\n\n        decoder_output = self.core.step_post(alignment, decoder_output)\n\n        # if debug:\n        return dict(\n            output=decoder_output,\n            alignment=alignment,\n            stop_prob=torch.tensor(0.),\n            params=output_params\n        )\n        # else: \n            # return decoder_output, alignment, 0\n\n    # TODO rewrite this or don't script it?\n    # @torch.jit.export\n    @torch.jit.ignore\n    def inference(self, inputs, \n            stop:bool=True, \n            max_steps:Optional[int]=None, \n            temperature:float=1.0,\n            alignments:Optional[Tensor]=None,\n            audio:Optional[Tensor]=None,\n            ):\n        r\"\"\"Decoder inference\n        Can supply forced alignments to text;\n        Can also supply audio for teacher forcing, \n        to test consistency with the training code or implement prompting\n        Args:\n            inputs: Text Encoder outputs.\n            stop: use stop gate\n            max_steps: stop after this many decoder steps\n            temperature: for the sampling distribution\n            alignments: forced alignment\n            audio: forced last-frame\n                (will be offset, you don't need to prepend a start frame)\n        Shapes:\n            - inputs: (B, T_text, D_text)\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_text, T_audio)\n            - audio: (B, T_audio, D_audio)\n            - stop_tokens: (B, T_audio)\n        \"\"\"\n        # max_steps = max_steps or self.max_decoder_steps\n        max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n        self.reset(inputs)\n\n        outputs = []\n        if alignments is None:\n            alignments = []\n            feed_alignment = False\n        else:\n            feed_alignment = True\n\n        for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n            alignment = alignments[:,:,i] if feed_alignment else None\n            audio_frame = (\n                audio[:,i-1,:] \n                if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n                else None)\n            r = self.step(\n                temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n            outputs.append(r['output'])\n            if not feed_alignment:\n                alignments.append(r['alignment'])\n\n            if stop and r['stop_prob']&gt;self.stop_threshold:\n                break\n            if len(outputs) == max_steps:\n                if stop:\n                    print(f\"   &gt; Decoder stopped with {max_steps=}\")\n                break\n\n        outputs = torch.stack(outputs, 1)\n        if not feed_alignment:\n            alignments = torch.stack(alignments, 1)\n\n        return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.__init__","title":"<code>__init__(in_channels=None, frame_channels=None, dropout=0.1, likelihood_type='nsf', mixture_n=16, flow_context=256, flow_hidden=256, flow_layers=2, flow_blocks=2, nsf_bins=16, ged_hidden=256, ged_layers=4, ged_unfold=None, ged_multiply_params=False, ged_project_params=None, ged_glu=False, ged_dropout=None, dropout_type='dropout', prenet_type='original', prenet_dropout=0.2, prenet_layers=2, prenet_size=256, prenet_wn=False, hidden_dropout=0, separate_stopnet=True, max_decoder_steps=10000, text_encoder=None, rnn_size=1200, rnn_bias=True, rnn_layers=1, noise_channels=0, decoder_type='lstm', decoder_layers=1, decoder_size=None, hidden_to_decoder=True, memory_to_decoder=False, init_proj=1.0, proj_wn=False, attn_wn=False, learn_go_frame=False, pitch_xform=False, length_reparam=False, text_encoder_type='canine', block_size=2048, max_batch=8, max_tokens=1024, prior_filter_len=11, tokens_per_frame=1.0, attention_type='dca', script=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels.</p> <code>None</code> <code>frame_channels</code> <code>int</code> <p>number of feature frame channels.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>dropout rate (except prenet).</p> <code>0.1</code> <code>prenet_dropout</code> <code>float</code> <p>prenet dropout rate.</p> <code>0.2</code> <code>max_decoder_steps</code> <code>int</code> <p>Maximum number of steps allowed for the decoder. Defaults to 10000.</p> <code>10000</code> <code>text_encoder</code> <code>Dict</code> <p>dict of text encoder kwargs</p> <code>None</code> Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(\n    self,\n    in_channels=None, # text embedding dim\n    frame_channels=None, # RAVE latent dim\n    dropout=0.1,\n    likelihood_type='nsf',#'normal'#'mixture'#'ged'\n    mixture_n=16,\n    flow_context=256,\n    flow_hidden=256,\n    flow_layers=2,\n    flow_blocks=2,\n    nsf_bins=16,\n    ged_hidden=256,\n    ged_layers=4,\n    ged_unfold=None,\n    ged_multiply_params=False,\n    ged_project_params=None,\n    ged_glu=False,\n    ged_dropout=None, #None follows main dropout, 0 turns off\n    dropout_type='dropout', #'zoneout'\n    prenet_type='original', # disabled\n    prenet_dropout=0.2,\n    prenet_layers=2,\n    prenet_size=256,\n    prenet_wn=False,\n    hidden_dropout=0,\n    separate_stopnet=True, # disabled\n    max_decoder_steps=10000,\n    text_encoder:Dict=None,\n    rnn_size=1200,\n    rnn_bias=True,\n    rnn_layers=1,\n    noise_channels=0,\n    decoder_type='lstm',\n    decoder_layers=1,\n    decoder_size=None,\n    hidden_to_decoder=True,\n    memory_to_decoder=False,\n    init_proj=1.0,\n    proj_wn=False,\n    attn_wn=False,\n    learn_go_frame=False,\n    pitch_xform=False,\n    length_reparam=False,\n    text_encoder_type='canine',\n    block_size=2048,\n    max_batch=8,\n    max_tokens=1024,\n    prior_filter_len=11,\n    tokens_per_frame=1.0,\n    attention_type='dca',\n    script=False\n):\n    \"\"\"\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        dropout (float): dropout rate (except prenet).\n        prenet_dropout (float): prenet dropout rate.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n        text_encoder: dict of text encoder kwargs\n    \"\"\"\n    super().__init__()\n    assert frame_channels is not None\n\n    if length_reparam:\n        frame_channels = frame_channels + 1\n\n    self.B = max_batch\n    self.T = max_tokens\n\n    if text_encoder_type not in [None, 'none']:\n        if text_encoder is None: text_encoder = {}\n        if text_encoder_type=='zero':\n            self.text_encoder = ZeroEncoder(**text_encoder)\n        elif text_encoder_type=='baseline':\n            self.text_encoder = TacotronEncoder(**text_encoder)\n        elif text_encoder_type=='canine':\n            self.text_encoder = CanineEncoder(**text_encoder)\n        elif text_encoder_type=='canine_embedding':\n            self.text_encoder = CanineEmbeddings(**text_encoder)\n        else:\n            raise ValueError(text_encoder_type)\n        if in_channels is None:\n            in_channels = self.text_encoder.channels\n        elif in_channels != self.text_encoder.channels:\n            raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n    else:\n        self.text_encoder = None\n        assert in_channels is not None\n\n    self.max_decoder_steps = max_decoder_steps\n    self.block_size = block_size\n    self.frame_channels = frame_channels\n    # self.pitch_xform = pitch_xform\n    # # self.r_init = r\n    # # self.r = r\n    # self.encoder_embedding_dim = in_channels\n    # self.separate_stopnet = separate_stopnet\n    # self.max_decoder_steps = max_decoder_steps\n    self.stop_threshold = 0.5\n\n    # decoder_size = decoder_size or rnn_size\n\n    # # model dimensions\n    # self.decoder_layers = decoder_layers\n    # self.attention_hidden_dim = rnn_size\n    # self.decoder_rnn_dim = decoder_size\n    # self.prenet_dim = prenet_size\n    # # self.attn_dim = 128\n    # self.p_attention_dropout = dropout\n    # self.p_decoder_dropout = dropout\n    # self.dropout_type = dropout_type\n\n    self.prenet_dropout = prenet_dropout\n\n    # the likelihood converts a hidden state to a probability distribution\n    # over each vocoder frame.\n    # in the simpler cases this is just unpacking location and scale from\n    # the hidden state.\n    # in other cases the likelihood can be a normalizing flow with its own\n    # trainable parameters.\n    if likelihood_type=='normal':\n        self.likelihood = StandardNormal()\n    elif likelihood_type=='diagonal':\n        self.likelihood = DiagonalNormal()\n    elif likelihood_type=='mixture':\n        self.likelihood = DiagonalNormalMixture(mixture_n)\n    elif likelihood_type=='ged':\n        ged_dropout = dropout if ged_dropout is None else ged_dropout\n        self.likelihood = GED(\n            self.frame_channels,\n            hidden_size=ged_hidden, hidden_layers=ged_layers, \n            dropout=ged_dropout, unfold=ged_unfold, \n            multiply_params=ged_multiply_params,\n            project_params=ged_project_params,\n            glu=ged_glu\n            )\n    elif likelihood_type=='nsf':\n        self.likelihood = NSF(\n            self.frame_channels, context_size=flow_context,\n            hidden_size=flow_hidden, hidden_layers=flow_layers,\n            blocks=flow_blocks, bins=nsf_bins,\n            dropout=dropout)\n    else:\n        raise ValueError(likelihood_type)\n\n    if script and likelihood_type!='nsf':\n        self.likelihood = torch.jit.script(self.likelihood)\n\n    self.core = TacotronCore(\n        in_channels=in_channels, # text embedding dim\n        out_channels=self.likelihood.n_params(self.frame_channels),\n        frame_channels=frame_channels, # RAVE latent dim\n        dropout=dropout,\n        dropout_type=dropout_type, #'zoneout'\n        prenet_type=prenet_type, # disabled\n        prenet_dropout=prenet_dropout,\n        prenet_layers=prenet_layers,\n        prenet_size=prenet_size,\n        prenet_wn=prenet_wn,\n        hidden_dropout=hidden_dropout,\n        separate_stopnet=separate_stopnet, # disabled\n        rnn_size=rnn_size,\n        rnn_bias=rnn_bias,\n        rnn_layers=rnn_layers,\n        noise_channels=noise_channels,\n        decoder_type=decoder_type,\n        decoder_layers=decoder_layers,\n        decoder_size=decoder_size,\n        hidden_to_decoder=hidden_to_decoder,\n        memory_to_decoder=memory_to_decoder,\n        init_proj=init_proj,\n        proj_wn=proj_wn,\n        attn_wn=attn_wn,\n        learn_go_frame=learn_go_frame,\n        pitch_xform=pitch_xform,\n        block_size=block_size,\n        max_batch=max_batch,\n        max_tokens=max_tokens,\n        prior_filter_len=prior_filter_len,\n        tokens_per_frame=tokens_per_frame,\n        attention_type=attention_type,\n        length_reparam=length_reparam\n    )\n\n    if script:\n        self.core = torch.jit.script(self.core)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.alignment_metrics","title":"<code>alignment_metrics(alignments, mask, audio_mask, t=2)</code>","text":"<p>alignments: (B, T_audio, T_text) mask: (B, T_text)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n    \"\"\"\n    alignments: (B, T_audio, T_text)\n    mask: (B, T_text)\n    \"\"\"\n    # TODO: could normalize concentration by logT and subtract from 1,\n    # so it represents a proportion of the entropy 'unused'\n    # then could have a cutoff parameter\n    # alignment should hit every token: max. entropy of mean token probs\n    # alignment should be sharp: min. mean of token entropy\n    concentration = []\n    concentration_norm = []\n    dispersion = []\n    # alternatively:\n    # max average length of token prob vectors, and of time-curve vectors\n    # if using L2, this enourages token energy to concentrate in few token\n    # dimensions per vector, but to spread across multiple time steps \n    # concentration_l2 = []\n    # dispersion_l2 = []\n    for a,mt,ma in zip(alignments, mask, audio_mask):\n        a = a[ma][:,mt]\n        mean_probs = a.mean(0)\n        ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n        # ent_mean = torch.special.entr(mean_probs).sum()\n        concentration.append(ent_mean)\n        concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n        dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n        # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n        # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n        # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n    return {\n        'concentration': torch.stack(concentration),#.mean(),\n        'concentration_norm': torch.stack(concentration_norm),#.mean(),\n        'dispersion': torch.stack(dispersion),#.mean(),\n        # 'concentration_l2': torch.stack(concentration_l2).mean(),\n        # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n    }\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.forward","title":"<code>forward(inputs, audio, mask, audio_mask, audio_lengths=None, prenet_dropout=None, chunk_pad_text=None, chunk_pad_audio=None, temperature=1)</code>","text":"<p>Train Decoder with teacher forcing. Args:     inputs: raw or encoded text.     audio: audio frames for teacher-forcing.     mask: text mask for sequence padding.     audio_mask: audio mask for loss computation.     prenet_dropout: if not None, override original value         (to implement e.g. annealing)     temperature: no effect on training, only on returned output/MSE Shapes:     - inputs:          FloatTensor (B, T_text, D_text)         or LongTensor (B, T_text)     - audio: (B, T_audio, D_audio)     - mask: (B, T_text)     - audio_mask: (B, T_audio)     - stop_target TODO</p> <pre><code>- outputs: (B, T_audio, D_audio)\n- alignments: (B, T_audio, T_text)\n- stop_tokens: (B, T_audio)\n</code></pre> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, inputs, audio, mask, audio_mask,\n        audio_lengths:Optional[Tensor]=None,\n        prenet_dropout:Optional[float]=None,\n        chunk_pad_text:int|None=None,\n        chunk_pad_audio:int|None=None,\n        temperature:float=1\n        ):\n    r\"\"\"Train Decoder with teacher forcing.\n    Args:\n        inputs: raw or encoded text.\n        audio: audio frames for teacher-forcing.\n        mask: text mask for sequence padding.\n        audio_mask: audio mask for loss computation.\n        prenet_dropout: if not None, override original value\n            (to implement e.g. annealing)\n        temperature: no effect on training, only on returned output/MSE\n    Shapes:\n        - inputs: \n            FloatTensor (B, T_text, D_text)\n            or LongTensor (B, T_text)\n        - audio: (B, T_audio, D_audio)\n        - mask: (B, T_text)\n        - audio_mask: (B, T_audio)\n        - stop_target TODO\n\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_audio, T_text)\n        - stop_tokens: (B, T_audio)\n\n    \"\"\"\n    if chunk_pad_audio:\n        audio, audio_mask = self.chunk_pad(\n            audio, audio_mask, chunk_pad_audio)\n\n    if audio_lengths is None:\n        audio_lengths = audio_mask.sum(-1).cpu()\n    ground_truth = audio\n    if prenet_dropout is None:\n        prenet_dropout = self.prenet_dropout\n\n    # print(f'{audio[...,0].min()=}')\n    audio = self.latent_map(audio)\n    # print(f'{audio[...,0].min()=}')\n\n    if inputs.dtype==torch.long:\n        assert self.text_encoder is not None\n        assert inputs.ndim==2\n        inputs = self.text_encoder.encode(inputs, mask)\n    if chunk_pad_text:\n        inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n    (\n        memory, context, alignment,\n        attention_hidden, attention_cell, \n        decoder_hidden, decoder_cell \n    ) = self.core.init_states(inputs)  \n\n    # concat the initial audio frame with training data\n    memories = torch.cat((memory[None], audio.transpose(0, 1)))\n    memories = self.core.prenet(memories, prenet_dropout)\n\n    # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n    hidden, contexts, alignments = self.core.decode_loop(\n        inputs, context, memories, alignment,\n        attention_hidden, attention_cell,\n        mask)\n\n    # compute the additional decoder layers \n    hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n        hidden, contexts, memories[:-1].transpose(0,1),\n        decoder_hidden, decoder_cell, \n        audio_lengths)\n\n    r, outputs = self.run_likelihood(\n        audio, audio_mask, output_params, temperature=temperature)\n\n    stop_loss = None\n    # TODO\n    # stop_tokens = self.predict_stop(hidden, outputs)\n    # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n    outputs = self.latent_unmap(outputs)\n\n    r.update({\n        'text': inputs,\n        # 'stop_loss': stop_loss,\n        'predicted': outputs,\n        'ground_truth': ground_truth,\n        'alignment': alignments,\n        'params': output_params,\n        # 'stop': stop_tokens,\n        'audio_mask': audio_mask,\n        'text_mask': mask,\n        **self.likelihood.metrics(output_params),\n        **self.alignment_metrics(alignments, mask, audio_mask)\n    })\n\n    return r\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.inference","title":"<code>inference(inputs, stop=True, max_steps=None, temperature=1.0, alignments=None, audio=None)</code>","text":"<p>Decoder inference Can supply forced alignments to text; Can also supply audio for teacher forcing,  to test consistency with the training code or implement prompting Args:     inputs: Text Encoder outputs.     stop: use stop gate     max_steps: stop after this many decoder steps     temperature: for the sampling distribution     alignments: forced alignment     audio: forced last-frame         (will be offset, you don't need to prepend a start frame) Shapes:     - inputs: (B, T_text, D_text)     - outputs: (B, T_audio, D_audio)     - alignments: (B, T_text, T_audio)     - audio: (B, T_audio, D_audio)     - stop_tokens: (B, T_audio)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef inference(self, inputs, \n        stop:bool=True, \n        max_steps:Optional[int]=None, \n        temperature:float=1.0,\n        alignments:Optional[Tensor]=None,\n        audio:Optional[Tensor]=None,\n        ):\n    r\"\"\"Decoder inference\n    Can supply forced alignments to text;\n    Can also supply audio for teacher forcing, \n    to test consistency with the training code or implement prompting\n    Args:\n        inputs: Text Encoder outputs.\n        stop: use stop gate\n        max_steps: stop after this many decoder steps\n        temperature: for the sampling distribution\n        alignments: forced alignment\n        audio: forced last-frame\n            (will be offset, you don't need to prepend a start frame)\n    Shapes:\n        - inputs: (B, T_text, D_text)\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_text, T_audio)\n        - audio: (B, T_audio, D_audio)\n        - stop_tokens: (B, T_audio)\n    \"\"\"\n    # max_steps = max_steps or self.max_decoder_steps\n    max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n    self.reset(inputs)\n\n    outputs = []\n    if alignments is None:\n        alignments = []\n        feed_alignment = False\n    else:\n        feed_alignment = True\n\n    for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n        alignment = alignments[:,:,i] if feed_alignment else None\n        audio_frame = (\n            audio[:,i-1,:] \n            if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n            else None)\n        r = self.step(\n            temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n        outputs.append(r['output'])\n        if not feed_alignment:\n            alignments.append(r['alignment'])\n\n        if stop and r['stop_prob']&gt;self.stop_threshold:\n            break\n        if len(outputs) == max_steps:\n            if stop:\n                print(f\"   &gt; Decoder stopped with {max_steps=}\")\n            break\n\n    outputs = torch.stack(outputs, 1)\n    if not feed_alignment:\n        alignments = torch.stack(alignments, 1)\n\n    return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.reset","title":"<code>reset(inputs)</code>","text":"<p>populates buffers with initial states and text inputs</p> <p>call with encoded text, before using <code>step</code></p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>(B, T_text, D_text)</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef reset(self, inputs):\n    r\"\"\"\n    populates buffers with initial states and text inputs\n\n    call with encoded text, before using `step`\n\n    Args:\n        inputs: (B, T_text, D_text)\n\n    \"\"\"\n    self.core.reset(inputs)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.step","title":"<code>step(alignment=None, audio_frame=None, temperature=1.0)</code>","text":"<p>single step of inference.</p> <p>optionally supply <code>alignment</code> to force the alignments. optionally supply <code>audio_frame</code> to set the previous frame.</p> <p>Parameters:</p> Name Type Description Default <code>alignment</code> <code>Optional[Tensor]</code> <p>B x T_text</p> <code>None</code> <code>audio_frame</code> <code>Optional[Tensor]</code> <p>B x D_audio</p> <code>None</code> <code>temperature</code> <code>float</code> <p>optional sampling temperature</p> <code>1.0</code> <p>Returns:     output: B x D_audio     alignment: B x T_text     stop_token: None (not implemented)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef step(self, \n        alignment:Optional[Tensor]=None,\n        audio_frame:Optional[Tensor]=None, \n        temperature:float=1.0\n    ):\n    r\"\"\"\n    single step of inference.\n\n    optionally supply `alignment` to force the alignments.\n    optionally supply `audio_frame` to set the previous frame.\n\n    Args:\n        alignment: B x T_text\n        audio_frame: B x D_audio\n        temperature: optional sampling temperature\n    Returns:\n        output: B x D_audio\n        alignment: B x T_text\n        stop_token: None (not implemented)\n    \"\"\"\n    alignment, output_params = self.core.step_pre(alignment, audio_frame)\n    decoder_output = self.likelihood.sample(\n        output_params, temperature=temperature).squeeze(1)\n\n    decoder_output = self.core.step_post(alignment, decoder_output)\n\n    # if debug:\n    return dict(\n        output=decoder_output,\n        alignment=alignment,\n        stop_prob=torch.tensor(0.),\n        params=output_params\n    )\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.update_kw_dict","title":"<code>update_kw_dict(d)</code>  <code>classmethod</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@classmethod\ndef update_kw_dict(cls, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    b = d.pop('text_bottleneck', None)\n    if b is not None:\n        if d['text_encoder'] is None:\n            d['text_encoder'] = {}\n        d['text_encoder']['bottleneck'] = b\n    return d\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronDecoder.update_state_dict","title":"<code>update_state_dict(d)</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef update_state_dict(self, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    # TODO: core.\n    def replace(old, new):\n        t = d.pop(old, None)\n        if t is not None:\n            d[new] = t\n    replace('go_attention_rnn_cell_state', 'go_attention_cell')\n    replace('go_query', 'go_attention_hidden')\n    # rnncell -&gt; dropoutrnn\n    replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n    replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n    replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n    replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n    # dropoutrnn -&gt; residualrnn\n    replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n    replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n    replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n    replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n    for name in (\n        'go_attention_hidden', 'go_attention_cell',\n        'attention_hidden', 'attention_cell'\n        ):\n        if name in d and d[name].ndim==2:\n            d[name] = d[name][None]\n    # move into core\n    for name in list(d):\n        if any(name.startswith(s) for s in (\n            \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n            replace(name, f'core.{name}')\n\n    # text bottleneck -&gt; text encoder\n    replace('core.text_proj.weight', 'text_encoder.proj.weight')\n    replace('core.text_proj.bias', 'text_encoder.proj.bias')\n    return d\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.TacotronEncoder","title":"<code>TacotronEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tacotron2 style Encoder for comparison with CANINE.</p> <p>Parameters:</p> Name Type Description Default <code>in_out_channels</code> <code>int</code> <p>number of input and output channels.</p> <code>768</code> Shapes <ul> <li>input: LongTensor (B, T)</li> <li>output: (B, T, D)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class TacotronEncoder(nn.Module):\n    r\"\"\"Tacotron2 style Encoder for comparison with CANINE.\n\n    Args:\n        in_out_channels (int): number of input and output channels.\n\n    Shapes:\n        - input: LongTensor (B, T)\n        - output: (B, T, D)\n    \"\"\"\n    def __init__(self, in_out_channels=768, end_tokens=True, conv_blocks=3, rnn=True):\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        # unicode points &gt; 1024 may collide\n        self.n_embeddings = 1025\n        self.embed = nn.Embedding(\n            self.n_embeddings, in_out_channels, padding_idx=0)\n\n        self.convolutions = nn.ModuleList()\n        for _ in range(conv_blocks):\n            self.convolutions.append(ConvBNBlock(\n                in_out_channels, in_out_channels, 5, \"relu\"))\n        if rnn:\n            self.lstm = nn.LSTM(\n                in_out_channels, int(in_out_channels / 2), num_layers=1, batch_first=True, bias=True, bidirectional=True\n            )\n        else:\n            self.lstm = None\n        # self.rnn_state = None\n        self.channels = in_out_channels\n\n    ### this function currently duplicated because torchscript is weird with inheritance\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n\n    @torch.jit.ignore\n    def encode(self, x, mask=None):\n        # if mask is not None:\n            # raise NotImplementedError(\"please implement mask\")\n        input_lengths = (x&gt;0).long().sum(-1)\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        o = self.embed(x%self.n_embeddings).transpose(-1,-2)\n        for layer in self.convolutions:\n            o = o + layer(o, mask)\n        o = o.transpose(-1, -2)\n        if self.lstm is not None:\n            o = nn.utils.rnn.pack_padded_sequence(\n                o, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n            self.lstm.flatten_parameters()\n            o, _ = self.lstm(o)\n            o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n        return o\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.ZeroEncoder","title":"<code>ZeroEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Just character embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>in_out_channels</code> <code>int</code> <p>number of input and output channels.</p> required Shapes <ul> <li>input: LongTensor (B, T)</li> <li>output: (B, T, D)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class ZeroEncoder(nn.Module):\n    r\"\"\"Just character embeddings.\n\n    Args:\n        in_out_channels (int): number of input and output channels.\n\n    Shapes:\n        - input: LongTensor (B, T)\n        - output: (B, T, D)\n    \"\"\"\n    def __init__(self, channels=768, end_tokens=True):\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        # unicode points &gt; 1024 may collide\n        self.n_embeddings = 1025\n        self.embed = nn.Embedding(self.n_embeddings, channels, padding_idx=0)\n\n        self.channels = channels\n\n    ### this function currently duplicated because torchscript is weird with inheritance\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n\n    @torch.jit.ignore\n    def encode(self, x, mask=None):\n        # if mask is not None:\n            # raise NotImplementedError(\"please implement mask\")\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        return self.embed(x%self.n_embeddings)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.deep_update","title":"<code>deep_update(a, b)</code>","text":"<p>in-place update a with contents of b, recursively for nested Mapping objects.</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def deep_update(a, b):\n    \"\"\"\n    in-place update a with contents of b, recursively for nested Mapping objects.\n    \"\"\"\n    for k in b:\n        if k in a and isinstance(a[k], Mapping) and isinstance(b[k], Mapping):\n            deep_update(a[k], b[k])\n        else:\n            a[k] = b[k]\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.get_class_defaults","title":"<code>get_class_defaults(cls)</code>","text":"<p>get the default argument values of a class constructor</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def get_class_defaults(cls):\n    \"\"\"get the default argument values of a class constructor\"\"\"\n    d = get_function_defaults(getattr(cls, '__init__'))\n    # ignore `self` argument, insist on default values\n    try:\n        d.pop('self')\n    except KeyError:\n        raise ValueError(\"\"\"\n            no `self` argument found in class __init__\n        \"\"\")\n    assert [v is not inspect._empty for v in d.values()], \"\"\"\n            get_class_defaults should be used on constructors with keyword arguments only.\n        \"\"\"\n    return d\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.get_function_defaults","title":"<code>get_function_defaults(fn)</code>","text":"<p>get dict of name:default for a function's arguments</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def get_function_defaults(fn):\n    \"\"\"get dict of name:default for a function's arguments\"\"\"\n    s = inspect.signature(fn)\n    return {k:v.default for k,v in s.parameters.items()}\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.lev","title":"<code>lev(t1, t2)</code>  <code>cached</code>","text":"<p>Returns:</p> Type Description <p>levenshtein distance between t1 and t2</p> <p>string representing edits from t1 to t2 - delete + insert ^ edit . no change</p> <p>list giving the corresponding index in t2 for each position in t1</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>@ft.cache\ndef lev(t1, t2):\n    \"\"\"\n    Returns:\n        levenshtein distance between t1 and t2\n        string representing edits from t1 to t2\n            - delete\n            + insert\n            ^ edit\n            . no change\n        list giving the corresponding index in t2 for each position in t1\n    \"\"\"\n    # print(t1, t2)\n    if len(t1)==0:\n        return len(t2), '+'*len(t2), []\n    elif len(t2)==0:\n        return len(t1), '-'*len(t1), [0]*len(t1)\n\n    hd1, tl1 = t1[0], t1[1:]\n    hd2, tl2 = t2[0], t2[1:]\n\n    if hd1==hd2:\n        n, s, i = lev(tl1, tl2)\n        return n, '.'+s, [0]+[j+1 for j in i]\n    (n, s, i), c, f = min(\n        (lev(tl1, tl2), '^', lambda i: [0]+[j+1 for j in i]), \n        (lev(tl1, t2), '-', lambda i: [0]+i), \n        (lev(t1, tl2), '+', lambda i: [j+1 for j in i]))\n    return n+1, c+s, f(i)\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.tokenize","title":"<code>tokenize(text, end_tokens)</code>","text":"<p>Returns:</p> Type Description <p>tokenized text as LongTensor</p> <p>representation of tokenized text as a string</p> <p>list mapping index in the original text to the tokenized text</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def tokenize(text:str, end_tokens:Tuple[int,int]|None):\n    \"\"\"\n    Returns:\n        tokenized text as LongTensor\n        representation of tokenized text as a string\n        list mapping index in the original text to the tokenized text\n    \"\"\"\n    n = len(text)\n\n    start_tok = [end_tokens[0]] if end_tokens else []\n    end_tok = [end_tokens[1]] if end_tokens else []\n    # if len(text)==0:\n    #     return torch.empty((1,0), dtype=torch.long)\n    tok = torch.tensor([\n        start_tok + [ord(char) for char in text] + end_tok\n        ])\n    rep = ' '+text+' ' if end_tokens else text\n    idx_map = list(range(1,n+1) if end_tokens else range(n))\n    return tok, rep, idx_map\n</code></pre>"},{"location":"reference/tungnaa/__init__/#tungnaa.zoneout","title":"<code>zoneout(x1, x2, p, training)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tensor</code> <p>old value</p> required <code>x2</code> <code>Tensor</code> <p>new value</p> required <code>p</code> <code>float</code> <p>prob of keeping old value</p> required <code>training</code> <code>bool</code> <p>stochastic if True, expectation if False</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>def zoneout(x1:Tensor, x2:Tensor, p:float, training:bool):\n    \"\"\"\n    Args:\n        x1: old value\n        x2: new value\n        p: prob of keeping old value\n        training: stochastic if True, expectation if False\n    \"\"\"\n    keep = torch.full_like(x1, p)\n    if training:\n        keep = torch.bernoulli(keep)\n    return torch.lerp(x2, x1, keep)\n</code></pre>"},{"location":"reference/tungnaa/model/","title":"Model","text":""},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormal","title":"<code>DiagonalNormal</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class DiagonalNormal(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        return size * 2\n\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"diagonal normal negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        mu, logsigma = params.chunk(2, -1)\n        logsigma = logsigma.clip(-7, 5) # TODO: clip range\n        loglik = 0.5 * (\n            ((x - mu) / logsigma.exp()) ** 2\n            + self.log2pi\n        ) + logsigma\n        return loglik.sum(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0):\n        mu, logsigma = params.chunk(2, -1)\n        return mu + temperature*logsigma.exp()*torch.randn_like(logsigma)\n\n    def metrics(self, params:Tensor):\n        mu, logsigma = params.chunk(2, -1)\n        return {\n            'logsigma_min': logsigma.min().detach(),\n            'logsigma_median': logsigma.median().detach(),\n            'logsigma_max': logsigma.max().detach(),\n        }\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormal.forward","title":"<code>forward(x, params)</code>","text":"<p>diagonal normal negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor):\n    \"\"\"diagonal normal negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    mu, logsigma = params.chunk(2, -1)\n    logsigma = logsigma.clip(-7, 5) # TODO: clip range\n    loglik = 0.5 * (\n        ((x - mu) / logsigma.exp()) ** 2\n        + self.log2pi\n    ) + logsigma\n    return loglik.sum(-1)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture","title":"<code>DiagonalNormalMixture</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class DiagonalNormalMixture(nn.Module):\n    def __init__(self, n:int=16):\n        \"\"\"n: number of mixture components\"\"\"\n        super().__init__()\n        self.n = n\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        \"\"\"# of distribution parameters as a function of # latent variables\"\"\"\n        return (2 * size + 1) * self.n\n\n    def get_params(self, params:Tensor):\n        \"\"\"\n        Args:\n            params: Tensor[batch, time, n_params] \n        Returns:\n            mu: Tensor[batch, time, n_latent, self.n]\n            logsigma: Tensor[batch, time, n_latent, self.n]\n            logitpi: Tensor[batch, time, self.n]\n        \"\"\"\n        #means, log stddevs, logit weights\n        locscale = params[...,:-self.n]\n        logitpi = params[...,-self.n:]\n\n        mu, logsigma = locscale.view(\n            params.shape[0], params.shape[1], -1, self.n, 2).unbind(-1)\n        # mu, logsigma = locscale.view(*params.shape[:-1], -1, self.n, 2).unbind(-1)\n        return mu, logsigma, logitpi\n\n    def forward(self, x:Tensor, params:Tensor, mode:str='normal'):\n        \"\"\"mixture of diagonal normals negative log likelihood.\n        should broadcast any number of leading dimensions\n        Args:\n            x: Tensor[batch, time, latent] or [..., latent]\n            params: Tensor[batch, time, n_params] or [..., n_params]\n        Return:\n            negative log likelihood: Tensor[batch, time] or [...]\n        \"\"\"\n        x = x[...,None] # broadcast against mixture component\n        mu, logsigma, logitpi = self.get_params(params)\n\n        if mode == 'fixed':\n            logsigma = torch.zeros_like(logsigma)\n        elif mode == 'posthoc':\n            logsigma = (x - mu).abs().log()\n        elif mode != 'normal':\n            raise ValueError(mode)\n\n        logsigma = logsigma.clip(-7, 5) # TODO: clip range?\n        # cmp_loglik = -0.5 * (\n        #     ((x - mu) / logsigma.exp()) ** 2\n        #     + log2pi\n        # ) - logsigma\n        # logpi = logitpi.log_softmax(-1)\n        # return -(cmp_loglik.sum(-2) + logpi).logsumexp(-1)\n        cmp_loglik = (\n            (x - mu) # materialize huge tensor when calling from sample_n\n            / logsigma.exp()) ** 2 \n\n        # reduce latent dim\n        # reduce before summing these components for memory reasons\n        cmp_loglik = -0.5 * (\n            (cmp_loglik.sum(-2) + self.log2pi*x.shape[-2]) \n            ) - logsigma.sum(-2)\n\n        logpi = logitpi.log_softmax(-1)\n        return -(cmp_loglik + logpi).logsumexp(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0, nsamp:Optional[int]=None):\n        # print(params.shape, temperature, nsamp)\n        if temperature != 1:\n            # return self.sample_n(params, temperature, nsamp=128)\n            if nsamp is not None:\n                return self.sample_n(params, temperature, nsamp=nsamp)\n            else:\n                return self.sample_components(params, temperature)\n        else:\n            mu, logsigma, logitpi = self.get_params(params)\n            # idx = torch.distributions.Categorical(\n                # logits=logitpi).sample()[...,None,None].expand(*mu.shape[:-1],1)\n            idx = categorical_sample_2d(logitpi, 1)[...,None].expand(\n                    mu.shape[0], mu.shape[1], mu.shape[2], 1)\n            mu = mu.gather(-1, idx).squeeze(-1)\n            logsigma = logsigma.gather(-1, idx).squeeze(-1)\n            return mu + logsigma.exp()*torch.randn_like(logsigma)\n            # return mu + temperature*logsigma.exp()*torch.randn_like(logsigma)\n\n    def sample_n(self, params:Tensor, temperature:float=1.0, nsamp:int=128):\n        \"\"\"\n        draw nsamp samples,\n        rerank and sample categorical with temperature\n        Args:\n            params: Tensor[batch, time, n_params]    \n        \"\"\"\n         # sample N \n        mu, logsigma, logitpi = self.get_params(params)\n        # logitpi = logitpi[...,None,:].expand(*logitpi.shape[:-1],nsamp,-1)\n        # logitpi = logitpi[...,None,:].expand(\n            # logitpi.shape[0],logitpi.shape[1],nsamp,-1)\n        # ..., nsamp, self.n\n        # print(f'{logitpi.shape=}')\n        # idx = torch.distributions.Categorical(logits=logitpi).sample()\n        # idx = torch.multinomial(\n            # logitpi.reshape(-1, logitpi.shape[-1]).exp(), nsamp, replacement=True\n            # ).reshape(logitpi.shape[0], logitpi.shape[1], nsamp)\n        idx = categorical_sample_2d(logitpi, nsamp)\n        # ..., nsamp\n        # print(f'{idx.shape=}')\n        # idx = idx[...,None,:].expand(*mu.shape[:-1], -1)\n        idx = idx[...,None,:].expand(mu.shape[0], mu.shape[1], mu.shape[2], -1)\n        # ..., latent, nsamp \n        # print(f'{idx.shape=}')\n\n        # mu is: ..., latent, self.n\n        mu = mu.gather(-1, idx)\n        logsigma = logsigma.gather(-1, idx)\n        # ..., latent, nsamp\n        # print(f'{mu.shape=}')\n\n        samps = (mu + torch.randn_like(mu)*logsigma.exp()).moveaxis(-1, 0)\n        # nsamp,...,latent\n        # print(f'{samps.shape=}')\n\n        # compute nll\n        # here there is a extra first dimension (nsamp)\n        # which broadcasts against the distribution params inside of self.forward,\n        # to compute the nll for each sample\n        nll = self(samps, params).moveaxis(0, -1)\n        # ...,nsamp\n        # print(f'{nll.shape=}')\n        # print(f'{nll=}')\n\n        # sample categorical with temperature\n        # idx = torch.distributions.Categorical(\n        #     logits=-nll/(temperature+1e-5)).sample()\n        # ...\n        idx = categorical_sample_2d(-nll/(temperature+1e-5), 1)\n        # print(f'{idx.shape=}')\n        # ...,1\n\n        # select\n        # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n        # 1,...,latent\n        idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n        # 1,...,latent\n        # print(f'{idx.shape=}')\n        samp = samps.gather(0, idx).squeeze(0)\n        # ...,latent\n        # print(f'{samp.shape=}')\n        # print(f'{samp=}')\n        return samp\n\n    def sample_components(self, params:Tensor, temperature:float=1.0):\n        \"\"\"\n        sample every mixture component with temperature,\n        rerank and sample categorical with temperature.\n        \"\"\"\n        # sample each component with temperature \n        mu, logsigma, _ = self.get_params(params)\n        samps = mu + torch.randn_like(mu)*logsigma.exp()*temperature**0.5\n        # ..., latent, self.n\n        samps = samps.moveaxis(-1, 0)\n        # self.n ...latent\n\n        # compute nll for each sample\n        # here there is an extra first dimension (nsamp)\n        # which broadcasts against the distribution params inside of self.forward\n        # to compute the nll for each sample\n        nll = self(samps, params).moveaxis(0, -1)\n        # ..., self.n\n\n        # sample categorical with temperature\n        if temperature &gt; 1e-5:\n            logits = nll.mul_(-1/temperature**0.5)\n            # idx = torch.distributions.Categorical(logits=logits).sample()\n            idx = categorical_sample_2d(logits, 1)\n        else:\n            idx = nll.argmin(-1)[...,None]\n        # print(f'{idx.shape=}')\n        # ...\n\n        # select\n        # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n        idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n\n        # 1,...,latent\n        # print(f'{idx.shape=}')\n        samp = samps.gather(0, idx).squeeze(0)\n        # ...,latent\n        # print(f'{samp.shape=}')\n        # print(samp.shape)\n        # print(f'{samp=}')\n        return samp\n\n    @torch.jit.ignore\n    def metrics(self, params:Tensor):\n        mu, logsigma, logitpi = self.get_params(params)\n        ent = torch.distributions.Categorical(logits=logitpi).entropy().detach()\n        return {\n            'logsigma_min': logsigma.min().detach(),\n            'logsigma_median': logsigma.median().detach(),\n            'logsigma_max': logsigma.max().detach(),\n            'pi_entropy_min': ent.min(),\n            'pi_entropy_median': ent.median(),\n            'pi_entropy_max': ent.max(),\n        } \n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.__init__","title":"<code>__init__(n=16)</code>","text":"<p>n: number of mixture components</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(self, n:int=16):\n    \"\"\"n: number of mixture components\"\"\"\n    super().__init__()\n    self.n = n\n    self.register_buffer(\n        'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.forward","title":"<code>forward(x, params, mode='normal')</code>","text":"<p>mixture of diagonal normals negative log likelihood. should broadcast any number of leading dimensions Args:     x: Tensor[batch, time, latent] or [..., latent]     params: Tensor[batch, time, n_params] or [..., n_params] Return:     negative log likelihood: Tensor[batch, time] or [...]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor, mode:str='normal'):\n    \"\"\"mixture of diagonal normals negative log likelihood.\n    should broadcast any number of leading dimensions\n    Args:\n        x: Tensor[batch, time, latent] or [..., latent]\n        params: Tensor[batch, time, n_params] or [..., n_params]\n    Return:\n        negative log likelihood: Tensor[batch, time] or [...]\n    \"\"\"\n    x = x[...,None] # broadcast against mixture component\n    mu, logsigma, logitpi = self.get_params(params)\n\n    if mode == 'fixed':\n        logsigma = torch.zeros_like(logsigma)\n    elif mode == 'posthoc':\n        logsigma = (x - mu).abs().log()\n    elif mode != 'normal':\n        raise ValueError(mode)\n\n    logsigma = logsigma.clip(-7, 5) # TODO: clip range?\n    # cmp_loglik = -0.5 * (\n    #     ((x - mu) / logsigma.exp()) ** 2\n    #     + log2pi\n    # ) - logsigma\n    # logpi = logitpi.log_softmax(-1)\n    # return -(cmp_loglik.sum(-2) + logpi).logsumexp(-1)\n    cmp_loglik = (\n        (x - mu) # materialize huge tensor when calling from sample_n\n        / logsigma.exp()) ** 2 \n\n    # reduce latent dim\n    # reduce before summing these components for memory reasons\n    cmp_loglik = -0.5 * (\n        (cmp_loglik.sum(-2) + self.log2pi*x.shape[-2]) \n        ) - logsigma.sum(-2)\n\n    logpi = logitpi.log_softmax(-1)\n    return -(cmp_loglik + logpi).logsumexp(-1)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.get_params","title":"<code>get_params(params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>params</code> <code>Tensor</code> <p>Tensor[batch, time, n_params] </p> required <p>Returns:     mu: Tensor[batch, time, n_latent, self.n]     logsigma: Tensor[batch, time, n_latent, self.n]     logitpi: Tensor[batch, time, self.n]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def get_params(self, params:Tensor):\n    \"\"\"\n    Args:\n        params: Tensor[batch, time, n_params] \n    Returns:\n        mu: Tensor[batch, time, n_latent, self.n]\n        logsigma: Tensor[batch, time, n_latent, self.n]\n        logitpi: Tensor[batch, time, self.n]\n    \"\"\"\n    #means, log stddevs, logit weights\n    locscale = params[...,:-self.n]\n    logitpi = params[...,-self.n:]\n\n    mu, logsigma = locscale.view(\n        params.shape[0], params.shape[1], -1, self.n, 2).unbind(-1)\n    # mu, logsigma = locscale.view(*params.shape[:-1], -1, self.n, 2).unbind(-1)\n    return mu, logsigma, logitpi\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.n_params","title":"<code>n_params(size)</code>","text":""},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.n_params--of-distribution-parameters-as-a-function-of-latent-variables","title":"of distribution parameters as a function of # latent variables","text":"Source code in <code>src/tungnaa/model.py</code> <pre><code>def n_params(self, size):\n    \"\"\"# of distribution parameters as a function of # latent variables\"\"\"\n    return (2 * size + 1) * self.n\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.sample_components","title":"<code>sample_components(params, temperature=1.0)</code>","text":"<p>sample every mixture component with temperature, rerank and sample categorical with temperature.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def sample_components(self, params:Tensor, temperature:float=1.0):\n    \"\"\"\n    sample every mixture component with temperature,\n    rerank and sample categorical with temperature.\n    \"\"\"\n    # sample each component with temperature \n    mu, logsigma, _ = self.get_params(params)\n    samps = mu + torch.randn_like(mu)*logsigma.exp()*temperature**0.5\n    # ..., latent, self.n\n    samps = samps.moveaxis(-1, 0)\n    # self.n ...latent\n\n    # compute nll for each sample\n    # here there is an extra first dimension (nsamp)\n    # which broadcasts against the distribution params inside of self.forward\n    # to compute the nll for each sample\n    nll = self(samps, params).moveaxis(0, -1)\n    # ..., self.n\n\n    # sample categorical with temperature\n    if temperature &gt; 1e-5:\n        logits = nll.mul_(-1/temperature**0.5)\n        # idx = torch.distributions.Categorical(logits=logits).sample()\n        idx = categorical_sample_2d(logits, 1)\n    else:\n        idx = nll.argmin(-1)[...,None]\n    # print(f'{idx.shape=}')\n    # ...\n\n    # select\n    # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n    idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n\n    # 1,...,latent\n    # print(f'{idx.shape=}')\n    samp = samps.gather(0, idx).squeeze(0)\n    # ...,latent\n    # print(f'{samp.shape=}')\n    # print(samp.shape)\n    # print(f'{samp=}')\n    return samp\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.DiagonalNormalMixture.sample_n","title":"<code>sample_n(params, temperature=1.0, nsamp=128)</code>","text":"<p>draw nsamp samples, rerank and sample categorical with temperature Args:     params: Tensor[batch, time, n_params]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def sample_n(self, params:Tensor, temperature:float=1.0, nsamp:int=128):\n    \"\"\"\n    draw nsamp samples,\n    rerank and sample categorical with temperature\n    Args:\n        params: Tensor[batch, time, n_params]    \n    \"\"\"\n     # sample N \n    mu, logsigma, logitpi = self.get_params(params)\n    # logitpi = logitpi[...,None,:].expand(*logitpi.shape[:-1],nsamp,-1)\n    # logitpi = logitpi[...,None,:].expand(\n        # logitpi.shape[0],logitpi.shape[1],nsamp,-1)\n    # ..., nsamp, self.n\n    # print(f'{logitpi.shape=}')\n    # idx = torch.distributions.Categorical(logits=logitpi).sample()\n    # idx = torch.multinomial(\n        # logitpi.reshape(-1, logitpi.shape[-1]).exp(), nsamp, replacement=True\n        # ).reshape(logitpi.shape[0], logitpi.shape[1], nsamp)\n    idx = categorical_sample_2d(logitpi, nsamp)\n    # ..., nsamp\n    # print(f'{idx.shape=}')\n    # idx = idx[...,None,:].expand(*mu.shape[:-1], -1)\n    idx = idx[...,None,:].expand(mu.shape[0], mu.shape[1], mu.shape[2], -1)\n    # ..., latent, nsamp \n    # print(f'{idx.shape=}')\n\n    # mu is: ..., latent, self.n\n    mu = mu.gather(-1, idx)\n    logsigma = logsigma.gather(-1, idx)\n    # ..., latent, nsamp\n    # print(f'{mu.shape=}')\n\n    samps = (mu + torch.randn_like(mu)*logsigma.exp()).moveaxis(-1, 0)\n    # nsamp,...,latent\n    # print(f'{samps.shape=}')\n\n    # compute nll\n    # here there is a extra first dimension (nsamp)\n    # which broadcasts against the distribution params inside of self.forward,\n    # to compute the nll for each sample\n    nll = self(samps, params).moveaxis(0, -1)\n    # ...,nsamp\n    # print(f'{nll.shape=}')\n    # print(f'{nll=}')\n\n    # sample categorical with temperature\n    # idx = torch.distributions.Categorical(\n    #     logits=-nll/(temperature+1e-5)).sample()\n    # ...\n    idx = categorical_sample_2d(-nll/(temperature+1e-5), 1)\n    # print(f'{idx.shape=}')\n    # ...,1\n\n    # select\n    # idx = idx[None,...,None].expand(1, *samps.shape[1:])\n    # 1,...,latent\n    idx = idx[None,...].expand(1, samps.shape[1], samps.shape[2], samps.shape[3])\n    # 1,...,latent\n    # print(f'{idx.shape=}')\n    samp = samps.gather(0, idx).squeeze(0)\n    # ...,latent\n    # print(f'{samp.shape=}')\n    # print(f'{samp=}')\n    return samp\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.GED","title":"<code>GED</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generalized energy distance (pseudo) likelihood</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class GED(nn.Module):\n    \"\"\"Generalized energy distance (pseudo) likelihood\"\"\"\n    def __init__(self, \n            latent_size,\n            hidden_size=256,\n            hidden_layers=3,\n            dropout=None,\n            unfold=None,\n            multiply_params=False,\n            project_params=None,\n            glu=False,\n            ):\n        super().__init__()\n        self.latent_size = latent_size\n        self.hidden_size = hidden_size\n        self.unfold = unfold\n        self.multiply_params = multiply_params\n        self.project_params = project_params\n\n        layers = []\n        norm = nn.utils.parametrizations.weight_norm \n        if glu:\n            act = nn.GLU\n            for _ in range(hidden_layers):\n                block = [norm(nn.Linear(hidden_size, 2*hidden_size))]\n                block.append(act())\n                if dropout:\n                    block.append(nn.Dropout(dropout))\n                layers.append(Residual(*block))\n            layers.append(norm(nn.Linear(hidden_size, latent_size)))\n        else:\n            act = nn.GELU\n            for _ in range(hidden_layers):\n                block = [nn.Dropout(dropout)] if dropout else []\n                block.append(act())\n                block.append(norm(nn.Linear(hidden_size, hidden_size)))\n                layers.append(Residual(*block))\n            layers.extend([\n                act(),\n                norm(nn.Linear(hidden_size, latent_size))\n                ])\n\n        self.net = nn.Sequential(*layers)\n\n    def n_params(self, size):\n        assert size == self.latent_size\n        if self.project_params:\n            p = self.project_params\n            return self.hidden_size + p*(p-1)\n        elif self.multiply_params:\n            return self.hidden_size\n        else:\n            return self.hidden_size//2\n\n    @torch.jit.ignore\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"compute generalized energy distance\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]\n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        # bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n        # params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n        # x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n        params = torch.cat((params, params), 0)\n        y = self.net(self.add_noise(params))\n        if self.unfold:\n            zeros = y.new_zeros(*y.shape[:-2],self.unfold-1,y.shape[-1])\n            y = torch.cat((zeros, y), -2)\n            y = y.unfold(dimension=-2, size=self.unfold, step=1)\n            zeros = x.new_zeros(*x.shape[:-2],self.unfold-1,x.shape[-1])\n            x = torch.cat((zeros, x), -2)\n            x = x.unfold(dimension=-2, size=self.unfold, step=1)\n            dim = (-1,-2)\n        else:\n            dim = -1\n        y1, y2 = y.chunk(2, 0)\n        return (\n            torch.linalg.vector_norm(x-y1, dim=dim)\n            + torch.linalg.vector_norm(x-y2, dim=dim)\n            - torch.linalg.vector_norm(y2-y1, dim=dim)\n            )\n\n    # @torch.jit.ignore\n    def sample(self, params, temperature:float=1.0):\n        return self.net(self.add_noise(params, temperature))\n\n    def add_noise(self, params, temperature:float=1):\n        if self.project_params is not None:\n            p = self.project_params\n            w, params = params.split((p**2),-1)\n            noise = torch.randn(\n                *params.shape[:2],1,p, device=params.device, dtype=params.dtype)\n            w = w.reshape(*w.shape[:2],p,p)\n            noise = (noise @ w).squeeze(-2)\n        else:\n            if self.multiply_params:\n                params, w = params.chunk(2,-1)\n                noise = w*torch.randn_like(params)*temperature\n            else:\n                noise = torch.randn_like(params)*temperature\n\n        return torch.cat((params, noise), -1)\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.GED.forward","title":"<code>forward(x, params)</code>","text":"<p>compute generalized energy distance Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params] Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, x:Tensor, params:Tensor):\n    \"\"\"compute generalized energy distance\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]\n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    # bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n    # params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n    # x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n    params = torch.cat((params, params), 0)\n    y = self.net(self.add_noise(params))\n    if self.unfold:\n        zeros = y.new_zeros(*y.shape[:-2],self.unfold-1,y.shape[-1])\n        y = torch.cat((zeros, y), -2)\n        y = y.unfold(dimension=-2, size=self.unfold, step=1)\n        zeros = x.new_zeros(*x.shape[:-2],self.unfold-1,x.shape[-1])\n        x = torch.cat((zeros, x), -2)\n        x = x.unfold(dimension=-2, size=self.unfold, step=1)\n        dim = (-1,-2)\n    else:\n        dim = -1\n    y1, y2 = y.chunk(2, 0)\n    return (\n        torch.linalg.vector_norm(x-y1, dim=dim)\n        + torch.linalg.vector_norm(x-y2, dim=dim)\n        - torch.linalg.vector_norm(y2-y1, dim=dim)\n        )\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.GaussianAttention","title":"<code>GaussianAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>More restrictive version of dynamic convolutional attention. the convolution is always a difference of gaussians (diffusion / unsharp mask) with a nonnegative shift.</p> <p>Parameters:</p> Name Type Description Default <code>query_dim</code> <code>int</code> <p>number of channels in the query tensor.</p> required <code>embedding_dim</code> <code>int</code> <p>number of channels in the value tensor.</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>class GaussianAttention(nn.Module):\n    \"\"\"\n    More restrictive version of dynamic convolutional attention.\n    the convolution is always a difference of gaussians (diffusion / unsharp mask)\n    with a nonnegative shift.\n\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the value tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim,\n        filter_size:int=21,\n        min_sigma:float=0.2,\n        prior_tokens_per_frame:float=1.0,\n        wn=False,\n    ):\n        super().__init__()\n        self._mask_value = 1e-8\n\n        norm = nn.utils.parametrizations.weight_norm if wn else lambda x:x\n        self.proj = norm(nn.Linear(query_dim, 4))\n        self.filter_size = filter_size\n        self.min_sigma = min_sigma\n        self.prior_tokens_per_frame = prior_tokens_per_frame\n\n        self.register_buffer(\n            'k', torch.arange(filter_size)[:, None, None], persistent=False)\n\n    def align(self, \n            query, attention_weights, \n            inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n        \"\"\"\n        Args:\n            query: [B, D_attn_rnn]\n            attention_weights: [B, T_text]\n            inputs: [B, T_text, D_text]\n            mask: [B, T_text]\n        Returns:\n            attention_weights: [B, T_text]\n        \"\"\"\n        B = query.size(0)\n\n        mu, alpha, sigma = F.sigmoid(self.proj(query)).split((1,1,2),dim=1)\n        # mu, alpha, sigma = F.softplus(self.proj(query)).split((1,1,2),dim=1)\n        # mu = self.filter_size//2 - mu * self.prior_tokens_per_frame# shift\n        mu = self.filter_size//2 - mu * (2 * self.prior_tokens_per_frame)# shift\n        # alpha, beta = 1+alpha, -alpha # diffuse weight, unsharp weight\n        sigma = self.min_sigma + sigma.cumsum(-1) # second more dispersed than first\n\n        # NOTE: may be able to compute k using torch.special.ndtr + diff?\n\n        k = self.k\n        # k = torch.arange(\n        #     self.filter_size, \n        #     device=query.device, dtype=query.dtype\n        #     )[:, None, None] # [K, 1, 1]\n\n        # g = (-(k-mu)**2/(2*sigma**2)).exp()/sigma\n        g = (-((k-mu)/(2*sigma))**2).exp()/sigma\n\n        alpha = alpha.T\n        k = (1+alpha)*g[...,0] - alpha*g[...,1] # [K, B]\n        k = k/k.sum(0) # normalize\n        k = k.T # [B, K]\n\n        attention_weights = F.conv1d(\n            attention_weights[None], k[:,None], \n            groups=B, padding=self.filter_size//2\n        ).squeeze(0)\n\n        if mask is not None:\n            attention_weights = attention_weights.masked_fill(\n                ~mask, self._mask_value)\n\n        attention_weights = attention_weights.clip(self._mask_value)\n\n        attention_weights = (\n            attention_weights / attention_weights.sum(-1, keepdim=True))\n\n        return attention_weights\n\n    def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n        \"\"\"this is split out to implement attention painting\n        \"\"\"\n        # do weights really need to be masked here if it's post-softmax anyway?\n        # is it enough that the inputs are zero padded?\n        # print(attention_weights.shape, inputs.shape)\n        # apply masking\n        # if mask is not None:\n        #     if torch.is_grad_enabled():\n        #         attention_weights = attention_weights.masked_fill(\n        #             ~mask, self._mask_value)\n        #     else:\n        #         attention_weights.masked_fill_(~mask, self._mask_value)\n        context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n        return context\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        attention_weights = inputs.new_zeros(B, T)\n        attention_weights[:, 0] = 1.0\n        return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.GaussianAttention.align","title":"<code>align(query, attention_weights, inputs=None, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <p>[B, D_attn_rnn]</p> required <code>attention_weights</code> <p>[B, T_text]</p> required <code>inputs</code> <code>Optional[Tensor]</code> <p>[B, T_text, D_text]</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>[B, T_text]</p> <code>None</code> <p>Returns:     attention_weights: [B, T_text]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def align(self, \n        query, attention_weights, \n        inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n    \"\"\"\n    Args:\n        query: [B, D_attn_rnn]\n        attention_weights: [B, T_text]\n        inputs: [B, T_text, D_text]\n        mask: [B, T_text]\n    Returns:\n        attention_weights: [B, T_text]\n    \"\"\"\n    B = query.size(0)\n\n    mu, alpha, sigma = F.sigmoid(self.proj(query)).split((1,1,2),dim=1)\n    # mu, alpha, sigma = F.softplus(self.proj(query)).split((1,1,2),dim=1)\n    # mu = self.filter_size//2 - mu * self.prior_tokens_per_frame# shift\n    mu = self.filter_size//2 - mu * (2 * self.prior_tokens_per_frame)# shift\n    # alpha, beta = 1+alpha, -alpha # diffuse weight, unsharp weight\n    sigma = self.min_sigma + sigma.cumsum(-1) # second more dispersed than first\n\n    # NOTE: may be able to compute k using torch.special.ndtr + diff?\n\n    k = self.k\n    # k = torch.arange(\n    #     self.filter_size, \n    #     device=query.device, dtype=query.dtype\n    #     )[:, None, None] # [K, 1, 1]\n\n    # g = (-(k-mu)**2/(2*sigma**2)).exp()/sigma\n    g = (-((k-mu)/(2*sigma))**2).exp()/sigma\n\n    alpha = alpha.T\n    k = (1+alpha)*g[...,0] - alpha*g[...,1] # [K, B]\n    k = k/k.sum(0) # normalize\n    k = k.T # [B, K]\n\n    attention_weights = F.conv1d(\n        attention_weights[None], k[:,None], \n        groups=B, padding=self.filter_size//2\n    ).squeeze(0)\n\n    if mask is not None:\n        attention_weights = attention_weights.masked_fill(\n            ~mask, self._mask_value)\n\n    attention_weights = attention_weights.clip(self._mask_value)\n\n    attention_weights = (\n        attention_weights / attention_weights.sum(-1, keepdim=True))\n\n    return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.GaussianAttention.apply","title":"<code>apply(attention_weights, inputs, mask=None)</code>","text":"<p>this is split out to implement attention painting</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n    \"\"\"this is split out to implement attention painting\n    \"\"\"\n    # do weights really need to be masked here if it's post-softmax anyway?\n    # is it enough that the inputs are zero padded?\n    # print(attention_weights.shape, inputs.shape)\n    # apply masking\n    # if mask is not None:\n    #     if torch.is_grad_enabled():\n    #         attention_weights = attention_weights.masked_fill(\n    #             ~mask, self._mask_value)\n    #     else:\n    #         attention_weights.masked_fill_(~mask, self._mask_value)\n    context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n    return context\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.MLPDecoder","title":"<code>MLPDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class MLPDecoder(nn.Module):\n    def __init__(self, in_size, size, num_layers=4, \n            bias=True, batch_first=True, \n            dropout=0.1):\n        super().__init__()\n        assert batch_first\n\n        norm = nn.utils.parametrizations.weight_norm \n        net = [norm(nn.Linear(in_size, size, bias))]\n        for _ in range(num_layers):\n            block = []\n            if dropout:\n                block.append(nn.Dropout(dropout))\n            block.append(nn.LeakyReLU(0.2))\n            block.append(norm(nn.Linear(size, size, bias)))\n            net.append(Residual(*block))\n\n        net.append(nn.LeakyReLU(0.2))\n        self.net = nn.Sequential(*net)\n        # self.net = torch.jit.script(nn.Sequential(*net))\n\n    def forward(self, x, states):\n        \"\"\"pseudo-RNN forward\"\"\"\n        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n            h = x.data\n        else:\n            h = x\n        h = self.net(h)\n        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n            x = torch.nn.utils.rnn.PackedSequence(h, *x[1:])\n        else:\n            x = h\n        return x, states\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.MLPDecoder.forward","title":"<code>forward(x, states)</code>","text":"<p>pseudo-RNN forward</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x, states):\n    \"\"\"pseudo-RNN forward\"\"\"\n    if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n        h = x.data\n    else:\n        h = x\n    h = self.net(h)\n    if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n        x = torch.nn.utils.rnn.PackedSequence(h, *x[1:])\n    else:\n        x = h\n    return x, states\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.MonotonicDynamicConvolutionAttention","title":"<code>MonotonicDynamicConvolutionAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Dynamic convolution attention from https://arxiv.org/pdf/1910.10288.pdf</p> <p>This is an altered version where the static filter is replaced by the bias  of the linear layer leading to the dynamic filter</p> <p>original docstring follows:</p> <p>query -&gt; linear -&gt; tanh -&gt; linear -&gt;|                                     |                                            mask values                                     v                                              |    |            atten_w(t-1) -|-&gt; conv1d_dynamic -&gt; linear -|-&gt; tanh -&gt; + -&gt; softmax -&gt; * -&gt; * -&gt; context                          |-&gt; conv1d_static  -&gt; linear -|           |                          |-&gt; conv1d_prior   -&gt; log ----------------| query: attention rnn output. Note:     Dynamic convolution attention is an alternation of the location senstive attention with dynamically computed convolution filters from the previous attention scores and a set of constraints to keep the attention alignment diagonal.     DCA is sensitive to mixed precision training and might cause instable training. Args:     query_dim (int): number of channels in the query tensor.     embedding_dim (int): number of channels in the value tensor.     static_filter_dim (int): number of channels in the convolution layer computing the static filters.     static_kernel_size (int): kernel size for the convolution layer computing the static filters.     dynamic_filter_dim (int): number of channels in the convolution layer computing the dynamic filters.     dynamic_kernel_size (int): kernel size for the convolution layer computing the dynamic filters.     prior_filter_len (int, optional): [description]. Defaults to 11 from the paper.     alpha (float, optional): [description]. Defaults to 0.1 from the paper.     beta (float, optional): [description]. Defaults to 0.9 from the paper.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class MonotonicDynamicConvolutionAttention(nn.Module):\n    \"\"\"Dynamic convolution attention from\n    https://arxiv.org/pdf/1910.10288.pdf\n\n    This is an altered version where the static filter is replaced by the bias \n    of the linear layer leading to the dynamic filter\n\n    original docstring follows:\n\n    query -&gt; linear -&gt; tanh -&gt; linear -&gt;|\n                                        |                                            mask values\n                                        v                                              |    |\n               atten_w(t-1) -|-&gt; conv1d_dynamic -&gt; linear -|-&gt; tanh -&gt; + -&gt; softmax -&gt; * -&gt; * -&gt; context\n                             |-&gt; conv1d_static  -&gt; linear -|           |\n                             |-&gt; conv1d_prior   -&gt; log ----------------|\n    query: attention rnn output.\n    Note:\n        Dynamic convolution attention is an alternation of the location senstive attention with\n    dynamically computed convolution filters from the previous attention scores and a set of\n    constraints to keep the attention alignment diagonal.\n        DCA is sensitive to mixed precision training and might cause instable training.\n    Args:\n        query_dim (int): number of channels in the query tensor.\n        embedding_dim (int): number of channels in the value tensor.\n        static_filter_dim (int): number of channels in the convolution layer computing the static filters.\n        static_kernel_size (int): kernel size for the convolution layer computing the static filters.\n        dynamic_filter_dim (int): number of channels in the convolution layer computing the dynamic filters.\n        dynamic_kernel_size (int): kernel size for the convolution layer computing the dynamic filters.\n        prior_filter_len (int, optional): [description]. Defaults to 11 from the paper.\n        alpha (float, optional): [description]. Defaults to 0.1 from the paper.\n        beta (float, optional): [description]. Defaults to 0.9 from the paper.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim,\n        attention_dim,\n        static_filter_dim=8, # unused\n        static_kernel_size=21, # unused\n        dynamic_filter_dim=8,\n        dynamic_kernel_size=21,\n        prior_filter_len=11,\n        alpha=0.1,\n        beta=0.9,\n    ):\n        super().__init__()\n        self._mask_value = 1e-8\n        self.dynamic_filter_dim = dynamic_filter_dim\n        self.dynamic_kernel_size = dynamic_kernel_size\n        self.prior_filter_len = prior_filter_len\n        # setup key and query layers\n        dynamic_weight_dim = dynamic_filter_dim * dynamic_kernel_size\n        # self.filter_mlp = torch.jit.script(nn.Sequential(\n        self.filter_mlp = (nn.Sequential(\n            nn.Linear(query_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, dynamic_weight_dim)#, bias=False)\n        ))\n\n        # self.post_mlp = torch.jit.script(nn.Sequential(\n        self.post_mlp = (nn.Sequential(\n            nn.Linear(dynamic_filter_dim, attention_dim),\n            nn.Tanh(),\n            nn.Linear(attention_dim, 1, bias=False)\n        ))\n\n        self.register_buffer(\n            \"prior\", prior_coefs(prior_filter_len, alpha, beta), \n            persistent=False)\n\n    def align(self, \n            query, attention_weights, \n            inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n        \"\"\"\n        Args:\n            query: [B, D_attn_rnn]\n            attention_weights: [B, T_text]\n            inputs: [B, T_text, D_text]\n            mask: [B, T_text]\n        Returns:\n            attention_weights: [B, T_text]\n        \"\"\"\n        B = query.shape[0]\n\n        prior_filter = do_prior_filter(\n            attention_weights, self.prior, self.prior_filter_len-1)\n\n        G = self.filter_mlp(query)\n        # compute dynamic filters\n        pad = (self.dynamic_kernel_size - 1) // 2\n        dynamic_filter = F.conv1d(\n            attention_weights[None],\n            G.view(-1, 1, self.dynamic_kernel_size),\n            padding=pad,\n            groups=B,\n        )\n        dynamic_filter = dynamic_filter.view(\n            B, self.dynamic_filter_dim, -1).transpose(1, 2)\n\n        attention_weights = addsoftmax(\n            self.post_mlp(dynamic_filter).squeeze(-1), prior_filter)\n\n        return attention_weights\n\n    def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n        \"\"\"this is split out to implement attention painting\n\n        do weights really need to be masked here if it's post-softmax anyway?\n        is it enough that the inputs are zero padded?\n        \"\"\"\n        # print(attention_weights.shape, inputs.shape)\n        # apply masking\n        if mask is not None:\n            if torch.is_grad_enabled():\n                attention_weights = attention_weights.masked_fill(\n                    ~mask, self._mask_value)\n            else:\n                attention_weights.masked_fill_(~mask, self._mask_value)\n        context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n        return context\n\n    def init_states(self, inputs):\n        B = inputs.size(0)\n        T = inputs.size(1)\n        attention_weights = inputs.new_zeros(B, T)\n        attention_weights[:, 0] = 1.0\n        return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.MonotonicDynamicConvolutionAttention.align","title":"<code>align(query, attention_weights, inputs=None, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <p>[B, D_attn_rnn]</p> required <code>attention_weights</code> <p>[B, T_text]</p> required <code>inputs</code> <code>Optional[Tensor]</code> <p>[B, T_text, D_text]</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>[B, T_text]</p> <code>None</code> <p>Returns:     attention_weights: [B, T_text]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def align(self, \n        query, attention_weights, \n        inputs:Optional[Tensor]=None, mask:Optional[Tensor]=None):\n    \"\"\"\n    Args:\n        query: [B, D_attn_rnn]\n        attention_weights: [B, T_text]\n        inputs: [B, T_text, D_text]\n        mask: [B, T_text]\n    Returns:\n        attention_weights: [B, T_text]\n    \"\"\"\n    B = query.shape[0]\n\n    prior_filter = do_prior_filter(\n        attention_weights, self.prior, self.prior_filter_len-1)\n\n    G = self.filter_mlp(query)\n    # compute dynamic filters\n    pad = (self.dynamic_kernel_size - 1) // 2\n    dynamic_filter = F.conv1d(\n        attention_weights[None],\n        G.view(-1, 1, self.dynamic_kernel_size),\n        padding=pad,\n        groups=B,\n    )\n    dynamic_filter = dynamic_filter.view(\n        B, self.dynamic_filter_dim, -1).transpose(1, 2)\n\n    attention_weights = addsoftmax(\n        self.post_mlp(dynamic_filter).squeeze(-1), prior_filter)\n\n    return attention_weights\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.MonotonicDynamicConvolutionAttention.apply","title":"<code>apply(attention_weights, inputs, mask=None)</code>","text":"<p>this is split out to implement attention painting</p> <p>do weights really need to be masked here if it's post-softmax anyway? is it enough that the inputs are zero padded?</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def apply(self, attention_weights, inputs, mask:Optional[Tensor]=None):\n    \"\"\"this is split out to implement attention painting\n\n    do weights really need to be masked here if it's post-softmax anyway?\n    is it enough that the inputs are zero padded?\n    \"\"\"\n    # print(attention_weights.shape, inputs.shape)\n    # apply masking\n    if mask is not None:\n        if torch.is_grad_enabled():\n            attention_weights = attention_weights.masked_fill(\n                ~mask, self._mask_value)\n        else:\n            attention_weights.masked_fill_(~mask, self._mask_value)\n    context = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n    return context\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.NSF","title":"<code>NSF</code>","text":"<p>               Bases: <code>Module</code></p> <p>Neural Spline Flow likelihood</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class NSF(nn.Module):\n    \"\"\"Neural Spline Flow likelihood\"\"\"\n    def __init__(self, \n            latent_size,\n            context_size=256,\n            hidden_size=256,\n            hidden_layers=2,\n            blocks=8,\n            bins=8,\n            dropout=None,\n            ):\n        super().__init__()\n        self.context_size = context_size\n        self.latent_size = latent_size\n\n        flows = []\n        for _ in range(blocks):\n            flows.append(nf.flows.CoupledRationalQuadraticSpline(\n                latent_size, hidden_layers, hidden_size, context_size,\n                num_bins=bins, dropout_probability=dropout))\n            flows.append(nf.flows.LULinearPermute(latent_size))\n        # base distribution\n        # q0 = nf.distributions.DiagGaussian(latent_size, trainable=False)\n        q0 = ConditionalDiagGaussian(\n            latent_size, context_encoder=nn.Linear(context_size, 2*latent_size))\n        # flow model\n        self.net = nf.ConditionalNormalizingFlow(q0=q0, flows=flows)\n        # self.net.q0.forward = torch.jit.ignore(self.net.q0.forward)\n\n    def n_params(self, size):\n        assert size == self.latent_size\n        return self.context_size\n\n    @torch.jit.ignore\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n        params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n        x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n        loglik = self.net.log_prob(x, context=params)\n        return -loglik.reshape(*bs)\n\n    @torch.jit.ignore\n    def sample(self, params:Tensor, temperature:float=1.0):\n        bs = params.shape[:-1]\n        params = params.reshape(-1, params.shape[-1])\n        # if temperature!=1:\n        #     print('warning: temperature not implemented in NSF')\n        # samp, logprob = self.net.sample(params.shape[0], context=params)\n        mu, logsigma = self.net.q0.context_encoder(params).chunk(2,-1)\n        base_samp = mu + torch.randn_like(mu)*logsigma.exp()*temperature\n        samp = self.net(base_samp, context=params)\n        samp = samp.reshape(*bs, -1)\n        return samp\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.NSF.forward","title":"<code>forward(x, params)</code>","text":"<p>negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, x:Tensor, params:Tensor):\n    \"\"\"negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    bs = torch.broadcast_shapes(x.shape[:-1], params.shape[:-1])\n    params = params.expand(*bs, -1).reshape(-1, params.shape[-1])\n    x = x.expand(*bs, -1).reshape(-1, x.shape[-1])\n    loglik = self.net.log_prob(x, context=params)\n    return -loglik.reshape(*bs)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.Prenet","title":"<code>Prenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tacotron specific Prenet with an optional Batch Normalization. Note:     Prenet with BN improves the model performance significantly especially if it is enabled after learning a diagonal attention alignment with the original prenet. However, if the target dataset is high quality then it also works from the start. It is also suggested to disable dropout if BN is in use.     prenet_type == \"original\"         x -&gt; [linear -&gt; ReLU -&gt; Dropout]xN -&gt; o     prenet_type == \"bn\"         x -&gt; [linear -&gt; BN -&gt; ReLU -&gt; Dropout]xN -&gt; o Args:     in_features (int): number of channels in the input tensor and the inner layers.     prenet_type (str, optional): prenet type \"original\" or \"bn\". Defaults to \"original\".     prenet_dropout (bool, optional): dropout rate. Defaults to True.     dropout_at_inference (bool, optional): use dropout at inference. It leads to a better quality for some models.     out_features (list, optional): List of output channels for each prenet block.         It also defines number of the prenet blocks based on the length of argument list.         Defaults to [256, 256].     bias (bool, optional): enable/disable bias in prenet linear layers. Defaults to True.</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class Prenet(nn.Module):\n    \"\"\"Tacotron specific Prenet with an optional Batch Normalization.\n    Note:\n        Prenet with BN improves the model performance significantly especially\n    if it is enabled after learning a diagonal attention alignment with the original\n    prenet. However, if the target dataset is high quality then it also works from\n    the start. It is also suggested to disable dropout if BN is in use.\n        prenet_type == \"original\"\n            x -&gt; [linear -&gt; ReLU -&gt; Dropout]xN -&gt; o\n        prenet_type == \"bn\"\n            x -&gt; [linear -&gt; BN -&gt; ReLU -&gt; Dropout]xN -&gt; o\n    Args:\n        in_features (int): number of channels in the input tensor and the inner layers.\n        prenet_type (str, optional): prenet type \"original\" or \"bn\". Defaults to \"original\".\n        prenet_dropout (bool, optional): dropout rate. Defaults to True.\n        dropout_at_inference (bool, optional): use dropout at inference. It leads to a better quality for some models.\n        out_features (list, optional): List of output channels for each prenet block.\n            It also defines number of the prenet blocks based on the length of argument list.\n            Defaults to [256, 256].\n        bias (bool, optional): enable/disable bias in prenet linear layers. Defaults to True.\n    \"\"\"\n\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_features,\n        prenet_type=\"original\",\n        dropout_at_inference=False,\n        out_features=[256, 256],\n        bias=True,\n        weight_norm=False\n    ):\n        super().__init__()\n        self.prenet_type = prenet_type\n        self.dropout_at_inference = dropout_at_inference\n        in_features = [in_features] + out_features[:-1]\n        # if prenet_type == \"bn\":\n        #     self.linear_layers = nn.ModuleList(\n        #         [LinearBN(in_size, out_size, bias=bias) for (in_size, out_size) in zip(in_features, out_features)]\n        #     )\n        # elif prenet_type == \"original\":\n        norm = (\n            nn.utils.parametrizations.weight_norm \n            if weight_norm else lambda x:x)\n        self.linear_layers = nn.ModuleList([\n            norm(nn.Linear(in_size, out_size, bias=bias))\n            for (in_size, out_size) in zip(in_features, out_features)\n        ])\n\n    def forward(self, x, dropout:float=0.5):\n        for linear in self.linear_layers:\n            if dropout:\n                x = F.dropout(\n                    F.relu(linear(x)), p=dropout, \n                    training=self.training or self.dropout_at_inference)\n            else:\n                x = F.relu(linear(x))\n        return x\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.StandardNormal","title":"<code>StandardNormal</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class StandardNormal(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\n            'log2pi', torch.tensor(np.log(2*np.pi)).float(), persistent=False)\n\n    def n_params(self, size):\n        return size\n\n    def forward(self, x:Tensor, params:Tensor):\n        \"\"\"standard normal negative log likelihood\n        Args:\n            x: Tensor[batch, time, channel]\n            params: Tensor[batch, time, n_params]    \n        Return:\n            negative log likelihood: Tensor[batch, time]\n        \"\"\"\n        mu = params\n        loglik = 0.5 * (\n            (x - mu) ** 2 + self.log2pi\n        )\n        return loglik.sum(-1)\n\n    def sample(self, params:Tensor, temperature:float=1.0):\n        return params + temperature*torch.randn_like(params)\n\n    def metrics(self, params:Tensor):\n        return {}\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.StandardNormal.forward","title":"<code>forward(x, params)</code>","text":"<p>standard normal negative log likelihood Args:     x: Tensor[batch, time, channel]     params: Tensor[batch, time, n_params]   Return:     negative log likelihood: Tensor[batch, time]</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, x:Tensor, params:Tensor):\n    \"\"\"standard normal negative log likelihood\n    Args:\n        x: Tensor[batch, time, channel]\n        params: Tensor[batch, time, n_params]    \n    Return:\n        negative log likelihood: Tensor[batch, time]\n    \"\"\"\n    mu = params\n    loglik = 0.5 * (\n        (x - mu) ** 2 + self.log2pi\n    )\n    return loglik.sum(-1)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore","title":"<code>TacotronCore</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class TacotronCore(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        frame_channels=None, # RAVE latent dim\n        dropout=0.1,\n        dropout_type='dropout', #'zoneout'\n        prenet_type='original', # disabled\n        prenet_dropout=0.2,\n        prenet_layers=2,\n        prenet_size=256,\n        prenet_wn=False,\n        hidden_dropout=0,\n        separate_stopnet=True, # disabled\n        rnn_size=1200,\n        rnn_bias=True,\n        rnn_layers=1,\n        noise_channels=0,\n        decoder_type='lstm',\n        decoder_layers=1,\n        decoder_size=None,\n        hidden_to_decoder=True,\n        memory_to_decoder=False,\n        init_proj=1,\n        proj_wn=False,\n        attn_wn=False,\n        learn_go_frame=False,\n        pitch_xform=False,\n        block_size=2048,\n        max_batch=8,\n        max_tokens=1024,\n        prior_filter_len=11,\n        tokens_per_frame=1.0,\n        attention_type='dca',\n        length_reparam=False\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input (text feature) channels.\n            out_channels (int): number of output (likelihood parameter) channels.\n            frame_channels (int): number of audio latent channels.\n            dropout (float): dropout rate (except prenet).\n            prenet_dropout (float): prenet dropout rate.\n            max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n        \"\"\"\n        super().__init__()\n        assert frame_channels is not None\n\n        self.B = max_batch\n        self.T = max_tokens\n\n        self.length_reparam = length_reparam\n        self.hidden_to_decoder = hidden_to_decoder\n        self.memory_to_decoder = memory_to_decoder\n\n        self.block_size = block_size\n        self.frame_channels = frame_channels\n        self.pitch_xform = pitch_xform\n        # self.r_init = r\n        # self.r = r\n        self.encoder_embedding_dim = in_channels\n        self.separate_stopnet = separate_stopnet\n\n        decoder_size = decoder_size or rnn_size\n\n        # model dimensions\n        self.rnn_layers = rnn_layers\n        self.decoder_layers = decoder_layers\n        self.attention_hidden_dim = rnn_size\n        self.decoder_rnn_dim = decoder_size\n        self.prenet_dim = prenet_size\n        self.noise_channels = noise_channels\n        # self.attn_dim = 128\n        self.p_attention_dropout = dropout\n        self.p_decoder_dropout = dropout\n        self.dropout_type = dropout_type\n\n        # memory -&gt; |Prenet| -&gt; processed_memory\n        prenet_dim = self.frame_channels\n        self.prenet_dropout = prenet_dropout\n        self.prenet = Prenet(\n            prenet_dim, prenet_type, \n            out_features=[self.prenet_dim]*prenet_layers, \n            bias=False, weight_norm=prenet_wn\n        )\n\n        self.hidden_dropout = hidden_dropout\n\n        # self.attention_rnn = DropoutRNN(nn.LSTMCell(\n            # self.prenet_dim + in_channels, self.attention_hidden_dim, bias=rnn_bias))\n\n        # interlayer dropout is given here, recurrent dropout in forward\n        # not for a good reason but it's fine\n        self.attention_rnn = ResidualRNN( \n            lambda i,h,**kw: DropoutRNN(nn.LSTMCell(i,h,bias=rnn_bias),**kw),\n            self.prenet_dim + in_channels + noise_channels, \n            self.attention_hidden_dim, \n            layers=rnn_layers, dropout=dropout\n        )\n\n        if attention_type=='dca':\n            prior_alpha = tokens_per_frame / (prior_filter_len-1)\n            prior_beta = 1-prior_alpha\n            self.attention = MonotonicDynamicConvolutionAttention(\n                query_dim=self.attention_hidden_dim,\n                attention_dim=128,\n                prior_filter_len=prior_filter_len,\n                alpha=prior_alpha,\n                beta=prior_beta\n            )\n        elif attention_type=='gauss':\n            self.attention = GaussianAttention(\n                query_dim=self.attention_hidden_dim,\n                prior_tokens_per_frame=tokens_per_frame,\n                wn=attn_wn\n            )\n\n        # a decoder network further processes the text glimpse and main RNN state,\n        # which can increase depth, while using optimized RNN kernels on GPU,\n        # which the main RNN can't because it is interleaved with attention.\n        # so moving parameters from attention RNN to decoder RNN may make training\n        # faster (though not inference)\n        # it can also be skipped entirely.\n        if decoder_type is None:\n            self.decoder_rnn = None\n        else:\n            if decoder_type=='lstm':\n                decoder_type = nn.LSTM\n            elif decoder_type=='mlp':\n                decoder_type = MLPDecoder\n            else:\n                raise ValueError(decoder_type)\n            decoder_in = in_channels\n            if hidden_to_decoder:\n                decoder_in += self.attention_hidden_dim \n            if memory_to_decoder: \n                decoder_in += self.prenet_dim + noise_channels\n            self.decoder_rnn = decoder_type(\n                decoder_in, self.decoder_rnn_dim, \n                num_layers=self.decoder_layers,\n                bias=rnn_bias, batch_first=True, \n                dropout=0 if decoder_layers==1 else dropout)\n\n        # this linear projection matches the sizes of the hidden state and\n        # the likelihood \n        hidden_size = (\n            self.attention_hidden_dim \n            if self.decoder_rnn is None \n            else self.decoder_rnn_dim)\n        linear_projection = nn.Linear(\n            hidden_size + in_channels, \n            out_channels)\n\n        with torch.no_grad():\n            linear_projection.weight.mul_(init_proj)\n\n        if proj_wn:\n            linear_projection = nn.utils.parametrizations.weight_norm(\n                linear_projection)\n        self.linear_projection = linear_projection\n\n\n        # the stopnet predicts whether the utterance has ended\n        self.stopnet = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(\n                # self.decoder_rnn_dim + self.frame_channels * self.r_init, 1, \n                hidden_size + self.frame_channels, 1, \n                # bias=True, init_gain=\"sigmoid\"\n                ),\n        )\n\n        # TODO: need to clone these or no?\n        # does register_buffer copy?\n        # does Parameter copy?\n        t_mem = torch.zeros(1, self.frame_channels)\n        t_attn = torch.zeros(self.rnn_layers, 1, self.attention_hidden_dim)\n        t_dec = torch.zeros(self.decoder_layers, 1, self.decoder_rnn_dim)\n        t_ctx = torch.zeros(1, self.encoder_embedding_dim)\n\n        # initial state parameters\n        self.learn_go_frame = learn_go_frame\n        # if self.learn_go_frame:\n        self.go_frame = nn.Parameter(t_mem.clone())\n        self.go_attention_hidden = nn.Parameter(t_attn.clone())\n        self.go_attention_cell = nn.Parameter(t_attn.clone())\n        self.go_decoder_hidden = nn.Parameter(t_dec.clone())\n        self.go_decoder_cell = nn.Parameter(t_dec.clone())\n        self.go_context = nn.Parameter(t_ctx.clone())\n\n        # print(id(self.go_attention_hidden.data))\n        # print(id(self.go_attention_cell.data))\n\n        # state buffers for inference\n        self.register_buffer(\n            \"memory\", t_mem.expand(max_batch, -1).clone()) \n        self.register_buffer(\n            \"context\", t_ctx.expand(max_batch, -1).clone())\n        self.register_buffer(\n            \"attention_hidden\", t_attn.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"attention_cell\", t_attn.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"decoder_hidden\", t_dec.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"decoder_cell\", t_dec.expand(-1, max_batch, -1).clone())\n        self.register_buffer(\n            \"alignment\", torch.zeros(max_batch, max_tokens))\n        self.register_buffer(\n            \"inputs\", torch.zeros(max_batch, max_tokens, in_channels))\n\n    def init_states(self, inputs):#, keep_states=False):\n        \"\"\"\n        return initial states\n        \"\"\"\n        B = inputs.size(0)\n        # if not keep_states:\n        if self.learn_go_frame:\n            attention_hidden = self.go_attention_hidden.expand(B, -1)\n            attention_cell = self.go_attention_cell.expand(-1, B, -1)\n            decoder_hidden = self.go_decoder_hidden.expand(-1, B, -1)\n            decoder_cell = self.go_decoder_cell.expand(-1, B, -1)\n            context = self.go_context.expand(B, -1)\n            memory = self.go_frame.expand(B,-1).clone()\n        else:\n            attention_hidden = inputs.new_zeros(\n                self.rnn_layers, B, self.attention_hidden_dim)\n            attention_cell = torch.zeros_like(attention_hidden)\n            decoder_hidden = inputs.new_zeros(\n                self.decoder_layers, B, self.decoder_rnn_dim)\n            decoder_cell = torch.zeros_like(decoder_hidden)\n            context = inputs.new_zeros(B, self.encoder_embedding_dim)\n            memory = inputs.new_zeros(B, self.frame_channels)           \n\n        alignment = self.attention.init_states(inputs)\n\n        # for t in (memory, context, alignment,\n        #     attention_hidden, attention_cell, \n        #     decoder_hidden, decoder_cell):\n        #     print(t.shape)\n\n        return (\n            memory, context, alignment,\n            attention_hidden, attention_cell, \n            decoder_hidden, decoder_cell)\n\n    @torch.jit.export\n    def reset(self, inputs):\n        r\"\"\"\n        populates buffers with initial states and text inputs\n\n        call with encoded text, before using `step`\n\n        Args:\n            inputs: (B, T_text, D_text)\n\n        \"\"\"\n        assert inputs.ndim==3, str(inputs.shape)#f'{inputs.shape=}'\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        assert B&lt;=self.inputs.shape[0], 'max batch size exceeded'\n        assert T&lt;=self.inputs.shape[1], 'max tokens exceeded'\n\n        (\n            self.memory[:B], self.context[:B], self.alignment[:B, :T],\n            self.attention_hidden[:,:B], self.attention_cell[:,:B], \n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n        ) = self.init_states(inputs)#, keep_states=False)\n\n        self.inputs[:B, :T] = inputs\n\n        self.B = B\n        self.T = T\n\n    # TODO: should there be option to include acoustic memory here?\n    @torch.jit.export\n    def get_state(self):\n        return {\n            'rnn_states': (\n                self.attention_hidden.clone(),\n                self.attention_cell.clone(),\n                self.decoder_hidden.clone(),\n                self.decoder_cell.clone()\n            ),\n            'context': self.context.clone(),\n            'attention_states': self.alignment.clone(),\n        }\n\n    @torch.jit.export\n    def set_state(self, state:Dict[str,Tensor|Tuple[Tensor, Tensor, Tensor, Tensor]]):\n        rnn_states = state['rnn_states']\n        if torch.jit.is_scripting():\n            # weirdly torschript seems to demand this while Python chokes on \n            # the use of generic types with isintance\n            assert isinstance(rnn_states, Tuple[Tensor, Tensor, Tensor, Tensor])\n        (\n            self.attention_hidden[:],\n            self.attention_cell[:],\n            self.decoder_hidden[:],\n            self.decoder_cell[:]\n        ) = rnn_states\n        context = state['context']\n        alignment = state['attention_states']\n        assert isinstance(context, Tensor)\n        assert isinstance(alignment, Tensor)\n        self.context[:] = context\n        self.alignment[:] = alignment\n\n    # @torch.jit.export\n    def forward(self, \n            inputs,\n            context, memory, alignment,\n            attention_hidden, attention_cell,\n            set_alignment:bool=False,\n            mask:Optional[Tensor]=None):\n        \"\"\"run step of attention loop\n        Args:\n            inputs: [B, T_text, D_text] encoded text\n            context: [B, D_text] combined text encoding from previous alignment\n            memory: [B, D_audio] acoustic memory of last output\n            alignment: [B, T_text]\n            attention_hidden: [B, attention_hidden_dim]\n            attention_cell: [B, attention_hidden_dim]\n            set_alignment: bool\n                if True, `alignment` is the next alignment to text\n                if False, `alignment` is the previous alignment, \n                    and the attention module computes next\n        Returns:\n            context: as above\n            alignment: as above\n            attention_hidden: as above\n            attention_cell: as above\n        \"\"\"\n        # feed the latest text and audio encodings into the RNN\n        query_input = [memory, context]\n        if self.noise_channels:\n            query_input.append(torch.randn(\n                context.shape[0], self.noise_channels, \n                device=context.device, dtype=context.dtype))\n        query_input = torch.cat(query_input, -1)\n        attention_hidden, attention_cell = self.attention_rnn(\n            query_input, (attention_hidden, attention_cell),\n            training=self.training, \n            dropout_p=self.p_attention_dropout, \n            dropout_type=self.dropout_type)\n\n        if not set_alignment:\n            # compute next alignment from the RNN state\n            alignment = self.attention.align(\n                attention_hidden[-1], alignment, inputs, mask)\n\n        # combine text encodings according to the new alignment\n        context = self.attention.apply(alignment, inputs, mask)\n\n        return (\n            context,\n            alignment,\n            attention_hidden, attention_cell\n        )\n\n    @torch.jit.export\n    def decode_post(self, \n            hidden, context, memory,\n            decoder_hidden, decoder_cell,\n            lengths:Optional[Tensor]=None\n        ):\n        \"\"\"run post-decoder (step or full time dimension)\n\n        Args:\n            hidden: B x T_audio x channel (hidden state after attention net)\n            context: B x T_audio x D_text (audio-aligned text features)\n            lengths: if not None, pack the inputs\n        Returns:\n            hidden: B x T_audio x channel (hidden state after decoder net)\n            output_params: B x T_audio x channel (likelihood parameters)\n        \"\"\"\n        if self.decoder_rnn is not None:\n            decoder_rnn_input = []\n            if self.hidden_to_decoder:\n                decoder_rnn_input.append(hidden)\n            if self.memory_to_decoder:\n                decoder_rnn_input.append(memory)\n            decoder_rnn_input.append(context)\n            decoder_rnn_input = torch.cat(decoder_rnn_input, -1)\n            if lengths is not None:\n                decoder_rnn_input_packed = nn.utils.rnn.pack_padded_sequence(\n                    decoder_rnn_input, lengths, \n                    batch_first=True, enforce_sorted=False)\n\n                # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n                hidden_packed, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                    decoder_rnn_input_packed, (decoder_hidden, decoder_cell))\n\n                hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(\n                    hidden_packed, batch_first=True)\n\n                # in case pad_packed messes up extra padding\n                hidden = torch.cat((hidden, hidden.new_zeros(\n                    hidden.shape[0], \n                    context.shape[1]-hidden.shape[1], \n                    hidden.shape[2])\n                    ), 1)\n            else:\n                # TODO why contiguous needed here when decoder_layers &gt; 1?\n                hidden, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                    decoder_rnn_input, (decoder_hidden.contiguous(), decoder_cell.contiguous()))\n\n        if self.hidden_dropout:\n            hidden = F.dropout(hidden, float(self.hidden_dropout), self.training)\n        # # B x T x (D_decoder_rnn + D_text)\n        decoder_hidden_context = torch.cat((hidden, context), dim=-1)\n        # B x T x self.frame_channels\n        output_params = self.linear_projection(decoder_hidden_context)\n        return hidden, output_params, decoder_hidden, decoder_cell\n\n    @torch.jit.export\n    def predict_stop(self, decoder_state, output):\n        # B x (D_decoder_rnn + (self.r * self.frame_channels))\n        stopnet_input = torch.cat((decoder_state, output), dim=-1)\n        if self.separate_stopnet:\n            stopnet_input = stopnet_input.detach()\n        stop_token = self.stopnet(stopnet_input)\n        return stop_token\n\n    @torch.jit.export\n    def latent_map(self, z):\n        if self.pitch_xform:\n            z = torch.cat((\n                hz_to_z(z[...,:1]),\n                z[...,1:]\n            ), -1) \n\n        if self.length_reparam:\n            m = torch.linalg.vector_norm(z, dim=-1, keepdim=True)\n            z = torch.cat((m, z), -1)\n        return z\n\n    @torch.jit.export\n    def latent_unmap(self, z):\n        if self.length_reparam:\n            m, z = z.split((1,z.shape[-1]-1), dim=-1)\n            z = z * (\n                m / (torch.linalg.vector_norm(z, dim=-1, keepdim=True)+1e-7))\n\n        if self.pitch_xform:\n            z = torch.cat((\n                z_to_hz(z[...,:1]),\n                z[...,1:]\n            ), -1) \n        return z\n\n    def decode_loop(self, \n            inputs, context, memories, alignment, att_hidden, att_cell, mask):\n        \"\"\"loop over training data frames, align to text\"\"\"\n        hidden, contexts, alignments = [], [], []\n        for memory in memories[:-1]:\n            context, alignment, att_hidden, att_cell = self(\n                inputs, context, memory, alignment,\n                att_hidden, att_cell, set_alignment=False,\n                mask=mask)\n            hidden.append(att_hidden[-1])\n            contexts.append(context)\n            alignments.append(alignment)\n        hidden = torch.stack(hidden, 1)\n        contexts = torch.stack(contexts, 1)\n        alignments = torch.stack(alignments, 1)\n        return hidden, contexts, alignments\n\n        # @torch.jit.ignore\n    @torch.jit.export\n    def step_pre(self, \n            alignment:Optional[Tensor]=None,\n            audio_frame:Optional[Tensor]=None\n        ):\n        B, T = self.B, self.T\n        if alignment is None:\n            alignment = self.alignment[:B, :T]\n            set_alignment = False\n        else:\n            assert alignment.ndim==2, str(alignment.shape)\n            set_alignment = True\n\n        if audio_frame is not None:\n            self.memory[:B] = self.latent_map(audio_frame)\n\n        # print(self.memory[:B])\n\n        memory = self.prenet(\n            self.memory[:B], dropout=float(self.prenet_dropout))\n        # if alignment is not None: print(f'DEBUG: {alignment.shape=}, {memory.shape=}')\n        (\n            self.context[:B], \n            alignment,\n            self.attention_hidden[:,:B], self.attention_cell[:,:B]\n        ) = self(\n            self.inputs[:B, :T], \n            self.context[:B], memory, alignment,\n            self.attention_hidden[:,:B], self.attention_cell[:,:B], \n            set_alignment=set_alignment,\n            mask=None)\n        (\n            _, output_params, \n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n        ) = self.decode_post(\n            self.attention_hidden[-1,:B,None], self.context[:B,None], \n            memory[:B,None],\n            self.decoder_hidden[:,:B], self.decoder_cell[:,:B],\n            lengths=None)\n\n        return alignment, output_params\n\n    @torch.jit.export\n    def step_post(self, alignment, decoder_output):\n        self.memory[:self.B] = decoder_output\n        self.alignment[:self.B,:self.T] = alignment\n\n        decoder_output = self.latent_unmap(decoder_output)\n        return decoder_output\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.__init__","title":"<code>__init__(in_channels, out_channels, frame_channels=None, dropout=0.1, dropout_type='dropout', prenet_type='original', prenet_dropout=0.2, prenet_layers=2, prenet_size=256, prenet_wn=False, hidden_dropout=0, separate_stopnet=True, rnn_size=1200, rnn_bias=True, rnn_layers=1, noise_channels=0, decoder_type='lstm', decoder_layers=1, decoder_size=None, hidden_to_decoder=True, memory_to_decoder=False, init_proj=1, proj_wn=False, attn_wn=False, learn_go_frame=False, pitch_xform=False, block_size=2048, max_batch=8, max_tokens=1024, prior_filter_len=11, tokens_per_frame=1.0, attention_type='dca', length_reparam=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input (text feature) channels.</p> required <code>out_channels</code> <code>int</code> <p>number of output (likelihood parameter) channels.</p> required <code>frame_channels</code> <code>int</code> <p>number of audio latent channels.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>dropout rate (except prenet).</p> <code>0.1</code> <code>prenet_dropout</code> <code>float</code> <p>prenet dropout rate.</p> <code>0.2</code> <code>max_decoder_steps</code> <code>int</code> <p>Maximum number of steps allowed for the decoder. Defaults to 10000.</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    frame_channels=None, # RAVE latent dim\n    dropout=0.1,\n    dropout_type='dropout', #'zoneout'\n    prenet_type='original', # disabled\n    prenet_dropout=0.2,\n    prenet_layers=2,\n    prenet_size=256,\n    prenet_wn=False,\n    hidden_dropout=0,\n    separate_stopnet=True, # disabled\n    rnn_size=1200,\n    rnn_bias=True,\n    rnn_layers=1,\n    noise_channels=0,\n    decoder_type='lstm',\n    decoder_layers=1,\n    decoder_size=None,\n    hidden_to_decoder=True,\n    memory_to_decoder=False,\n    init_proj=1,\n    proj_wn=False,\n    attn_wn=False,\n    learn_go_frame=False,\n    pitch_xform=False,\n    block_size=2048,\n    max_batch=8,\n    max_tokens=1024,\n    prior_filter_len=11,\n    tokens_per_frame=1.0,\n    attention_type='dca',\n    length_reparam=False\n):\n    \"\"\"\n    Args:\n        in_channels (int): number of input (text feature) channels.\n        out_channels (int): number of output (likelihood parameter) channels.\n        frame_channels (int): number of audio latent channels.\n        dropout (float): dropout rate (except prenet).\n        prenet_dropout (float): prenet dropout rate.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n    \"\"\"\n    super().__init__()\n    assert frame_channels is not None\n\n    self.B = max_batch\n    self.T = max_tokens\n\n    self.length_reparam = length_reparam\n    self.hidden_to_decoder = hidden_to_decoder\n    self.memory_to_decoder = memory_to_decoder\n\n    self.block_size = block_size\n    self.frame_channels = frame_channels\n    self.pitch_xform = pitch_xform\n    # self.r_init = r\n    # self.r = r\n    self.encoder_embedding_dim = in_channels\n    self.separate_stopnet = separate_stopnet\n\n    decoder_size = decoder_size or rnn_size\n\n    # model dimensions\n    self.rnn_layers = rnn_layers\n    self.decoder_layers = decoder_layers\n    self.attention_hidden_dim = rnn_size\n    self.decoder_rnn_dim = decoder_size\n    self.prenet_dim = prenet_size\n    self.noise_channels = noise_channels\n    # self.attn_dim = 128\n    self.p_attention_dropout = dropout\n    self.p_decoder_dropout = dropout\n    self.dropout_type = dropout_type\n\n    # memory -&gt; |Prenet| -&gt; processed_memory\n    prenet_dim = self.frame_channels\n    self.prenet_dropout = prenet_dropout\n    self.prenet = Prenet(\n        prenet_dim, prenet_type, \n        out_features=[self.prenet_dim]*prenet_layers, \n        bias=False, weight_norm=prenet_wn\n    )\n\n    self.hidden_dropout = hidden_dropout\n\n    # self.attention_rnn = DropoutRNN(nn.LSTMCell(\n        # self.prenet_dim + in_channels, self.attention_hidden_dim, bias=rnn_bias))\n\n    # interlayer dropout is given here, recurrent dropout in forward\n    # not for a good reason but it's fine\n    self.attention_rnn = ResidualRNN( \n        lambda i,h,**kw: DropoutRNN(nn.LSTMCell(i,h,bias=rnn_bias),**kw),\n        self.prenet_dim + in_channels + noise_channels, \n        self.attention_hidden_dim, \n        layers=rnn_layers, dropout=dropout\n    )\n\n    if attention_type=='dca':\n        prior_alpha = tokens_per_frame / (prior_filter_len-1)\n        prior_beta = 1-prior_alpha\n        self.attention = MonotonicDynamicConvolutionAttention(\n            query_dim=self.attention_hidden_dim,\n            attention_dim=128,\n            prior_filter_len=prior_filter_len,\n            alpha=prior_alpha,\n            beta=prior_beta\n        )\n    elif attention_type=='gauss':\n        self.attention = GaussianAttention(\n            query_dim=self.attention_hidden_dim,\n            prior_tokens_per_frame=tokens_per_frame,\n            wn=attn_wn\n        )\n\n    # a decoder network further processes the text glimpse and main RNN state,\n    # which can increase depth, while using optimized RNN kernels on GPU,\n    # which the main RNN can't because it is interleaved with attention.\n    # so moving parameters from attention RNN to decoder RNN may make training\n    # faster (though not inference)\n    # it can also be skipped entirely.\n    if decoder_type is None:\n        self.decoder_rnn = None\n    else:\n        if decoder_type=='lstm':\n            decoder_type = nn.LSTM\n        elif decoder_type=='mlp':\n            decoder_type = MLPDecoder\n        else:\n            raise ValueError(decoder_type)\n        decoder_in = in_channels\n        if hidden_to_decoder:\n            decoder_in += self.attention_hidden_dim \n        if memory_to_decoder: \n            decoder_in += self.prenet_dim + noise_channels\n        self.decoder_rnn = decoder_type(\n            decoder_in, self.decoder_rnn_dim, \n            num_layers=self.decoder_layers,\n            bias=rnn_bias, batch_first=True, \n            dropout=0 if decoder_layers==1 else dropout)\n\n    # this linear projection matches the sizes of the hidden state and\n    # the likelihood \n    hidden_size = (\n        self.attention_hidden_dim \n        if self.decoder_rnn is None \n        else self.decoder_rnn_dim)\n    linear_projection = nn.Linear(\n        hidden_size + in_channels, \n        out_channels)\n\n    with torch.no_grad():\n        linear_projection.weight.mul_(init_proj)\n\n    if proj_wn:\n        linear_projection = nn.utils.parametrizations.weight_norm(\n            linear_projection)\n    self.linear_projection = linear_projection\n\n\n    # the stopnet predicts whether the utterance has ended\n    self.stopnet = nn.Sequential(\n        nn.Dropout(0.1),\n        nn.Linear(\n            # self.decoder_rnn_dim + self.frame_channels * self.r_init, 1, \n            hidden_size + self.frame_channels, 1, \n            # bias=True, init_gain=\"sigmoid\"\n            ),\n    )\n\n    # TODO: need to clone these or no?\n    # does register_buffer copy?\n    # does Parameter copy?\n    t_mem = torch.zeros(1, self.frame_channels)\n    t_attn = torch.zeros(self.rnn_layers, 1, self.attention_hidden_dim)\n    t_dec = torch.zeros(self.decoder_layers, 1, self.decoder_rnn_dim)\n    t_ctx = torch.zeros(1, self.encoder_embedding_dim)\n\n    # initial state parameters\n    self.learn_go_frame = learn_go_frame\n    # if self.learn_go_frame:\n    self.go_frame = nn.Parameter(t_mem.clone())\n    self.go_attention_hidden = nn.Parameter(t_attn.clone())\n    self.go_attention_cell = nn.Parameter(t_attn.clone())\n    self.go_decoder_hidden = nn.Parameter(t_dec.clone())\n    self.go_decoder_cell = nn.Parameter(t_dec.clone())\n    self.go_context = nn.Parameter(t_ctx.clone())\n\n    # print(id(self.go_attention_hidden.data))\n    # print(id(self.go_attention_cell.data))\n\n    # state buffers for inference\n    self.register_buffer(\n        \"memory\", t_mem.expand(max_batch, -1).clone()) \n    self.register_buffer(\n        \"context\", t_ctx.expand(max_batch, -1).clone())\n    self.register_buffer(\n        \"attention_hidden\", t_attn.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"attention_cell\", t_attn.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"decoder_hidden\", t_dec.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"decoder_cell\", t_dec.expand(-1, max_batch, -1).clone())\n    self.register_buffer(\n        \"alignment\", torch.zeros(max_batch, max_tokens))\n    self.register_buffer(\n        \"inputs\", torch.zeros(max_batch, max_tokens, in_channels))\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.decode_loop","title":"<code>decode_loop(inputs, context, memories, alignment, att_hidden, att_cell, mask)</code>","text":"<p>loop over training data frames, align to text</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def decode_loop(self, \n        inputs, context, memories, alignment, att_hidden, att_cell, mask):\n    \"\"\"loop over training data frames, align to text\"\"\"\n    hidden, contexts, alignments = [], [], []\n    for memory in memories[:-1]:\n        context, alignment, att_hidden, att_cell = self(\n            inputs, context, memory, alignment,\n            att_hidden, att_cell, set_alignment=False,\n            mask=mask)\n        hidden.append(att_hidden[-1])\n        contexts.append(context)\n        alignments.append(alignment)\n    hidden = torch.stack(hidden, 1)\n    contexts = torch.stack(contexts, 1)\n    alignments = torch.stack(alignments, 1)\n    return hidden, contexts, alignments\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.decode_post","title":"<code>decode_post(hidden, context, memory, decoder_hidden, decoder_cell, lengths=None)</code>","text":"<p>run post-decoder (step or full time dimension)</p> <p>Parameters:</p> Name Type Description Default <code>hidden</code> <p>B x T_audio x channel (hidden state after attention net)</p> required <code>context</code> <p>B x T_audio x D_text (audio-aligned text features)</p> required <code>lengths</code> <code>Optional[Tensor]</code> <p>if not None, pack the inputs</p> <code>None</code> <p>Returns:     hidden: B x T_audio x channel (hidden state after decoder net)     output_params: B x T_audio x channel (likelihood parameters)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef decode_post(self, \n        hidden, context, memory,\n        decoder_hidden, decoder_cell,\n        lengths:Optional[Tensor]=None\n    ):\n    \"\"\"run post-decoder (step or full time dimension)\n\n    Args:\n        hidden: B x T_audio x channel (hidden state after attention net)\n        context: B x T_audio x D_text (audio-aligned text features)\n        lengths: if not None, pack the inputs\n    Returns:\n        hidden: B x T_audio x channel (hidden state after decoder net)\n        output_params: B x T_audio x channel (likelihood parameters)\n    \"\"\"\n    if self.decoder_rnn is not None:\n        decoder_rnn_input = []\n        if self.hidden_to_decoder:\n            decoder_rnn_input.append(hidden)\n        if self.memory_to_decoder:\n            decoder_rnn_input.append(memory)\n        decoder_rnn_input.append(context)\n        decoder_rnn_input = torch.cat(decoder_rnn_input, -1)\n        if lengths is not None:\n            decoder_rnn_input_packed = nn.utils.rnn.pack_padded_sequence(\n                decoder_rnn_input, lengths, \n                batch_first=True, enforce_sorted=False)\n\n            # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n            hidden_packed, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                decoder_rnn_input_packed, (decoder_hidden, decoder_cell))\n\n            hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(\n                hidden_packed, batch_first=True)\n\n            # in case pad_packed messes up extra padding\n            hidden = torch.cat((hidden, hidden.new_zeros(\n                hidden.shape[0], \n                context.shape[1]-hidden.shape[1], \n                hidden.shape[2])\n                ), 1)\n        else:\n            # TODO why contiguous needed here when decoder_layers &gt; 1?\n            hidden, (decoder_hidden, decoder_cell) = self.decoder_rnn( \n                decoder_rnn_input, (decoder_hidden.contiguous(), decoder_cell.contiguous()))\n\n    if self.hidden_dropout:\n        hidden = F.dropout(hidden, float(self.hidden_dropout), self.training)\n    # # B x T x (D_decoder_rnn + D_text)\n    decoder_hidden_context = torch.cat((hidden, context), dim=-1)\n    # B x T x self.frame_channels\n    output_params = self.linear_projection(decoder_hidden_context)\n    return hidden, output_params, decoder_hidden, decoder_cell\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.forward","title":"<code>forward(inputs, context, memory, alignment, attention_hidden, attention_cell, set_alignment=False, mask=None)</code>","text":"<p>run step of attention loop Args:     inputs: [B, T_text, D_text] encoded text     context: [B, D_text] combined text encoding from previous alignment     memory: [B, D_audio] acoustic memory of last output     alignment: [B, T_text]     attention_hidden: [B, attention_hidden_dim]     attention_cell: [B, attention_hidden_dim]     set_alignment: bool         if True, <code>alignment</code> is the next alignment to text         if False, <code>alignment</code> is the previous alignment,              and the attention module computes next Returns:     context: as above     alignment: as above     attention_hidden: as above     attention_cell: as above</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def forward(self, \n        inputs,\n        context, memory, alignment,\n        attention_hidden, attention_cell,\n        set_alignment:bool=False,\n        mask:Optional[Tensor]=None):\n    \"\"\"run step of attention loop\n    Args:\n        inputs: [B, T_text, D_text] encoded text\n        context: [B, D_text] combined text encoding from previous alignment\n        memory: [B, D_audio] acoustic memory of last output\n        alignment: [B, T_text]\n        attention_hidden: [B, attention_hidden_dim]\n        attention_cell: [B, attention_hidden_dim]\n        set_alignment: bool\n            if True, `alignment` is the next alignment to text\n            if False, `alignment` is the previous alignment, \n                and the attention module computes next\n    Returns:\n        context: as above\n        alignment: as above\n        attention_hidden: as above\n        attention_cell: as above\n    \"\"\"\n    # feed the latest text and audio encodings into the RNN\n    query_input = [memory, context]\n    if self.noise_channels:\n        query_input.append(torch.randn(\n            context.shape[0], self.noise_channels, \n            device=context.device, dtype=context.dtype))\n    query_input = torch.cat(query_input, -1)\n    attention_hidden, attention_cell = self.attention_rnn(\n        query_input, (attention_hidden, attention_cell),\n        training=self.training, \n        dropout_p=self.p_attention_dropout, \n        dropout_type=self.dropout_type)\n\n    if not set_alignment:\n        # compute next alignment from the RNN state\n        alignment = self.attention.align(\n            attention_hidden[-1], alignment, inputs, mask)\n\n    # combine text encodings according to the new alignment\n    context = self.attention.apply(alignment, inputs, mask)\n\n    return (\n        context,\n        alignment,\n        attention_hidden, attention_cell\n    )\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.init_states","title":"<code>init_states(inputs)</code>","text":"<p>return initial states</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def init_states(self, inputs):#, keep_states=False):\n    \"\"\"\n    return initial states\n    \"\"\"\n    B = inputs.size(0)\n    # if not keep_states:\n    if self.learn_go_frame:\n        attention_hidden = self.go_attention_hidden.expand(B, -1)\n        attention_cell = self.go_attention_cell.expand(-1, B, -1)\n        decoder_hidden = self.go_decoder_hidden.expand(-1, B, -1)\n        decoder_cell = self.go_decoder_cell.expand(-1, B, -1)\n        context = self.go_context.expand(B, -1)\n        memory = self.go_frame.expand(B,-1).clone()\n    else:\n        attention_hidden = inputs.new_zeros(\n            self.rnn_layers, B, self.attention_hidden_dim)\n        attention_cell = torch.zeros_like(attention_hidden)\n        decoder_hidden = inputs.new_zeros(\n            self.decoder_layers, B, self.decoder_rnn_dim)\n        decoder_cell = torch.zeros_like(decoder_hidden)\n        context = inputs.new_zeros(B, self.encoder_embedding_dim)\n        memory = inputs.new_zeros(B, self.frame_channels)           \n\n    alignment = self.attention.init_states(inputs)\n\n    # for t in (memory, context, alignment,\n    #     attention_hidden, attention_cell, \n    #     decoder_hidden, decoder_cell):\n    #     print(t.shape)\n\n    return (\n        memory, context, alignment,\n        attention_hidden, attention_cell, \n        decoder_hidden, decoder_cell)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronCore.reset","title":"<code>reset(inputs)</code>","text":"<p>populates buffers with initial states and text inputs</p> <p>call with encoded text, before using <code>step</code></p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>(B, T_text, D_text)</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef reset(self, inputs):\n    r\"\"\"\n    populates buffers with initial states and text inputs\n\n    call with encoded text, before using `step`\n\n    Args:\n        inputs: (B, T_text, D_text)\n\n    \"\"\"\n    assert inputs.ndim==3, str(inputs.shape)#f'{inputs.shape=}'\n    B = inputs.shape[0]\n    T = inputs.shape[1]\n    assert B&lt;=self.inputs.shape[0], 'max batch size exceeded'\n    assert T&lt;=self.inputs.shape[1], 'max tokens exceeded'\n\n    (\n        self.memory[:B], self.context[:B], self.alignment[:B, :T],\n        self.attention_hidden[:,:B], self.attention_cell[:,:B], \n        self.decoder_hidden[:,:B], self.decoder_cell[:,:B] \n    ) = self.init_states(inputs)#, keep_states=False)\n\n    self.inputs[:B, :T] = inputs\n\n    self.B = B\n    self.T = T\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder","title":"<code>TacotronDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class TacotronDecoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=None, # text embedding dim\n        frame_channels=None, # RAVE latent dim\n        dropout=0.1,\n        likelihood_type='nsf',#'normal'#'mixture'#'ged'\n        mixture_n=16,\n        flow_context=256,\n        flow_hidden=256,\n        flow_layers=2,\n        flow_blocks=2,\n        nsf_bins=16,\n        ged_hidden=256,\n        ged_layers=4,\n        ged_unfold=None,\n        ged_multiply_params=False,\n        ged_project_params=None,\n        ged_glu=False,\n        ged_dropout=None, #None follows main dropout, 0 turns off\n        dropout_type='dropout', #'zoneout'\n        prenet_type='original', # disabled\n        prenet_dropout=0.2,\n        prenet_layers=2,\n        prenet_size=256,\n        prenet_wn=False,\n        hidden_dropout=0,\n        separate_stopnet=True, # disabled\n        max_decoder_steps=10000,\n        text_encoder:Dict=None,\n        rnn_size=1200,\n        rnn_bias=True,\n        rnn_layers=1,\n        noise_channels=0,\n        decoder_type='lstm',\n        decoder_layers=1,\n        decoder_size=None,\n        hidden_to_decoder=True,\n        memory_to_decoder=False,\n        init_proj=1.0,\n        proj_wn=False,\n        attn_wn=False,\n        learn_go_frame=False,\n        pitch_xform=False,\n        length_reparam=False,\n        text_encoder_type='canine',\n        block_size=2048,\n        max_batch=8,\n        max_tokens=1024,\n        prior_filter_len=11,\n        tokens_per_frame=1.0,\n        attention_type='dca',\n        script=False\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input channels.\n            frame_channels (int): number of feature frame channels.\n            dropout (float): dropout rate (except prenet).\n            prenet_dropout (float): prenet dropout rate.\n            max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n            text_encoder: dict of text encoder kwargs\n        \"\"\"\n        super().__init__()\n        assert frame_channels is not None\n\n        if length_reparam:\n            frame_channels = frame_channels + 1\n\n        self.B = max_batch\n        self.T = max_tokens\n\n        if text_encoder_type not in [None, 'none']:\n            if text_encoder is None: text_encoder = {}\n            if text_encoder_type=='zero':\n                self.text_encoder = ZeroEncoder(**text_encoder)\n            elif text_encoder_type=='baseline':\n                self.text_encoder = TacotronEncoder(**text_encoder)\n            elif text_encoder_type=='canine':\n                self.text_encoder = CanineEncoder(**text_encoder)\n            elif text_encoder_type=='canine_embedding':\n                self.text_encoder = CanineEmbeddings(**text_encoder)\n            else:\n                raise ValueError(text_encoder_type)\n            if in_channels is None:\n                in_channels = self.text_encoder.channels\n            elif in_channels != self.text_encoder.channels:\n                raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n        else:\n            self.text_encoder = None\n            assert in_channels is not None\n\n        self.max_decoder_steps = max_decoder_steps\n        self.block_size = block_size\n        self.frame_channels = frame_channels\n        # self.pitch_xform = pitch_xform\n        # # self.r_init = r\n        # # self.r = r\n        # self.encoder_embedding_dim = in_channels\n        # self.separate_stopnet = separate_stopnet\n        # self.max_decoder_steps = max_decoder_steps\n        self.stop_threshold = 0.5\n\n        # decoder_size = decoder_size or rnn_size\n\n        # # model dimensions\n        # self.decoder_layers = decoder_layers\n        # self.attention_hidden_dim = rnn_size\n        # self.decoder_rnn_dim = decoder_size\n        # self.prenet_dim = prenet_size\n        # # self.attn_dim = 128\n        # self.p_attention_dropout = dropout\n        # self.p_decoder_dropout = dropout\n        # self.dropout_type = dropout_type\n\n        self.prenet_dropout = prenet_dropout\n\n        # the likelihood converts a hidden state to a probability distribution\n        # over each vocoder frame.\n        # in the simpler cases this is just unpacking location and scale from\n        # the hidden state.\n        # in other cases the likelihood can be a normalizing flow with its own\n        # trainable parameters.\n        if likelihood_type=='normal':\n            self.likelihood = StandardNormal()\n        elif likelihood_type=='diagonal':\n            self.likelihood = DiagonalNormal()\n        elif likelihood_type=='mixture':\n            self.likelihood = DiagonalNormalMixture(mixture_n)\n        elif likelihood_type=='ged':\n            ged_dropout = dropout if ged_dropout is None else ged_dropout\n            self.likelihood = GED(\n                self.frame_channels,\n                hidden_size=ged_hidden, hidden_layers=ged_layers, \n                dropout=ged_dropout, unfold=ged_unfold, \n                multiply_params=ged_multiply_params,\n                project_params=ged_project_params,\n                glu=ged_glu\n                )\n        elif likelihood_type=='nsf':\n            self.likelihood = NSF(\n                self.frame_channels, context_size=flow_context,\n                hidden_size=flow_hidden, hidden_layers=flow_layers,\n                blocks=flow_blocks, bins=nsf_bins,\n                dropout=dropout)\n        else:\n            raise ValueError(likelihood_type)\n\n        if script and likelihood_type!='nsf':\n            self.likelihood = torch.jit.script(self.likelihood)\n\n        self.core = TacotronCore(\n            in_channels=in_channels, # text embedding dim\n            out_channels=self.likelihood.n_params(self.frame_channels),\n            frame_channels=frame_channels, # RAVE latent dim\n            dropout=dropout,\n            dropout_type=dropout_type, #'zoneout'\n            prenet_type=prenet_type, # disabled\n            prenet_dropout=prenet_dropout,\n            prenet_layers=prenet_layers,\n            prenet_size=prenet_size,\n            prenet_wn=prenet_wn,\n            hidden_dropout=hidden_dropout,\n            separate_stopnet=separate_stopnet, # disabled\n            rnn_size=rnn_size,\n            rnn_bias=rnn_bias,\n            rnn_layers=rnn_layers,\n            noise_channels=noise_channels,\n            decoder_type=decoder_type,\n            decoder_layers=decoder_layers,\n            decoder_size=decoder_size,\n            hidden_to_decoder=hidden_to_decoder,\n            memory_to_decoder=memory_to_decoder,\n            init_proj=init_proj,\n            proj_wn=proj_wn,\n            attn_wn=attn_wn,\n            learn_go_frame=learn_go_frame,\n            pitch_xform=pitch_xform,\n            block_size=block_size,\n            max_batch=max_batch,\n            max_tokens=max_tokens,\n            prior_filter_len=prior_filter_len,\n            tokens_per_frame=tokens_per_frame,\n            attention_type=attention_type,\n            length_reparam=length_reparam\n        )\n\n        if script:\n            self.core = torch.jit.script(self.core)\n\n    @property\n    def memory(self):\n        return self.core.memory\n\n    @classmethod\n    def from_checkpoint(cls, path_or_dict):\n        if isinstance(path_or_dict, dict):\n            ckpt = path_or_dict\n        else:\n            ckpt = torch.load(\n                path_or_dict, map_location='cpu', weights_only=False)\n\n        kw = ckpt['kw']\n        model_kw = cls.update_kw_dict(kw['model'])\n\n        model = cls(**model_kw)\n        try:\n            model.load_state_dict(ckpt['model_state'], strict=True)\n        except Exception as e:\n            print(e.__traceback__)\n            model.load_state_dict(ckpt['model_state'], strict=False)\n\n\n        return model\n\n    @classmethod\n    def update_kw_dict(cls, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        b = d.pop('text_bottleneck', None)\n        if b is not None:\n            if d['text_encoder'] is None:\n                d['text_encoder'] = {}\n            d['text_encoder']['bottleneck'] = b\n        return d\n\n    @torch.jit.ignore\n    def update_state_dict(self, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        # TODO: core.\n        def replace(old, new):\n            t = d.pop(old, None)\n            if t is not None:\n                d[new] = t\n        replace('go_attention_rnn_cell_state', 'go_attention_cell')\n        replace('go_query', 'go_attention_hidden')\n        # rnncell -&gt; dropoutrnn\n        replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n        replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n        replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n        replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n        # dropoutrnn -&gt; residualrnn\n        replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n        replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n        replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n        replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n        for name in (\n            'go_attention_hidden', 'go_attention_cell',\n            'attention_hidden', 'attention_cell'\n            ):\n            if name in d and d[name].ndim==2:\n                d[name] = d[name][None]\n        # move into core\n        for name in list(d):\n            if any(name.startswith(s) for s in (\n                \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n                replace(name, f'core.{name}')\n\n        # text bottleneck -&gt; text encoder\n        replace('core.text_proj.weight', 'text_encoder.proj.weight')\n        replace('core.text_proj.bias', 'text_encoder.proj.bias')\n        return d\n\n    @torch.jit.ignore\n    def load_state_dict(self, d, **kw):\n        super().load_state_dict(self.update_state_dict(d), **kw)\n\n    @torch.jit.export\n    def reset(self, inputs):\n        r\"\"\"\n        populates buffers with initial states and text inputs\n\n        call with encoded text, before using `step`\n\n        Args:\n            inputs: (B, T_text, D_text)\n\n        \"\"\"\n        self.core.reset(inputs)\n\n    # TODO: should there be option to include acoustic memory here?\n    @torch.jit.export\n    def get_state(self):\n        return self.core.get_state()\n\n    @torch.jit.export\n    def set_state(self, \n            state:Dict[str,Tensor|Tuple[Tensor, Tensor, Tensor, Tensor]]):\n        self.core.set_state(state)\n\n    def latent_map(self, z):\n        return self.core.latent_map(z)\n\n    def latent_unmap(self, z):\n        return self.core.latent_unmap(z)\n\n    def chunk_pad(self, inputs, mask, c=128):\n        b, t = mask.shape\n        p = math.ceil(t / c) * c - t\n        if p&gt;0:\n            inputs = torch.cat(\n                (inputs, inputs.new_zeros(b, p, *inputs.shape[2:])), 1)\n            mask = torch.cat(\n                (mask, mask.new_zeros(b, p)), 1)\n        return inputs, mask\n\n\n    @torch.jit.ignore\n    def forward(self, inputs, audio, mask, audio_mask,\n            audio_lengths:Optional[Tensor]=None,\n            prenet_dropout:Optional[float]=None,\n            chunk_pad_text:int|None=None,\n            chunk_pad_audio:int|None=None,\n            temperature:float=1\n            ):\n        r\"\"\"Train Decoder with teacher forcing.\n        Args:\n            inputs: raw or encoded text.\n            audio: audio frames for teacher-forcing.\n            mask: text mask for sequence padding.\n            audio_mask: audio mask for loss computation.\n            prenet_dropout: if not None, override original value\n                (to implement e.g. annealing)\n            temperature: no effect on training, only on returned output/MSE\n        Shapes:\n            - inputs: \n                FloatTensor (B, T_text, D_text)\n                or LongTensor (B, T_text)\n            - audio: (B, T_audio, D_audio)\n            - mask: (B, T_text)\n            - audio_mask: (B, T_audio)\n            - stop_target TODO\n\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_audio, T_text)\n            - stop_tokens: (B, T_audio)\n\n        \"\"\"\n        if chunk_pad_audio:\n            audio, audio_mask = self.chunk_pad(\n                audio, audio_mask, chunk_pad_audio)\n\n        if audio_lengths is None:\n            audio_lengths = audio_mask.sum(-1).cpu()\n        ground_truth = audio\n        if prenet_dropout is None:\n            prenet_dropout = self.prenet_dropout\n\n        # print(f'{audio[...,0].min()=}')\n        audio = self.latent_map(audio)\n        # print(f'{audio[...,0].min()=}')\n\n        if inputs.dtype==torch.long:\n            assert self.text_encoder is not None\n            assert inputs.ndim==2\n            inputs = self.text_encoder.encode(inputs, mask)\n        if chunk_pad_text:\n            inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n        (\n            memory, context, alignment,\n            attention_hidden, attention_cell, \n            decoder_hidden, decoder_cell \n        ) = self.core.init_states(inputs)  \n\n        # concat the initial audio frame with training data\n        memories = torch.cat((memory[None], audio.transpose(0, 1)))\n        memories = self.core.prenet(memories, prenet_dropout)\n\n        # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n        hidden, contexts, alignments = self.core.decode_loop(\n            inputs, context, memories, alignment,\n            attention_hidden, attention_cell,\n            mask)\n\n        # compute the additional decoder layers \n        hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n            hidden, contexts, memories[:-1].transpose(0,1),\n            decoder_hidden, decoder_cell, \n            audio_lengths)\n\n        r, outputs = self.run_likelihood(\n            audio, audio_mask, output_params, temperature=temperature)\n\n        stop_loss = None\n        # TODO\n        # stop_tokens = self.predict_stop(hidden, outputs)\n        # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n        outputs = self.latent_unmap(outputs)\n\n        r.update({\n            'text': inputs,\n            # 'stop_loss': stop_loss,\n            'predicted': outputs,\n            'ground_truth': ground_truth,\n            'alignment': alignments,\n            'params': output_params,\n            # 'stop': stop_tokens,\n            'audio_mask': audio_mask,\n            'text_mask': mask,\n            **self.likelihood.metrics(output_params),\n            **self.alignment_metrics(alignments, mask, audio_mask)\n        })\n\n        return r\n\n    def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n        \"\"\"\n        alignments: (B, T_audio, T_text)\n        mask: (B, T_text)\n        \"\"\"\n        # TODO: could normalize concentration by logT and subtract from 1,\n        # so it represents a proportion of the entropy 'unused'\n        # then could have a cutoff parameter\n        # alignment should hit every token: max. entropy of mean token probs\n        # alignment should be sharp: min. mean of token entropy\n        concentration = []\n        concentration_norm = []\n        dispersion = []\n        # alternatively:\n        # max average length of token prob vectors, and of time-curve vectors\n        # if using L2, this enourages token energy to concentrate in few token\n        # dimensions per vector, but to spread across multiple time steps \n        # concentration_l2 = []\n        # dispersion_l2 = []\n        for a,mt,ma in zip(alignments, mask, audio_mask):\n            a = a[ma][:,mt]\n            mean_probs = a.mean(0)\n            ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n            # ent_mean = torch.special.entr(mean_probs).sum()\n            concentration.append(ent_mean)\n            concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n            dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n            # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n            # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n            # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n        return {\n            'concentration': torch.stack(concentration),#.mean(),\n            'concentration_norm': torch.stack(concentration_norm),#.mean(),\n            'dispersion': torch.stack(dispersion),#.mean(),\n            # 'concentration_l2': torch.stack(concentration_l2).mean(),\n            # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n        }\n\n    @torch.jit.ignore\n    def run_likelihood(self, \n            audio, audio_mask, output_params, temperature:float=1):\n        r = {}\n        m = audio_mask[...,None]\n        audio_m = audio*m\n        params_m = output_params*m\n        # nll = self.likelihood(audio*m, output_params*m)\n        # nll = nll.masked_select(audio_mask).mean()\n\n        # NOTE: could improve training performance here?\n        #   use audio_mask before likelihood instead of after\n        r['nll'] = (\n            self.likelihood(audio_m, params_m)\n            .masked_select(audio_mask).mean())\n\n        if isinstance(self.likelihood, DiagonalNormalMixture):\n            r['nll_fixed'] = (\n                self.likelihood(audio_m, params_m, mode='fixed')\n                .masked_select(audio_mask).mean())\n            r['nll_posthoc'] = (\n                self.likelihood(audio_m, params_m, mode='posthoc')\n                .masked_select(audio_mask).mean())\n\n        with torch.no_grad():\n            # return self.likelihood.sample(output_params, temperature=0)\n            outputs = self.likelihood.sample(\n                output_params, temperature=temperature) # low memory\n\n        d = audio_m - outputs*m\n        r['mse'] = (d*d).mean() * m.numel() / m.float().sum()\n        return r, outputs\n\n    # @torch.jit.ignore\n    @torch.jit.export\n    def step(self, \n            alignment:Optional[Tensor]=None,\n            audio_frame:Optional[Tensor]=None, \n            temperature:float=1.0\n        ):\n        r\"\"\"\n        single step of inference.\n\n        optionally supply `alignment` to force the alignments.\n        optionally supply `audio_frame` to set the previous frame.\n\n        Args:\n            alignment: B x T_text\n            audio_frame: B x D_audio\n            temperature: optional sampling temperature\n        Returns:\n            output: B x D_audio\n            alignment: B x T_text\n            stop_token: None (not implemented)\n        \"\"\"\n        alignment, output_params = self.core.step_pre(alignment, audio_frame)\n        decoder_output = self.likelihood.sample(\n            output_params, temperature=temperature).squeeze(1)\n\n        decoder_output = self.core.step_post(alignment, decoder_output)\n\n        # if debug:\n        return dict(\n            output=decoder_output,\n            alignment=alignment,\n            stop_prob=torch.tensor(0.),\n            params=output_params\n        )\n        # else: \n            # return decoder_output, alignment, 0\n\n    # TODO rewrite this or don't script it?\n    # @torch.jit.export\n    @torch.jit.ignore\n    def inference(self, inputs, \n            stop:bool=True, \n            max_steps:Optional[int]=None, \n            temperature:float=1.0,\n            alignments:Optional[Tensor]=None,\n            audio:Optional[Tensor]=None,\n            ):\n        r\"\"\"Decoder inference\n        Can supply forced alignments to text;\n        Can also supply audio for teacher forcing, \n        to test consistency with the training code or implement prompting\n        Args:\n            inputs: Text Encoder outputs.\n            stop: use stop gate\n            max_steps: stop after this many decoder steps\n            temperature: for the sampling distribution\n            alignments: forced alignment\n            audio: forced last-frame\n                (will be offset, you don't need to prepend a start frame)\n        Shapes:\n            - inputs: (B, T_text, D_text)\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_text, T_audio)\n            - audio: (B, T_audio, D_audio)\n            - stop_tokens: (B, T_audio)\n        \"\"\"\n        # max_steps = max_steps or self.max_decoder_steps\n        max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n        self.reset(inputs)\n\n        outputs = []\n        if alignments is None:\n            alignments = []\n            feed_alignment = False\n        else:\n            feed_alignment = True\n\n        for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n            alignment = alignments[:,:,i] if feed_alignment else None\n            audio_frame = (\n                audio[:,i-1,:] \n                if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n                else None)\n            r = self.step(\n                temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n            outputs.append(r['output'])\n            if not feed_alignment:\n                alignments.append(r['alignment'])\n\n            if stop and r['stop_prob']&gt;self.stop_threshold:\n                break\n            if len(outputs) == max_steps:\n                if stop:\n                    print(f\"   &gt; Decoder stopped with {max_steps=}\")\n                break\n\n        outputs = torch.stack(outputs, 1)\n        if not feed_alignment:\n            alignments = torch.stack(alignments, 1)\n\n        return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.__init__","title":"<code>__init__(in_channels=None, frame_channels=None, dropout=0.1, likelihood_type='nsf', mixture_n=16, flow_context=256, flow_hidden=256, flow_layers=2, flow_blocks=2, nsf_bins=16, ged_hidden=256, ged_layers=4, ged_unfold=None, ged_multiply_params=False, ged_project_params=None, ged_glu=False, ged_dropout=None, dropout_type='dropout', prenet_type='original', prenet_dropout=0.2, prenet_layers=2, prenet_size=256, prenet_wn=False, hidden_dropout=0, separate_stopnet=True, max_decoder_steps=10000, text_encoder=None, rnn_size=1200, rnn_bias=True, rnn_layers=1, noise_channels=0, decoder_type='lstm', decoder_layers=1, decoder_size=None, hidden_to_decoder=True, memory_to_decoder=False, init_proj=1.0, proj_wn=False, attn_wn=False, learn_go_frame=False, pitch_xform=False, length_reparam=False, text_encoder_type='canine', block_size=2048, max_batch=8, max_tokens=1024, prior_filter_len=11, tokens_per_frame=1.0, attention_type='dca', script=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels.</p> <code>None</code> <code>frame_channels</code> <code>int</code> <p>number of feature frame channels.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>dropout rate (except prenet).</p> <code>0.1</code> <code>prenet_dropout</code> <code>float</code> <p>prenet dropout rate.</p> <code>0.2</code> <code>max_decoder_steps</code> <code>int</code> <p>Maximum number of steps allowed for the decoder. Defaults to 10000.</p> <code>10000</code> <code>text_encoder</code> <code>Dict</code> <p>dict of text encoder kwargs</p> <code>None</code> Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(\n    self,\n    in_channels=None, # text embedding dim\n    frame_channels=None, # RAVE latent dim\n    dropout=0.1,\n    likelihood_type='nsf',#'normal'#'mixture'#'ged'\n    mixture_n=16,\n    flow_context=256,\n    flow_hidden=256,\n    flow_layers=2,\n    flow_blocks=2,\n    nsf_bins=16,\n    ged_hidden=256,\n    ged_layers=4,\n    ged_unfold=None,\n    ged_multiply_params=False,\n    ged_project_params=None,\n    ged_glu=False,\n    ged_dropout=None, #None follows main dropout, 0 turns off\n    dropout_type='dropout', #'zoneout'\n    prenet_type='original', # disabled\n    prenet_dropout=0.2,\n    prenet_layers=2,\n    prenet_size=256,\n    prenet_wn=False,\n    hidden_dropout=0,\n    separate_stopnet=True, # disabled\n    max_decoder_steps=10000,\n    text_encoder:Dict=None,\n    rnn_size=1200,\n    rnn_bias=True,\n    rnn_layers=1,\n    noise_channels=0,\n    decoder_type='lstm',\n    decoder_layers=1,\n    decoder_size=None,\n    hidden_to_decoder=True,\n    memory_to_decoder=False,\n    init_proj=1.0,\n    proj_wn=False,\n    attn_wn=False,\n    learn_go_frame=False,\n    pitch_xform=False,\n    length_reparam=False,\n    text_encoder_type='canine',\n    block_size=2048,\n    max_batch=8,\n    max_tokens=1024,\n    prior_filter_len=11,\n    tokens_per_frame=1.0,\n    attention_type='dca',\n    script=False\n):\n    \"\"\"\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        dropout (float): dropout rate (except prenet).\n        prenet_dropout (float): prenet dropout rate.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n        text_encoder: dict of text encoder kwargs\n    \"\"\"\n    super().__init__()\n    assert frame_channels is not None\n\n    if length_reparam:\n        frame_channels = frame_channels + 1\n\n    self.B = max_batch\n    self.T = max_tokens\n\n    if text_encoder_type not in [None, 'none']:\n        if text_encoder is None: text_encoder = {}\n        if text_encoder_type=='zero':\n            self.text_encoder = ZeroEncoder(**text_encoder)\n        elif text_encoder_type=='baseline':\n            self.text_encoder = TacotronEncoder(**text_encoder)\n        elif text_encoder_type=='canine':\n            self.text_encoder = CanineEncoder(**text_encoder)\n        elif text_encoder_type=='canine_embedding':\n            self.text_encoder = CanineEmbeddings(**text_encoder)\n        else:\n            raise ValueError(text_encoder_type)\n        if in_channels is None:\n            in_channels = self.text_encoder.channels\n        elif in_channels != self.text_encoder.channels:\n            raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n    else:\n        self.text_encoder = None\n        assert in_channels is not None\n\n    self.max_decoder_steps = max_decoder_steps\n    self.block_size = block_size\n    self.frame_channels = frame_channels\n    # self.pitch_xform = pitch_xform\n    # # self.r_init = r\n    # # self.r = r\n    # self.encoder_embedding_dim = in_channels\n    # self.separate_stopnet = separate_stopnet\n    # self.max_decoder_steps = max_decoder_steps\n    self.stop_threshold = 0.5\n\n    # decoder_size = decoder_size or rnn_size\n\n    # # model dimensions\n    # self.decoder_layers = decoder_layers\n    # self.attention_hidden_dim = rnn_size\n    # self.decoder_rnn_dim = decoder_size\n    # self.prenet_dim = prenet_size\n    # # self.attn_dim = 128\n    # self.p_attention_dropout = dropout\n    # self.p_decoder_dropout = dropout\n    # self.dropout_type = dropout_type\n\n    self.prenet_dropout = prenet_dropout\n\n    # the likelihood converts a hidden state to a probability distribution\n    # over each vocoder frame.\n    # in the simpler cases this is just unpacking location and scale from\n    # the hidden state.\n    # in other cases the likelihood can be a normalizing flow with its own\n    # trainable parameters.\n    if likelihood_type=='normal':\n        self.likelihood = StandardNormal()\n    elif likelihood_type=='diagonal':\n        self.likelihood = DiagonalNormal()\n    elif likelihood_type=='mixture':\n        self.likelihood = DiagonalNormalMixture(mixture_n)\n    elif likelihood_type=='ged':\n        ged_dropout = dropout if ged_dropout is None else ged_dropout\n        self.likelihood = GED(\n            self.frame_channels,\n            hidden_size=ged_hidden, hidden_layers=ged_layers, \n            dropout=ged_dropout, unfold=ged_unfold, \n            multiply_params=ged_multiply_params,\n            project_params=ged_project_params,\n            glu=ged_glu\n            )\n    elif likelihood_type=='nsf':\n        self.likelihood = NSF(\n            self.frame_channels, context_size=flow_context,\n            hidden_size=flow_hidden, hidden_layers=flow_layers,\n            blocks=flow_blocks, bins=nsf_bins,\n            dropout=dropout)\n    else:\n        raise ValueError(likelihood_type)\n\n    if script and likelihood_type!='nsf':\n        self.likelihood = torch.jit.script(self.likelihood)\n\n    self.core = TacotronCore(\n        in_channels=in_channels, # text embedding dim\n        out_channels=self.likelihood.n_params(self.frame_channels),\n        frame_channels=frame_channels, # RAVE latent dim\n        dropout=dropout,\n        dropout_type=dropout_type, #'zoneout'\n        prenet_type=prenet_type, # disabled\n        prenet_dropout=prenet_dropout,\n        prenet_layers=prenet_layers,\n        prenet_size=prenet_size,\n        prenet_wn=prenet_wn,\n        hidden_dropout=hidden_dropout,\n        separate_stopnet=separate_stopnet, # disabled\n        rnn_size=rnn_size,\n        rnn_bias=rnn_bias,\n        rnn_layers=rnn_layers,\n        noise_channels=noise_channels,\n        decoder_type=decoder_type,\n        decoder_layers=decoder_layers,\n        decoder_size=decoder_size,\n        hidden_to_decoder=hidden_to_decoder,\n        memory_to_decoder=memory_to_decoder,\n        init_proj=init_proj,\n        proj_wn=proj_wn,\n        attn_wn=attn_wn,\n        learn_go_frame=learn_go_frame,\n        pitch_xform=pitch_xform,\n        block_size=block_size,\n        max_batch=max_batch,\n        max_tokens=max_tokens,\n        prior_filter_len=prior_filter_len,\n        tokens_per_frame=tokens_per_frame,\n        attention_type=attention_type,\n        length_reparam=length_reparam\n    )\n\n    if script:\n        self.core = torch.jit.script(self.core)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.alignment_metrics","title":"<code>alignment_metrics(alignments, mask, audio_mask, t=2)</code>","text":"<p>alignments: (B, T_audio, T_text) mask: (B, T_text)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n    \"\"\"\n    alignments: (B, T_audio, T_text)\n    mask: (B, T_text)\n    \"\"\"\n    # TODO: could normalize concentration by logT and subtract from 1,\n    # so it represents a proportion of the entropy 'unused'\n    # then could have a cutoff parameter\n    # alignment should hit every token: max. entropy of mean token probs\n    # alignment should be sharp: min. mean of token entropy\n    concentration = []\n    concentration_norm = []\n    dispersion = []\n    # alternatively:\n    # max average length of token prob vectors, and of time-curve vectors\n    # if using L2, this enourages token energy to concentrate in few token\n    # dimensions per vector, but to spread across multiple time steps \n    # concentration_l2 = []\n    # dispersion_l2 = []\n    for a,mt,ma in zip(alignments, mask, audio_mask):\n        a = a[ma][:,mt]\n        mean_probs = a.mean(0)\n        ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n        # ent_mean = torch.special.entr(mean_probs).sum()\n        concentration.append(ent_mean)\n        concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n        dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n        # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n        # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n        # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n    return {\n        'concentration': torch.stack(concentration),#.mean(),\n        'concentration_norm': torch.stack(concentration_norm),#.mean(),\n        'dispersion': torch.stack(dispersion),#.mean(),\n        # 'concentration_l2': torch.stack(concentration_l2).mean(),\n        # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n    }\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.forward","title":"<code>forward(inputs, audio, mask, audio_mask, audio_lengths=None, prenet_dropout=None, chunk_pad_text=None, chunk_pad_audio=None, temperature=1)</code>","text":"<p>Train Decoder with teacher forcing. Args:     inputs: raw or encoded text.     audio: audio frames for teacher-forcing.     mask: text mask for sequence padding.     audio_mask: audio mask for loss computation.     prenet_dropout: if not None, override original value         (to implement e.g. annealing)     temperature: no effect on training, only on returned output/MSE Shapes:     - inputs:          FloatTensor (B, T_text, D_text)         or LongTensor (B, T_text)     - audio: (B, T_audio, D_audio)     - mask: (B, T_text)     - audio_mask: (B, T_audio)     - stop_target TODO</p> <pre><code>- outputs: (B, T_audio, D_audio)\n- alignments: (B, T_audio, T_text)\n- stop_tokens: (B, T_audio)\n</code></pre> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, inputs, audio, mask, audio_mask,\n        audio_lengths:Optional[Tensor]=None,\n        prenet_dropout:Optional[float]=None,\n        chunk_pad_text:int|None=None,\n        chunk_pad_audio:int|None=None,\n        temperature:float=1\n        ):\n    r\"\"\"Train Decoder with teacher forcing.\n    Args:\n        inputs: raw or encoded text.\n        audio: audio frames for teacher-forcing.\n        mask: text mask for sequence padding.\n        audio_mask: audio mask for loss computation.\n        prenet_dropout: if not None, override original value\n            (to implement e.g. annealing)\n        temperature: no effect on training, only on returned output/MSE\n    Shapes:\n        - inputs: \n            FloatTensor (B, T_text, D_text)\n            or LongTensor (B, T_text)\n        - audio: (B, T_audio, D_audio)\n        - mask: (B, T_text)\n        - audio_mask: (B, T_audio)\n        - stop_target TODO\n\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_audio, T_text)\n        - stop_tokens: (B, T_audio)\n\n    \"\"\"\n    if chunk_pad_audio:\n        audio, audio_mask = self.chunk_pad(\n            audio, audio_mask, chunk_pad_audio)\n\n    if audio_lengths is None:\n        audio_lengths = audio_mask.sum(-1).cpu()\n    ground_truth = audio\n    if prenet_dropout is None:\n        prenet_dropout = self.prenet_dropout\n\n    # print(f'{audio[...,0].min()=}')\n    audio = self.latent_map(audio)\n    # print(f'{audio[...,0].min()=}')\n\n    if inputs.dtype==torch.long:\n        assert self.text_encoder is not None\n        assert inputs.ndim==2\n        inputs = self.text_encoder.encode(inputs, mask)\n    if chunk_pad_text:\n        inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n    (\n        memory, context, alignment,\n        attention_hidden, attention_cell, \n        decoder_hidden, decoder_cell \n    ) = self.core.init_states(inputs)  \n\n    # concat the initial audio frame with training data\n    memories = torch.cat((memory[None], audio.transpose(0, 1)))\n    memories = self.core.prenet(memories, prenet_dropout)\n\n    # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n    hidden, contexts, alignments = self.core.decode_loop(\n        inputs, context, memories, alignment,\n        attention_hidden, attention_cell,\n        mask)\n\n    # compute the additional decoder layers \n    hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n        hidden, contexts, memories[:-1].transpose(0,1),\n        decoder_hidden, decoder_cell, \n        audio_lengths)\n\n    r, outputs = self.run_likelihood(\n        audio, audio_mask, output_params, temperature=temperature)\n\n    stop_loss = None\n    # TODO\n    # stop_tokens = self.predict_stop(hidden, outputs)\n    # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n    outputs = self.latent_unmap(outputs)\n\n    r.update({\n        'text': inputs,\n        # 'stop_loss': stop_loss,\n        'predicted': outputs,\n        'ground_truth': ground_truth,\n        'alignment': alignments,\n        'params': output_params,\n        # 'stop': stop_tokens,\n        'audio_mask': audio_mask,\n        'text_mask': mask,\n        **self.likelihood.metrics(output_params),\n        **self.alignment_metrics(alignments, mask, audio_mask)\n    })\n\n    return r\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.inference","title":"<code>inference(inputs, stop=True, max_steps=None, temperature=1.0, alignments=None, audio=None)</code>","text":"<p>Decoder inference Can supply forced alignments to text; Can also supply audio for teacher forcing,  to test consistency with the training code or implement prompting Args:     inputs: Text Encoder outputs.     stop: use stop gate     max_steps: stop after this many decoder steps     temperature: for the sampling distribution     alignments: forced alignment     audio: forced last-frame         (will be offset, you don't need to prepend a start frame) Shapes:     - inputs: (B, T_text, D_text)     - outputs: (B, T_audio, D_audio)     - alignments: (B, T_text, T_audio)     - audio: (B, T_audio, D_audio)     - stop_tokens: (B, T_audio)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef inference(self, inputs, \n        stop:bool=True, \n        max_steps:Optional[int]=None, \n        temperature:float=1.0,\n        alignments:Optional[Tensor]=None,\n        audio:Optional[Tensor]=None,\n        ):\n    r\"\"\"Decoder inference\n    Can supply forced alignments to text;\n    Can also supply audio for teacher forcing, \n    to test consistency with the training code or implement prompting\n    Args:\n        inputs: Text Encoder outputs.\n        stop: use stop gate\n        max_steps: stop after this many decoder steps\n        temperature: for the sampling distribution\n        alignments: forced alignment\n        audio: forced last-frame\n            (will be offset, you don't need to prepend a start frame)\n    Shapes:\n        - inputs: (B, T_text, D_text)\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_text, T_audio)\n        - audio: (B, T_audio, D_audio)\n        - stop_tokens: (B, T_audio)\n    \"\"\"\n    # max_steps = max_steps or self.max_decoder_steps\n    max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n    self.reset(inputs)\n\n    outputs = []\n    if alignments is None:\n        alignments = []\n        feed_alignment = False\n    else:\n        feed_alignment = True\n\n    for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n        alignment = alignments[:,:,i] if feed_alignment else None\n        audio_frame = (\n            audio[:,i-1,:] \n            if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n            else None)\n        r = self.step(\n            temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n        outputs.append(r['output'])\n        if not feed_alignment:\n            alignments.append(r['alignment'])\n\n        if stop and r['stop_prob']&gt;self.stop_threshold:\n            break\n        if len(outputs) == max_steps:\n            if stop:\n                print(f\"   &gt; Decoder stopped with {max_steps=}\")\n            break\n\n    outputs = torch.stack(outputs, 1)\n    if not feed_alignment:\n        alignments = torch.stack(alignments, 1)\n\n    return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.reset","title":"<code>reset(inputs)</code>","text":"<p>populates buffers with initial states and text inputs</p> <p>call with encoded text, before using <code>step</code></p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>(B, T_text, D_text)</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef reset(self, inputs):\n    r\"\"\"\n    populates buffers with initial states and text inputs\n\n    call with encoded text, before using `step`\n\n    Args:\n        inputs: (B, T_text, D_text)\n\n    \"\"\"\n    self.core.reset(inputs)\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.step","title":"<code>step(alignment=None, audio_frame=None, temperature=1.0)</code>","text":"<p>single step of inference.</p> <p>optionally supply <code>alignment</code> to force the alignments. optionally supply <code>audio_frame</code> to set the previous frame.</p> <p>Parameters:</p> Name Type Description Default <code>alignment</code> <code>Optional[Tensor]</code> <p>B x T_text</p> <code>None</code> <code>audio_frame</code> <code>Optional[Tensor]</code> <p>B x D_audio</p> <code>None</code> <code>temperature</code> <code>float</code> <p>optional sampling temperature</p> <code>1.0</code> <p>Returns:     output: B x D_audio     alignment: B x T_text     stop_token: None (not implemented)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef step(self, \n        alignment:Optional[Tensor]=None,\n        audio_frame:Optional[Tensor]=None, \n        temperature:float=1.0\n    ):\n    r\"\"\"\n    single step of inference.\n\n    optionally supply `alignment` to force the alignments.\n    optionally supply `audio_frame` to set the previous frame.\n\n    Args:\n        alignment: B x T_text\n        audio_frame: B x D_audio\n        temperature: optional sampling temperature\n    Returns:\n        output: B x D_audio\n        alignment: B x T_text\n        stop_token: None (not implemented)\n    \"\"\"\n    alignment, output_params = self.core.step_pre(alignment, audio_frame)\n    decoder_output = self.likelihood.sample(\n        output_params, temperature=temperature).squeeze(1)\n\n    decoder_output = self.core.step_post(alignment, decoder_output)\n\n    # if debug:\n    return dict(\n        output=decoder_output,\n        alignment=alignment,\n        stop_prob=torch.tensor(0.),\n        params=output_params\n    )\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.update_kw_dict","title":"<code>update_kw_dict(d)</code>  <code>classmethod</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@classmethod\ndef update_kw_dict(cls, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    b = d.pop('text_bottleneck', None)\n    if b is not None:\n        if d['text_encoder'] is None:\n            d['text_encoder'] = {}\n        d['text_encoder']['bottleneck'] = b\n    return d\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.TacotronDecoder.update_state_dict","title":"<code>update_state_dict(d)</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef update_state_dict(self, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    # TODO: core.\n    def replace(old, new):\n        t = d.pop(old, None)\n        if t is not None:\n            d[new] = t\n    replace('go_attention_rnn_cell_state', 'go_attention_cell')\n    replace('go_query', 'go_attention_hidden')\n    # rnncell -&gt; dropoutrnn\n    replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n    replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n    replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n    replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n    # dropoutrnn -&gt; residualrnn\n    replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n    replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n    replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n    replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n    for name in (\n        'go_attention_hidden', 'go_attention_cell',\n        'attention_hidden', 'attention_cell'\n        ):\n        if name in d and d[name].ndim==2:\n            d[name] = d[name][None]\n    # move into core\n    for name in list(d):\n        if any(name.startswith(s) for s in (\n            \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n            replace(name, f'core.{name}')\n\n    # text bottleneck -&gt; text encoder\n    replace('core.text_proj.weight', 'text_encoder.proj.weight')\n    replace('core.text_proj.bias', 'text_encoder.proj.bias')\n    return d\n</code></pre>"},{"location":"reference/tungnaa/model/#tungnaa.model.zoneout","title":"<code>zoneout(x1, x2, p, training)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tensor</code> <p>old value</p> required <code>x2</code> <code>Tensor</code> <p>new value</p> required <code>p</code> <code>float</code> <p>prob of keeping old value</p> required <code>training</code> <code>bool</code> <p>stochastic if True, expectation if False</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>def zoneout(x1:Tensor, x2:Tensor, p:float, training:bool):\n    \"\"\"\n    Args:\n        x1: old value\n        x2: new value\n        p: prob of keeping old value\n        training: stochastic if True, expectation if False\n    \"\"\"\n    keep = torch.full_like(x1, p)\n    if training:\n        keep = torch.bernoulli(keep)\n    return torch.lerp(x2, x1, keep)\n</code></pre>"},{"location":"reference/tungnaa/prep/","title":"Prep","text":""},{"location":"reference/tungnaa/prep/#tungnaa.prep.rate_interp","title":"<code>rate_interp(audio, rate)</code>","text":"<p>audio: Tensor[channel x time]</p> Source code in <code>src/tungnaa/prep.py</code> <pre><code>def rate_interp(audio, rate):\n    \"\"\"\n    audio: Tensor[channel x time]\n    \"\"\"\n    if rate==1:\n        return audio\n    audio = audio[None] #batch, channel, time\n    mode = 'linear' if rate&gt;1 else 'area' \n    audio = torch.nn.functional.interpolate(\n        audio, scale_factor=rate, mode=mode)\n    return audio[0]\n</code></pre>"},{"location":"reference/tungnaa/split/","title":"Split","text":"<p>construct and split datasets. when run as a script, split the audio dataset for vocoder training.</p>"},{"location":"reference/tungnaa/text/","title":"Text","text":""},{"location":"reference/tungnaa/text/#tungnaa.text.CanineEmbeddings","title":"<code>CanineEmbeddings</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/text.py</code> <pre><code>class CanineEmbeddings(nn.Module):\n    def __init__(self, \n            pretrained='google/canine-c', \n            end_tokens=True,\n            bottleneck=False,\n            use_positions=True\n            ):\n        # canine-c: pre-trained with autoregressive character loss\n        # canine-s: pre-trained with subword masking loss\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        self.net = CanineModel.from_pretrained(pretrained).char_embeddings\n        if not use_positions:\n            self.net.position_embedding_type = None\n        self.channels = 768\n        self.bottleneck = False\n        if bottleneck:\n            self.bottleneck = True\n            self.proj = nn.Linear(self.channels, bottleneck)\n            self.channels = bottleneck\n\n    def init(self):\n        \"\"\"random initialization\"\"\"\n        self.net = CanineModel(self.net.config)\n\n    def pad(self, tokens, mask=None):\n        pad = 4 - tokens.shape[1]\n        if pad &gt; 0:\n            tokens = torch.cat((\n                tokens, tokens.new_zeros(tokens.shape[0], pad)\n                ), 1)\n            if mask is not None:\n                mask = torch.cat((\n                    mask, mask.new_zeros(mask.shape[0], pad)\n                ), 1)\n        return tokens, mask\n\n    def forward(self, text_t, mask):\n        # no mixing, mask can be ignored\n        h = self.net(text_t)\n        if self.bottleneck:\n            h = self.proj(h)\n        return h\n\n    def encode(self, text, mask=None):\n        if isinstance(text, str):\n            text, *_ = self.tokenize(text)\n        # n = text.shape[1]\n        # text, mask = self.pad(text, mask)\n        h = self(text, mask)\n        # h = h[:, :n] #unpad\n        return h\n\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.CanineEmbeddings.init","title":"<code>init()</code>","text":"<p>random initialization</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def init(self):\n    \"\"\"random initialization\"\"\"\n    self.net = CanineModel(self.net.config)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.CanineEncoder","title":"<code>CanineEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/text.py</code> <pre><code>class CanineEncoder(nn.Module):\n    def __init__(self, \n            pretrained='google/canine-c', \n            end_tokens=True):\n        # canine-c: pre-trained with autoregressive character loss\n        # canine-s: pre-trained with subword masking loss\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # self.net = CanineModel.from_pretrained(pretrained)\n        config = AutoConfig.from_pretrained(pretrained)\n        self.net = AutoModel.from_config(config)\n\n        self.channels = 768\n\n    def init(self):\n        \"\"\"random initialization\"\"\"\n        self.net = CanineModel(self.net.config)\n\n    def pad(self, tokens, mask=None):\n        pad = 4 - tokens.shape[1]\n        if pad &gt; 0:\n            tokens = torch.cat((\n                tokens, tokens.new_zeros(tokens.shape[0], pad)\n                ), 1)\n            if mask is not None:\n                mask = torch.cat((\n                    mask, mask.new_zeros(mask.shape[0], pad)\n                ), 1)\n        return tokens, mask\n\n    def forward(self, text_t, mask):\n        return self.net(text_t, mask, output_hidden_states=True)\n\n    def encode(self, text, mask=None, layer=-1):\n        if isinstance(text, str):\n            text, _, _ = self.tokenize(text)\n        n = text.shape[1]\n        text, mask = self.pad(text, mask)\n        h = self(text, mask).hidden_states[layer]\n        h = h[:, :n] #unpad\n        return h\n\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.CanineEncoder.init","title":"<code>init()</code>","text":"<p>random initialization</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def init(self):\n    \"\"\"random initialization\"\"\"\n    self.net = CanineModel(self.net.config)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.ConvBNBlock","title":"<code>ConvBNBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutions with Batch Normalization and non-linear activation.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>number of output channels.</p> required <code>kernel_size</code> <code>int</code> <p>convolution kernel size.</p> required <code>activation</code> <code>str</code> <p>'relu', 'tanh', None (linear).</p> <code>None</code> Shapes <ul> <li>input: (B, C_in, T)</li> <li>output: (B, C_out, T)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class ConvBNBlock(nn.Module):\n    r\"\"\"Convolutions with Batch Normalization and non-linear activation.\n\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): convolution kernel size.\n        activation (str): 'relu', 'tanh', None (linear).\n\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_out, T)\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, activation=None):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0\n        padding = (kernel_size - 1) // 2\n        self.convolution1d = nn.Conv1d(\n            in_channels, out_channels, kernel_size, padding=padding)\n        self.batch_normalization = nn.BatchNorm1d(\n            out_channels, momentum=0.1, eps=1e-5)\n        self.dropout = nn.Dropout(p=0.5)\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.Identity()\n\n    def forward(self, x, mask:Optional[Tensor]=None):\n        o = self.batch_normalization(x)\n        o = self.activation(o)\n        o = self.dropout(o)\n        if mask is not None:\n            # o = o.where(mask.bool()[:,None], 0)\n            o = o.where(mask[:,None]&gt;0, 0)\n        o = self.convolution1d(o)\n        return o\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.TacotronEncoder","title":"<code>TacotronEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Tacotron2 style Encoder for comparison with CANINE.</p> <p>Parameters:</p> Name Type Description Default <code>in_out_channels</code> <code>int</code> <p>number of input and output channels.</p> <code>768</code> Shapes <ul> <li>input: LongTensor (B, T)</li> <li>output: (B, T, D)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class TacotronEncoder(nn.Module):\n    r\"\"\"Tacotron2 style Encoder for comparison with CANINE.\n\n    Args:\n        in_out_channels (int): number of input and output channels.\n\n    Shapes:\n        - input: LongTensor (B, T)\n        - output: (B, T, D)\n    \"\"\"\n    def __init__(self, in_out_channels=768, end_tokens=True, conv_blocks=3, rnn=True):\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        # unicode points &gt; 1024 may collide\n        self.n_embeddings = 1025\n        self.embed = nn.Embedding(\n            self.n_embeddings, in_out_channels, padding_idx=0)\n\n        self.convolutions = nn.ModuleList()\n        for _ in range(conv_blocks):\n            self.convolutions.append(ConvBNBlock(\n                in_out_channels, in_out_channels, 5, \"relu\"))\n        if rnn:\n            self.lstm = nn.LSTM(\n                in_out_channels, int(in_out_channels / 2), num_layers=1, batch_first=True, bias=True, bidirectional=True\n            )\n        else:\n            self.lstm = None\n        # self.rnn_state = None\n        self.channels = in_out_channels\n\n    ### this function currently duplicated because torchscript is weird with inheritance\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n\n    @torch.jit.ignore\n    def encode(self, x, mask=None):\n        # if mask is not None:\n            # raise NotImplementedError(\"please implement mask\")\n        input_lengths = (x&gt;0).long().sum(-1)\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        o = self.embed(x%self.n_embeddings).transpose(-1,-2)\n        for layer in self.convolutions:\n            o = o + layer(o, mask)\n        o = o.transpose(-1, -2)\n        if self.lstm is not None:\n            o = nn.utils.rnn.pack_padded_sequence(\n                o, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n            self.lstm.flatten_parameters()\n            o, _ = self.lstm(o)\n            o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n        return o\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.ZeroEncoder","title":"<code>ZeroEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Just character embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>in_out_channels</code> <code>int</code> <p>number of input and output channels.</p> required Shapes <ul> <li>input: LongTensor (B, T)</li> <li>output: (B, T, D)</li> </ul> Source code in <code>src/tungnaa/text.py</code> <pre><code>class ZeroEncoder(nn.Module):\n    r\"\"\"Just character embeddings.\n\n    Args:\n        in_out_channels (int): number of input and output channels.\n\n    Shapes:\n        - input: LongTensor (B, T)\n        - output: (B, T, D)\n    \"\"\"\n    def __init__(self, channels=768, end_tokens=True):\n        super().__init__()\n        self.end_tokens = [57344, 57345] if end_tokens else None\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        # unicode points &gt; 1024 may collide\n        self.n_embeddings = 1025\n        self.embed = nn.Embedding(self.n_embeddings, channels, padding_idx=0)\n\n        self.channels = channels\n\n    ### this function currently duplicated because torchscript is weird with inheritance\n    @torch.jit.ignore\n    def tokenize(self, text):\n        return tokenize(text, self.end_tokens)\n\n    @torch.jit.ignore\n    def encode(self, x, mask=None):\n        # if mask is not None:\n            # raise NotImplementedError(\"please implement mask\")\n        # NOTE: stupid hack to support arbitrary number of embeddings\n        return self.embed(x%self.n_embeddings)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.lev","title":"<code>lev(t1, t2)</code>  <code>cached</code>","text":"<p>Returns:</p> Type Description <p>levenshtein distance between t1 and t2</p> <p>string representing edits from t1 to t2 - delete + insert ^ edit . no change</p> <p>list giving the corresponding index in t2 for each position in t1</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>@ft.cache\ndef lev(t1, t2):\n    \"\"\"\n    Returns:\n        levenshtein distance between t1 and t2\n        string representing edits from t1 to t2\n            - delete\n            + insert\n            ^ edit\n            . no change\n        list giving the corresponding index in t2 for each position in t1\n    \"\"\"\n    # print(t1, t2)\n    if len(t1)==0:\n        return len(t2), '+'*len(t2), []\n    elif len(t2)==0:\n        return len(t1), '-'*len(t1), [0]*len(t1)\n\n    hd1, tl1 = t1[0], t1[1:]\n    hd2, tl2 = t2[0], t2[1:]\n\n    if hd1==hd2:\n        n, s, i = lev(tl1, tl2)\n        return n, '.'+s, [0]+[j+1 for j in i]\n    (n, s, i), c, f = min(\n        (lev(tl1, tl2), '^', lambda i: [0]+[j+1 for j in i]), \n        (lev(tl1, t2), '-', lambda i: [0]+i), \n        (lev(t1, tl2), '+', lambda i: [j+1 for j in i]))\n    return n+1, c+s, f(i)\n</code></pre>"},{"location":"reference/tungnaa/text/#tungnaa.text.tokenize","title":"<code>tokenize(text, end_tokens)</code>","text":"<p>Returns:</p> Type Description <p>tokenized text as LongTensor</p> <p>representation of tokenized text as a string</p> <p>list mapping index in the original text to the tokenized text</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>def tokenize(text:str, end_tokens:Tuple[int,int]|None):\n    \"\"\"\n    Returns:\n        tokenized text as LongTensor\n        representation of tokenized text as a string\n        list mapping index in the original text to the tokenized text\n    \"\"\"\n    n = len(text)\n\n    start_tok = [end_tokens[0]] if end_tokens else []\n    end_tok = [end_tokens[1]] if end_tokens else []\n    # if len(text)==0:\n    #     return torch.empty((1,0), dtype=torch.long)\n    tok = torch.tensor([\n        start_tok + [ord(char) for char in text] + end_tok\n        ])\n    rep = ' '+text+' ' if end_tokens else text\n    idx_map = list(range(1,n+1) if end_tokens else range(n))\n    return tok, rep, idx_map\n</code></pre>"},{"location":"reference/tungnaa/train/","title":"Train","text":""},{"location":"reference/tungnaa/train/#tungnaa.train.Trainer","title":"<code>Trainer</code>","text":"<p>Instantiate a Trainer object</p> <p>follow with a <code>train</code> subcommand to start training</p> <p>Parameters:</p> Name Type Description Default <code>experiment</code> <code>str</code> <p>experiment name</p> required <code>model_dir</code> <code>Path</code> <p>where to store checkpoints</p> required <code>log_dir</code> <code>Path</code> <p>where to store tensorboard logs</p> required <code>manifest</code> <code>Path</code> <p>path to HiFiTTS-style json manifest</p> required <code>rave_model</code> <code>Path</code> <p>path to vocoder .ts file, for logs and determining latent size</p> <code>None</code> <code>csv</code> <code>Path</code> <p>path to additional annotations in a CSV file currently based on the format of <code>jvs_labels_encoder_k7.csv</code> the first column should be the name of the audio file without extension the second column will be added to the text, separated by a colon: \"val:original text\"</p> <code>None</code> <code>concat_speakers</code> <code>int</code> <p>number of utterances to concatenate. for each training example, this will load n utterances from the dataset, apply any annotations from <code>csv</code> or <code>speaker_annotate</code>, then concatenate the texts and audio.</p> <code>0</code> <code>strip_quotes</code> <code>bool</code> <p>remove all double quotes from text</p> <code>True</code> <code>speaker_annotate</code> <code>bool</code> <p>prepend speaker id to text, as \"[speaker]\"</p> <code>False</code> <code>speaker_dataset</code> <code>bool</code> <p>when speaker_annotate is True, also prepend speaker dataset id to text, as \"[dataset:speaker]\"</p> <code>False</code> <code>results_dir</code> <code>Path</code> <p>where to store results</p> <code>None</code> <code>model</code> <p>dict of model constructor overrides e.g. '{text_encoder:{end_tokens:True}, likelihood_type:nsf, flow_blocks:2, nsf_bins:16, prenet_dropout:0.2, rnn_size:1200, decoder_layers:1, frame_channels:11}'</p> <code>None</code> <code>freeze_text</code> <code>bool</code> <p>freeze text encoder</p> <code>False</code> <code>freeze_embeddings</code> <code>bool</code> <p>freeze text encoder embeddings only</p> <code>True</code> <code>init_text</code> <code>bool</code> <p>reinitialize pretrained text encoder</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>training batch dimensions</p> <code>32</code> <code>batch_max_tokens</code> <code>int</code> <p>max text length during training, in tokens. should be set to usually result in longer text than audio. (this depends on the vododer rate and tokens/second of the speech)</p> <code>256</code> <code>batch_max_frames</code> <code>int</code> <p>max audio length during training, in vocoder frames. should be set to usually result in longer text than audio. (this depends on the vododer rate and tokens/second of the speech)</p> <code>512</code> <code>lr</code> <code>float</code> <p>learning rate</p> <code>0.0003</code> <code>lr_text</code> <code>float</code> <p>override learning rate for text encoder</p> <code>None</code> <code>adam_betas</code> <code>Tuple[float, float]</code> <p>AdamW optimizer beta parameters</p> <code>(0.9, 0.998)</code> <code>adam_eps</code> <code>float</code> <p>AdamW optimizer epsilon parameter</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>AdamW optimizer weight decay parameter</p> <code>1e-06</code> <code>grad_clip</code> <code>float</code> <p>gradient clipping</p> <code>5.0</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>0</code> <code>n_jobs</code> <code>int</code> <p>dataloader parallelism</p> <code>4</code> <code>device</code> <code>str</code> <p>training device, e.g. 'cpu', 'cuda:0'</p> <code>'cuda:0'</code> <code>epoch_size</code> <code>int</code> <p>in iterations, None for whole dataset</p> <code>None</code> <code>valid_size</code> <code>int</code> <p>set size of validation set, in batches may be useful if validation set is very large or small</p> <code>None</code> <code>save_epochs</code> <code>int</code> <p>save checkpoint every so many epochs</p> <code>1</code> <code>nll_scale</code> <code>float</code> <p>scale NLL loss</p> <code>1</code> <code>data_portion</code> <code>float</code> <p>fraction of dataset to train on (for restricted data experiments)</p> <code>1</code> <code>debug_loading</code> <code>bool</code> <p>debug dataloading</p> <code>False</code> <code>jit</code> <code>bool</code> <p>attempt to compile model with torch.jit.script</p> <code>False</code> <code>checkpoint</code> <code>Path</code> <p>file for resuming training or transfer learning</p> <code>None</code> <code>resume</code> <code>bool</code> <p>if True, resume training, otherwise transfer weights</p> <code>True</code> <code>rand_text_subs</code> <p>dict of text substitutions</p> <code>None</code> <code>style_annotate</code> <p>probability of prepending style annotations</p> <code>0</code> Source code in <code>src/tungnaa/train.py</code> <pre><code>class Trainer:\n    \"\"\"Instantiate a Trainer object\n\n    follow with a `train` subcommand to start training\n\n    Args:\n        experiment: experiment name\n        model_dir: where to store checkpoints\n        log_dir: where to store tensorboard logs\n        manifest: path to HiFiTTS-style json manifest\n        rave_model: path to vocoder .ts file, for logs and determining latent size\n        csv: path to additional annotations in a CSV file\n            currently based on the format of `jvs_labels_encoder_k7.csv`\n            the first column should be the name of the audio file without extension\n            the second column will be added to the text, separated by a colon:\n            \"val:original text\"\n        concat_speakers: number of utterances to concatenate.\n            for each training example, this will load n utterances from the dataset,\n            apply any annotations from `csv` or `speaker_annotate`,\n            then concatenate the texts and audio.\n        strip_quotes: remove all double quotes from text\n        speaker_annotate: prepend speaker id to text, as \"[speaker]\"\n        speaker_dataset: when speaker_annotate is True,\n            also prepend speaker dataset id to text, as \"[dataset:speaker]\"\n        results_dir: where to store results\n        model: dict of model constructor overrides\n            e.g. '{text_encoder:{end_tokens:True}, likelihood_type:nsf, flow_blocks:2, nsf_bins:16, prenet_dropout:0.2, rnn_size:1200, decoder_layers:1, frame_channels:11}'\n        freeze_text: freeze text encoder\n        freeze_embeddings: freeze text encoder embeddings only\n        init_text: reinitialize pretrained text encoder\n        batch_size: training batch dimensions\n        batch_max_tokens: max text length during training, in tokens.\n            should be set to usually result in longer text than audio.\n            (this depends on the vododer rate and tokens/second of the speech)\n        batch_max_frames: max audio length during training, in vocoder frames.\n            should be set to usually result in longer text than audio.\n            (this depends on the vododer rate and tokens/second of the speech)\n        lr: learning rate\n        lr_text: override learning rate for text encoder\n        adam_betas: AdamW optimizer beta parameters\n        adam_eps: AdamW optimizer epsilon parameter\n        weight_decay: AdamW optimizer weight decay parameter\n        grad_clip: gradient clipping\n        seed: random seed\n        n_jobs: dataloader parallelism\n        device: training device, e.g. 'cpu', 'cuda:0'\n        epoch_size: in iterations, None for whole dataset\n        valid_size: set size of validation set, in batches\n            may be useful if validation set is very large or small\n        save_epochs: save checkpoint every so many epochs\n        nll_scale: scale NLL loss\n        data_portion: fraction of dataset to train on\n            (for restricted data experiments)\n        debug_loading: debug dataloading\n        jit: attempt to compile model with torch.jit.script\n        checkpoint: file for resuming training or transfer learning\n        resume: if True, resume training, otherwise transfer weights\n        rand_text_subs: dict of text substitutions\n        style_annotate: probability of prepending style annotations\n    \"\"\"\n    def __init__(self, \n        experiment:str, \n        model_dir:Path,\n        log_dir:Path, \n        manifest:Path,\n        rave_model:Path = None,\n        csv:Path=None,\n        concat_speakers:int = 0,\n        strip_quotes:bool = True,\n        speaker_annotate:bool = False,\n        speaker_dataset:bool = False,\n        results_dir:Path = None,\n        model = None, \n        freeze_text:bool = False,\n        freeze_embeddings:bool = True,\n        init_text:bool = False, \n        batch_size:int = 32,\n        # TODO: specify estimated tokens / frame ? \n        batch_max_tokens:int = 256,\n        batch_max_frames:int = 512, \n        lr:float = 3e-4,\n        lr_text:float = None,\n        adam_betas:Tuple[float,float] = (0.9, 0.998),\n        adam_eps:float = 1e-08, \n        weight_decay:float = 1e-6,\n        grad_clip:float = 5.0,\n        seed:int = 0, \n        n_jobs:int = 4,\n        device:str = 'cuda:0',\n        epoch_size:int = None,\n        valid_size:int = None,\n        save_epochs:int = 1,\n        max_epochs:int = None,\n        nll_scale:float = 1, \n        dispersion_scale:float = 0, \n        dispersion_cutoff:float = 0,\n        concentration_scale:float = 0, \n        concentration_norm_scale:float = 0, \n        concentration_cutoff:float = 0, \n        # dispersion_l2_scale:float = 0, \n        # concentration_l2_scale:float = 0, \n        # TODO: anneal_prenet = None, # number of epochs to anneal prenet dropout to zero\n        data_portion:float = 1,\n        debug_loading:bool = False,\n        jit:bool = False,\n        compile:bool = False,\n        checkpoint:Path = None,\n        resume:bool = True,\n        rand_text_subs = None,\n        replace_runs = False,\n        style_annotate = 0,\n        drop_prefix = None\n        ):\n\n        kw = dict(locals()); kw.pop('self')\n\n        kw.pop('checkpoint')\n        if checkpoint is not None:\n            print(f'loading checkpoint {checkpoint}')\n            checkpoint = torch.load(checkpoint, map_location=torch.device('cpu'))\n            # merges sub dicts, e.g. model hyperparameters\n            deep_update(checkpoint['kw'], kw) # provided arguments override stored\n            kw = checkpoint['kw']\n            load = lambda: self.load_state(checkpoint, resume=resume)\n        else:\n            load = lambda: None\n\n        # store all hyperparams for checkpointing\n        self.kw = kw\n\n        self.best_step = None\n        self.best_loss = np.inf\n\n        # get model defaults from model class\n        model_cls = TacotronDecoder\n        if model is None: model = {}\n        assert isinstance(model, dict), \"\"\"\n            model keywords are not a dict. check shell/fire syntax\n            \"\"\"\n        kw['model'] = model = get_class_defaults(model_cls) | model\n\n        # assign all arguments to self by default\n        self.__dict__.update(kw)\n        # mutate some arguments:\n        # self.lr_text = self.lr_text or self.lr\n        self.model_dir = Path(model_dir) / self.experiment\n        self.log_dir = Path(log_dir) / self.experiment\n        if results_dir is None:\n            self.results_dir = None\n        else:\n            self.results_dir = Path(results_dir) / self.experiment\n        self.manifest = Path(manifest)\n        self.device = torch.device(device)\n\n        # filesystem\n        for di in (self.model_dir, self.log_dir, self.results_dir):\n            if di is not None:\n                di.mkdir(parents=True, exist_ok=True)\n\n        if rave_model is None:\n            self.rave_model = None\n        else:\n            self.rave_model = torch.jit.load(rave_model)\n            model['block_size'] = self.rave_model.encode_params[3]\n            model['frame_channels'] = self.rave_model.latent_size\n\n        # random states\n        self.seed_random()\n\n        # logging\n        self.writer = SummaryWriter(self.log_dir)\n\n        # Trainer state\n        self.iteration = 0\n        self.exposure = 0\n        self.epoch = 0\n\n        if jit:\n            kw['model']['script'] = True\n\n        # construct model from arguments \n        self.model = model_cls(**model)\n        tqdm.write(repr(self.model))\n\n        tqdm.write(f'{sum(p.numel() for p in self.model.parameters())} parameters')\n\n        if init_text:\n            self.model.text_encoder.init()\n        if freeze_text:\n            self.model.text_encoder.requires_grad_(False)\n        if freeze_embeddings:\n            for n,m in self.model.text_encoder.named_modules():\n                if isinstance(m, torch.nn.Embedding):\n                    tqdm.write(f'freezing {n}')\n                    m.requires_grad_(False)\n\n        tqdm.write(f'{sum(p.numel() for p in self.model.parameters() if p.requires_grad)} trainable parameters')\n\n        tqdm.write(f'moving model to {self.device}')\n        self.model = self.model.to(self.device)\n\n        try:\n            self.model.core.attention_rnn.flatten_parameters()\n        except Exception:\n            tqdm.write('did not flatten attention_rnn parameters')\n        try:\n            self.model.core.decoder_rnn.flatten_parameters()\n            tqdm.write('did not flatten decoder_rnn parameters')\n        except Exception:\n            pass\n\n        # if jit:\n            # print(f'scripting model')\n            # self.model = torch.jit.script(self.model)\n            # self.model.core_loop = torch.jit.script(self.model.core_loop)\n\n        if compile:\n            tqdm.write(f'compiling model')\n            self.model.core.compile(\n                dynamic=True, \n                mode='reduce-overhead'\n                )\n            # self.model.compile(\n            #     dynamic=True, \n            #     mode='reduce-overhead'\n            #     )\n\n        tqdm.write(f'constructing datasets')\n        self.train_dataset, self.valid_dataset, self.test_dataset = get_datasets(\n            manifest, \n            data_portion=data_portion, \n            # concat_speakers=concat_speakers,\n            csv_file=csv, \n            max_tokens=batch_max_tokens, \n            max_frames=batch_max_frames,\n            speaker_annotate=speaker_annotate,\n            speaker_dataset=speaker_dataset,\n            strip_quotes=strip_quotes, \n            rave=self.rave_model,\n            text_encoder=self.model.text_encoder,\n            rand_text_subs=rand_text_subs,\n            style_annotate=style_annotate,\n            replace_runs=replace_runs\n        )\n        for tag, ds in (('train', self.train_dataset), ('valid', self.valid_dataset)):\n            time_s = 0\n            text_c = 0\n            for item in ds:\n                time_s += item['audio'].shape[1] * self.rave_model.encode_params[3] / self.rave_model.sr\n                text_c += len(item['plain_text'])\n            tqdm.write(f'{tag} dataset length: {len(ds)} utterances, {time_s} seconds, {text_c} characters')\n\n        self.valid_size = valid_size or len(self.valid_dataset)//batch_size\n\n        if concat_speakers &gt; 1:\n            self.train_dataset = ConcatSpeakers(self.train_dataset, concat_speakers, drop_prefix=drop_prefix)\n            self.valid_dataset = ConcatSpeakers(self.valid_dataset, concat_speakers, drop_prefix=drop_prefix)\n            self.test_dataset = ConcatSpeakers(self.test_dataset, concat_speakers, drop_prefix=drop_prefix)\n\n\n        tqdm.write(f'creating optimizer')\n        if self.lr_text is None:\n            self.lr_text = self.lr\n        params = [{\n            'params':p, 'lr':(self.lr_text if 'text_encoder' in n else self.lr)\n        } for n, p in self.model.named_parameters()]\n\n        self.opt = torch.optim.AdamW(params,\n            self.lr, self.adam_betas, self.adam_eps, self.weight_decay)\n\n        load()\n\n\n    @property\n    def gpu(self):\n        return self.device.type!='cpu'\n\n    def seed_random(self):\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n\n    def set_random_state(self, states):\n        # note: GPU rng state not handled\n        std_state, np_state, torch_state = states\n        random.setstate(std_state)\n        np.random.set_state(np_state)\n        torch.set_rng_state(torch_state)\n\n    @property\n    def step(self):\n        return self.exposure, self.iteration, self.epoch\n\n    def save(self, fname):\n        torch.save(dict(\n            kw=self.kw,\n            model_state=self.model.state_dict(),\n            optimizer_state=self.opt.state_dict(),\n            step=self.step,\n            best_loss=self.best_loss,\n            best_step=self.best_step,\n            random_state=(random.getstate(), np.random.get_state(), torch.get_rng_state())\n        ), fname)\n\n    def load_state(self, d, resume):\n        print(f'{resume=}')\n        d = d if hasattr(d, '__getitem__') else torch.load(d)\n        sd = d['model_state']\n        sd = self.model.update_state_dict(sd)\n        if not resume:\n            msd = self.model.state_dict()\n            for k in list(sd):\n                print(k, sd[k].shape, msd[k].shape)\n                if k not in msd or sd[k].shape != msd[k].shape:\n                    print(f'skipping {k}')\n                    sd.pop(k)\n        self.model.load_state_dict(sd, strict=resume)\n        # self.model.load_state_dict(d['model_state'], strict=resume)\n        if resume:\n            print('loading optimizer state, RNG state, step counts')\n            print(\"\"\"\n            warning: optimizer lr, beta etc are restored with optimizer state,\n            even if different values given on the command line, when resume=True\n            \"\"\")\n            self.opt.load_state_dict(d['optimizer_state'])\n            self.exposure, self.iteration, self.epoch = d['step']\n            self.set_random_state(d['random_state'])\n            try:\n                self.best_loss = d['best_loss']\n                self.best_step = d['best_step']\n            except KeyError:\n                print('old checkpoint: no best_loss')\n        else:\n            print('fresh run transferring only model weights')\n\n    def log(self, tag, d):\n        # self.writer.add_scalars(tag, d, self.exposure)\n        for k,v in d.items():\n            self.writer.add_scalar(f'{tag}/{k}', v, self.exposure)\n\n    def process_grad(self):\n        r = {}\n        if self.grad_clip is not None:\n            r['grad_l2'] = torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(), self.grad_clip, error_if_nonfinite=True)\n        return r\n\n    def get_loss_components(self, result):\n        # return {'error': result['mse']}\n        return {\n            'nll': result['nll'].mean()*self.nll_scale,\n            'dispersion': (\n                result['dispersion']\n                    .clip(self.dispersion_cutoff, 1e9)\n                    .mean()\n                    *self.dispersion_scale),\n            'concentration': (\n                result['concentration']\n                    .mean()\n                    *self.concentration_scale),\n            'concentration_norm': (\n                result['concentration_norm']\n                    .clip(self.concentration_cutoff, 1e9)\n                    .mean()\n                    *self.concentration_norm_scale),\n            # 'dispersion_l2': result['dispersion_l2'].mean()*self.dispersion_l2_scale,\n            # 'concentration_l2': result['concentration_l2'].mean()*self.concentration_l2_scale,\n            }\n\n    def forward(self, batch):\n        audio_lengths = batch['audio_mask'].sum(-1).cpu()\n        text_mask = batch['text_mask'].to(self.device, non_blocking=True)\n        audio_mask = batch['audio_mask'].to(self.device, non_blocking=True)\n        if self.model.text_encoder is not None:\n            text = batch['text'].to(self.device, non_blocking=True)\n        else:\n            text = batch['text_emb']\n            if text is None:\n                raise ValueError(\"\"\"\n                    no text embeddings in dataset but no text encoder in model\n                \"\"\")\n            text = text.to(self.device, non_blocking=True)\n        audio = batch['audio'].to(self.device, non_blocking=True)\n        # torch.compiler.cudagraph_mark_step_begin()\n        return self.model(\n            text, audio, text_mask, audio_mask, audio_lengths, \n            chunk_pad_text=128 if self.compile else None)\n\n    def get_scalars(self, d):\n        r = {}\n        for k,v in d.items():\n            if v.numel()==1:\n                r[k] = v.item()\n            elif v.shape==(self.batch_size,):\n                r[k] = v.mean().item()\n        return r\n\n    def _validate(self, valid_loader, ar_mask=None):\n        \"\"\"\"\"\"\n        pops = defaultdict(list)\n        self.model.eval()\n        i = 0\n        # for batch in tqdm(valid_loader, desc=f'validating epoch {self.epoch}'):\n        vs = self.valid_size\n        for batch in tqdm(\n                it.islice(it.chain.from_iterable(it.repeat(valid_loader)), vs), \n                desc=f'validating epoch {self.epoch}', total=vs):\n            with torch.no_grad():\n                result = self.forward(batch)\n                losses = {\n                    k:v.item() \n                    for k,v in self.get_loss_components(result).items()}\n                for k,v in losses.items():\n                    pops[k].append(v)\n                pops['loss'].append(sum(losses.values()))\n                # pops['mse'].append(result['mse'].item())\n                # pops['nll_fixed'].append(result['nll_fixed'].item())\n                # pops['nll_posthoc'].append(result['nll_posthoc'].item())\n                for k,v in self.get_scalars(result).items():\n                    pops[k].append(v)\n                # for k,v in result.items():\n                    # NOTE: here (and in train loop), could check for batch size vector and take mean\n                    # that would allow processing losses per batch item in training script\n                    # if v.numel()==1:\n                        # pops[k].append(v.item())\n            if i==0:\n                self.rich_logs('valid', batch, result)\n                i+=1\n        return {\n            'logs':{k:np.mean(v) for k,v in pops.items()},\n            'pops':pops\n        }\n\n    def train(self):\n        \"\"\"Entry point to model training\"\"\"\n        try:\n            self._train()\n        except Exception as e:\n            import traceback; traceback.print_exc()\n            import pdb; pdb.post_mortem()\n\n    def _train(self):\n        # self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n\n        # TODO: can remove this?\n        def get_collate(ds):\n            while True:\n                try:\n                    return ds.collate_fn()\n                except AttributeError:\n                    ds = ds.dataset\n\n        train_loader = DataLoader(\n            self.train_dataset, self.batch_size,\n            shuffle=not isinstance(\n                self.train_dataset, torch.utils.data.IterableDataset),\n            num_workers=self.n_jobs, pin_memory=self.gpu,\n            worker_init_fn=getattr(self.train_dataset, 'worker_init', None),\n            persistent_workers=True,\n            collate_fn=get_collate(self.train_dataset),\n            drop_last=True)\n\n        valid_loader = DataLoader(\n            self.valid_dataset, self.batch_size,\n            shuffle=False, num_workers=self.n_jobs, pin_memory=self.gpu,\n            worker_init_fn=getattr(self.valid_dataset, 'worker_init', None),\n            persistent_workers=False,\n            collate_fn=get_collate(self.valid_dataset))\n\n        ##### validation loop\n        def run_validation():\n            if self.debug_loading: return\n            logs = self._validate(valid_loader)['logs']\n            self.log('valid', logs)\n            if logs['loss'] &lt; self.best_loss:\n                self.best_loss = logs['loss']\n                self.best_step = self.step\n                self.save(self.model_dir / f'best.ckpt')\n\n        try:\n            epoch_size = self.epoch_size or len(train_loader)\n        except TypeError:\n            raise ValueError(\"specify epoch_size when using IterableDataset\")\n\n        # validate at initialization\n        run_validation()\n\n        if self.debug_loading:\n            f = open('debug.txt', 'w')\n\n        while self.max_epochs is None or self.epoch &lt; self.max_epochs:\n            self.epoch += 1\n\n            ##### training loop\n            self.model.train()\n            for batch in tqdm(\n                # itertools incantation to support epoch_size larger than train set\n                # NOTE important to use persistent_workers in DataLoader!\n                # otherwise RNG gets repeated\n                it.islice(\n                    it.chain.from_iterable(it.repeat(train_loader)), epoch_size), \n                desc=f'training epoch {self.epoch}', total=epoch_size\n                ):\n\n                self.iteration += 1\n                self.exposure += self.batch_size\n                logs = {}\n\n                if self.debug_loading:\n                    f.write(f'{self.epoch}, {self.iteration}, {list(batch[\"index\"].numpy())}\\n')\n                    continue\n\n                ### forward+backward+optimizer step ###\n                self.opt.zero_grad(set_to_none=True)\n                result = self.forward(batch)\n                losses = self.get_loss_components(result)\n                loss = sum(losses.values())\n                loss.backward()\n                logs |= self.process_grad()\n                self.opt.step()\n                ########\n\n                # log loss components\n                logs |= {f'loss/{k}':v.item() for k,v in losses.items()}\n                # log total loss\n                logs |= {'loss':loss.item()}\n                # log any other returned scalars\n                # logs |= {k:v.item() for k,v in result.items() if v.numel()==1}\n                logs |= self.get_scalars(result)\n                # other logs\n                self.log('train', logs)\n\n            if self.epoch%self.save_epochs == 0: \n                self.save(self.model_dir / f'{self.epoch:04d}.ckpt')\n            self.save(self.model_dir / f'last.ckpt')\n            run_validation()\n\n\n    def add_audio(self, tag, audio):\n        try:\n            sr = self.rave_model.sampling_rate\n        except Exception:\n            sr = self.rave_model.sr\n        self.writer.add_audio(\n            tag, audio, \n            global_step=self.epoch, sample_rate=int(sr))\n\n    def wrap_text(self, s, n):\n        w = ''\n        while s!='':\n            w = w+'\\n'+s[:n]\n            s = s[n:]\n        return w\n\n    def rich_logs(self, tag, batch, result):\n        \"\"\"runs RAVE inference and logs audio\"\"\"\n        z = result['predicted'].detach().cpu()\n        gt = result['ground_truth'].detach().cpu()\n        a = result['alignment'].detach().cpu()\n        am = result['audio_mask'].detach().cpu()\n        tm = result['text_mask'].detach().cpu()\n        for i in tqdm(range(3), desc='rich logs', leave=False):\n            nt = tm[i].sum()\n            na = am[i].sum()\n            z_i = z[i, :na].T[None]\n            gt_i = gt[i, :na].T[None]\n            with torch.inference_mode():\n                audio_tf = self.rave_model.decode(z_i)[0]\n                audio_gt = self.rave_model.decode(gt_i)[0]\n            self.add_audio(f'{tag}/audio/tf/{i}', audio_tf)\n            self.add_audio(f'{tag}/audio/gt/{i}', audio_gt)\n\n            with torch.inference_mode():\n                t = result['text'].detach()[i:i+1, :nt]\n                z_ar,_ = self.model.inference(\n                    t, stop=False, max_steps=int(na*1.2))\n                z_ar = z_ar.cpu().transpose(1,2)\n                z_ar_zero,_ = self.model.inference(\n                    t, stop=False, max_steps=int(na*1.2), temperature=0.0)\n                z_ar_zero = z_ar_zero.cpu().transpose(1,2)\n                z_ar_half,_ = self.model.inference(\n                    t, stop=False, max_steps=int(na*1.2), temperature=0.5)\n                z_ar_half = z_ar_half.cpu().transpose(1,2)\n                audio_ar = self.rave_model.decode(z_ar)[0]\n                audio_ar_zero = self.rave_model.decode(z_ar_zero)[0]\n                audio_ar_half = self.rave_model.decode(z_ar_half)[0]\n            self.add_audio(f'{tag}/audio/ar/{i}', audio_ar)\n            self.add_audio(f'{tag}/audio/ar/zerotemp/{i}', audio_ar_zero)\n            self.add_audio(f'{tag}/audio/ar/halftemp/{i}', audio_ar_half)\n\n            fig = plt.figure()\n            a_i = a[i, :na, :nt].T\n            plt.imshow(\n                a_i, \n                interpolation='nearest', \n                aspect='auto', \n                origin='lower')\n            plt.title(self.wrap_text(batch['plain_text'][i], 40))\n            plt.xlabel(self.wrap_text(batch['audio_path'][i], 64))\n            plt.tight_layout()\n            self.writer.add_figure(f'{tag}/align/{i}', fig, global_step=self.epoch)\n</code></pre>"},{"location":"reference/tungnaa/train/#tungnaa.train.Trainer.rich_logs","title":"<code>rich_logs(tag, batch, result)</code>","text":"<p>runs RAVE inference and logs audio</p> Source code in <code>src/tungnaa/train.py</code> <pre><code>def rich_logs(self, tag, batch, result):\n    \"\"\"runs RAVE inference and logs audio\"\"\"\n    z = result['predicted'].detach().cpu()\n    gt = result['ground_truth'].detach().cpu()\n    a = result['alignment'].detach().cpu()\n    am = result['audio_mask'].detach().cpu()\n    tm = result['text_mask'].detach().cpu()\n    for i in tqdm(range(3), desc='rich logs', leave=False):\n        nt = tm[i].sum()\n        na = am[i].sum()\n        z_i = z[i, :na].T[None]\n        gt_i = gt[i, :na].T[None]\n        with torch.inference_mode():\n            audio_tf = self.rave_model.decode(z_i)[0]\n            audio_gt = self.rave_model.decode(gt_i)[0]\n        self.add_audio(f'{tag}/audio/tf/{i}', audio_tf)\n        self.add_audio(f'{tag}/audio/gt/{i}', audio_gt)\n\n        with torch.inference_mode():\n            t = result['text'].detach()[i:i+1, :nt]\n            z_ar,_ = self.model.inference(\n                t, stop=False, max_steps=int(na*1.2))\n            z_ar = z_ar.cpu().transpose(1,2)\n            z_ar_zero,_ = self.model.inference(\n                t, stop=False, max_steps=int(na*1.2), temperature=0.0)\n            z_ar_zero = z_ar_zero.cpu().transpose(1,2)\n            z_ar_half,_ = self.model.inference(\n                t, stop=False, max_steps=int(na*1.2), temperature=0.5)\n            z_ar_half = z_ar_half.cpu().transpose(1,2)\n            audio_ar = self.rave_model.decode(z_ar)[0]\n            audio_ar_zero = self.rave_model.decode(z_ar_zero)[0]\n            audio_ar_half = self.rave_model.decode(z_ar_half)[0]\n        self.add_audio(f'{tag}/audio/ar/{i}', audio_ar)\n        self.add_audio(f'{tag}/audio/ar/zerotemp/{i}', audio_ar_zero)\n        self.add_audio(f'{tag}/audio/ar/halftemp/{i}', audio_ar_half)\n\n        fig = plt.figure()\n        a_i = a[i, :na, :nt].T\n        plt.imshow(\n            a_i, \n            interpolation='nearest', \n            aspect='auto', \n            origin='lower')\n        plt.title(self.wrap_text(batch['plain_text'][i], 40))\n        plt.xlabel(self.wrap_text(batch['audio_path'][i], 64))\n        plt.tight_layout()\n        self.writer.add_figure(f'{tag}/align/{i}', fig, global_step=self.epoch)\n</code></pre>"},{"location":"reference/tungnaa/train/#tungnaa.train.Trainer.train","title":"<code>train()</code>","text":"<p>Entry point to model training</p> Source code in <code>src/tungnaa/train.py</code> <pre><code>def train(self):\n    \"\"\"Entry point to model training\"\"\"\n    try:\n        self._train()\n    except Exception as e:\n        import traceback; traceback.print_exc()\n        import pdb; pdb.post_mortem()\n</code></pre>"},{"location":"reference/tungnaa/train/#tungnaa.train.resume","title":"<code>resume(checkpoint, resume=True, **kw)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <p>path to training checkpoint file</p> required <code>resume</code> <code>bool</code> <p>if True, restore optimizer states etc otherwise, restore only model weights (for transfer learning)</p> <code>True</code> Source code in <code>src/tungnaa/train.py</code> <pre><code>def resume(checkpoint, resume:bool=True, **kw):\n    \"\"\"\n    Args:\n        checkpoint: path to training checkpoint file\n        resume: if True, restore optimizer states etc\n            otherwise, restore only model weights (for transfer learning)\n    \"\"\"\n    d = torch.load(checkpoint, map_location=torch.device('cpu'))\n    print(f'loaded checkpoint {checkpoint}')\n    # merges sub dicts, e.g. model hyperparameters\n    deep_update(d['kw'], kw)\n    trainer = Trainer(**d['kw'])\n    trainer.load_state(d, resume=resume)\n\n    return trainer\n</code></pre>"},{"location":"reference/tungnaa/util/","title":"Util","text":""},{"location":"reference/tungnaa/util/#tungnaa.util.ConcatSpeakers","title":"<code>ConcatSpeakers</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Iterable-style dataset which combines two JSONDatasets</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>class ConcatSpeakers(torch.utils.data.IterableDataset):\n    \"\"\"Iterable-style dataset which combines two JSONDatasets\"\"\"\n    def __init__(self, dataset, n=2, end_tokens=True, drop_prefix=None):\n        super().__init__()\n        \"\"\"\n        \"\"\"\n        self.dataset = dataset\n        self.n = n\n        self.end_tokens = end_tokens\n        self.drop_prefix = drop_prefix #(prob, prefix)\n        self.iter_count = 0\n\n    def worker_init(self, i):\n        s = torch.initial_seed() ^ self.iter_count\n        random.seed(s)\n        torch.random.manual_seed(s)\n\n    def __iter__(self):\n        self.iter_count += 1\n        return self\n    def __next__(self):\n        # get a random element from each dataset\n        items = [\n            self.dataset[torch.randint(len(self.dataset),size=(1,)).item()]\n            for _ in range(self.n)\n        ]\n        # concat audio, text\n        # print(items)\n\n        text = ''.join(item['plain_text'] for item in items)\n\n        # randomly drop certain prefixes -- use this for dropping 'wildcard' annotations from the first utterance only, while always including them for subsequent utterances\n        if self.drop_prefix is not None:\n            s, p = self.drop_prefix\n            if text.startswith(s) and random.random() &lt; p:\n                text = text[len(s):]\n\n        return {\n            'audio': torch.cat([item['audio'] for item in items], 1),\n            'plain_text': text,\n            'audio_path': ';'.join(item['audio_path'] for item in items)\n        }\n\n    # @property\n    # def dataset(self):\n    #     # compat with split\n    #     return self\n\n    def collate_fn(self):\n        return self.datasets[0].dataset.collate_fn()\n</code></pre>"},{"location":"reference/tungnaa/util/#tungnaa.util.JSONDataset","title":"<code>JSONDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/tungnaa/util.py</code> <pre><code>class JSONDataset(torch.utils.data.Dataset):\n    \"\"\"\n    \"\"\"\n    def __init__(self, \n            manifest_file, csv_file, max_tokens, max_frames, \n            speaker_annotate=False, speaker_dataset=False, strip_quotes=False,\n            rave=None, text_encoder=None, return_path=False, rand_text_subs=None, style_annotate=0, replace_runs=False):\n        super().__init__()\n        \"\"\"\n        manifest_file: hifi-tts style json manifest file (see prep.py)\n        csv_file: csv file where first column is audio filename, \n            other columns contain additional metadata\n\n        rand_text_subs: list of (originals, replacement) token pairs\n        replace_runs: if True, always replace runs of the same token together\n        \"\"\"\n        self.root_dir = Path(manifest_file).parent\n        with open(manifest_file) as f:\n            self.index = [json.loads(line) for line in f]\n\n        self.csv_data = {}\n        if csv_file is not None:\n            with open(csv_file) as f:\n                for line in f:\n                    k,*vs = line.strip().split(',')\n                    self.csv_data[k] = vs\n            # print(self.csv_data)###DEBUG\n        self.max_frames = max_frames\n        self.max_tokens = max_tokens\n        self.speaker_annotate = speaker_annotate\n        self.speaker_dataset = speaker_dataset\n        self.style_annotate = style_annotate\n        self.strip_quotes = strip_quotes\n        self.rave = rave # needed for pitch models\n        self.text_encoder = text_encoder\n        self.return_path = return_path\n\n        if rand_text_subs is not None:\n            subs = defaultdict(list)\n            for ogs,t in rand_text_subs:\n                for c in ogs:\n                    subs[c].append(t)\n            self.rand_text_subs = dict(subs)\n        else:\n            self.rand_text_subs = None\n        self.replace_runs = replace_runs\n\n    def worker_init(self, i):\n        s = torch.initial_seed()\n        random.seed(s)\n\n    def __getitem__(self, i):\n        item = self.index[i]\n        if self.return_path:\n            return item['audio_path']\n\n        # pre-computed data augmentation\n        # if not exists, choose random\n        # TODO\n\n        if 'audio_std_path' in item:\n            # data augmentation if RAVE prep included posterior stddevs\n            audio, stddev = torch.load(choose_path(\n                self.root_dir / item['audio_std_path']))\n            stddev.mul_(torch.randn_like(stddev))\n            audio.add_(stddev)\n        else:\n            print(f'{item=}')\n            audio = torch.load(choose_path(\n                self.root_dir / item['audio_feature_path']))\n\n        if 'audio_pitch_path' in item:\n            pitch_probs = torch.load(choose_path(\n                self.root_dir / item['audio_pitch_path']))\n            pitch_bins = torch.distributions.Categorical(pitch_probs).sample()\n            pitch_hz = self.rave.pitch_encoder.bins_to_frequency(pitch_bins)\n            # pitch becomes first latent.\n            # print(pitch_hz.shape, audio.shape)\n            # slice off the last frame of audio if needed\n            # TODO: look into this discrepancy when input sizes aren't round...\n            # print(pitch_hz.shape, audio.shape)\n            audio = torch.cat((pitch_hz, audio[...,:pitch_hz.shape[-1]]), 1)\n\n        text = item['text']\n\n        if self.strip_quotes:\n            text = re.sub(_quotes_re, '', text)\n\n        if self.rand_text_subs is not None:\n            rate = random.random()**2\n            if self.replace_runs:\n                runs = []\n                for run in get_char_runs(text):\n                    c, n = run[0], len(run)\n                    if c in self.rand_text_subs and random.random() &lt; rate:\n                        toks = self.rand_text_subs[c]\n                        runs.append(random.choice(toks) * n)\n                    else:\n                        runs.append(run)\n                text = ''.join(runs)\n            else:\n                text = list(text)\n                for i in range(len(text)):\n                    if text[i] in self.rand_text_subs and random.random() &lt; rate:\n                        toks = self.rand_text_subs[text[i]]\n                        text[i] = random.choice(toks)\n                text = ''.join(text)\n            # print(text)\n\n        if self.speaker_annotate:\n            speaker = item['speaker']\n            if not self.speaker_dataset:\n                speaker = speaker.split(':')[1]\n            text = f'[{speaker}] {text}'\n\n        if self.style_annotate:\n            if random.random() &lt; self.style_annotate:\n                style = item['style']\n                # capitalize first letter of style\n                if style=='neutral':\n                    style = '|'\n                elif style=='aggressive':\n                    style = '+'\n                elif style=='happy':\n                    style = '^'\n                elif style=='worried':\n                    style = '&amp;'\n                else:\n                    print(f'warning: unrecognized style \"{style}\"')\n                    style = ''\n            else:\n                style = '%'\n            text = style + text\n\n\n        if len(self.csv_data):\n            k = item['audio_path']\n            # k = Path(k).name.replace('.flac', '.wav')\n            k = Path(k).name.split('.')[0]\n            # assert k in self.csv_data, k\n            if k not in self.csv_data:\n                print(f'WARNING: {k} not found in csv')\n            else:\n                text = f'{self.csv_data[k][0]}:{text}'\n            # print(text) ###DEBUG\n\n        r = {\n            'index': i,\n            'plain_text': text,\n            # batch x time x channel\n            'audio': audio.transpose(1,2),\n            'audio_path': item['audio_path'],\n        }\n        if 'text_feature_path' in item:\n            if self.speaker_annotate:\n                raise NotImplementedError\n            # batch x time x channel\n            r['emb_text'] = torch.load(\n                self.root_dir / item['text_feature_path'])\n        return r\n\n    def __len__(self):\n        return len(self.index)\n\n    def collate_fn(self):\n        \"\"\"\n        returns a function suitable for the collate_fn argument of a torch DataLoader\n        \"\"\"\n        def collate(batch):\n            \"\"\"\n            Args:\n                batch: list of dicts which are single data points\n\n            Return:\n                dict of batch tensors\n            \"\"\"\n            n_text = min(self.max_tokens, max(len(b['plain_text']) for b in batch))\n            n_audio = min(self.max_frames, max(b['audio'].shape[1] for b in batch))\n            text_idxs = []\n            text_embs = []\n            audio_ts = []\n            text_masks = []\n            audio_masks = []\n            # index_ts = []\n            for b in batch:\n                text_idx, *_ = self.text_encoder.tokenize(b['plain_text'])\n                # text_idx = torch.tensor(\n                    # [[ord(char) for char in b['plain_text']]])\n                if 'emb_text' in b:\n                    text_emb = b['emb_text']\n                    assert text_idx.shape[1] == text_emb.shape[1]\n                    text_embs.append(pad1(text_emb, n_text))\n                text_masks.append(torch.arange(n_text) &lt; text_idx.shape[1])\n                text_idxs.append(pad1(text_idx, n_text))\n                audio = b['audio']\n                audio_masks.append(torch.arange(n_audio) &lt; audio.shape[1])\n                audio_ts.append(pad1(audio, n_audio))\n                # index_ts.append(torch.LongTensor((b['index'],)))\n            return {\n                # 'index': torch.cat(index_ts, 0),\n                'plain_text': [b['plain_text'] for b in batch],\n                'audio_path': [b['audio_path'] for b in batch],\n                'text': torch.cat(text_idxs, 0),\n                'text_emb': torch.cat(text_embs, 0) if len(text_embs) else None,\n                'audio': torch.cat(audio_ts, 0),\n                'text_mask': torch.stack(text_masks, 0),\n                'audio_mask': torch.stack(audio_masks, 0),\n            }\n\n        return collate\n</code></pre>"},{"location":"reference/tungnaa/util/#tungnaa.util.JSONDataset.collate_fn","title":"<code>collate_fn()</code>","text":"<p>returns a function suitable for the collate_fn argument of a torch DataLoader</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def collate_fn(self):\n    \"\"\"\n    returns a function suitable for the collate_fn argument of a torch DataLoader\n    \"\"\"\n    def collate(batch):\n        \"\"\"\n        Args:\n            batch: list of dicts which are single data points\n\n        Return:\n            dict of batch tensors\n        \"\"\"\n        n_text = min(self.max_tokens, max(len(b['plain_text']) for b in batch))\n        n_audio = min(self.max_frames, max(b['audio'].shape[1] for b in batch))\n        text_idxs = []\n        text_embs = []\n        audio_ts = []\n        text_masks = []\n        audio_masks = []\n        # index_ts = []\n        for b in batch:\n            text_idx, *_ = self.text_encoder.tokenize(b['plain_text'])\n            # text_idx = torch.tensor(\n                # [[ord(char) for char in b['plain_text']]])\n            if 'emb_text' in b:\n                text_emb = b['emb_text']\n                assert text_idx.shape[1] == text_emb.shape[1]\n                text_embs.append(pad1(text_emb, n_text))\n            text_masks.append(torch.arange(n_text) &lt; text_idx.shape[1])\n            text_idxs.append(pad1(text_idx, n_text))\n            audio = b['audio']\n            audio_masks.append(torch.arange(n_audio) &lt; audio.shape[1])\n            audio_ts.append(pad1(audio, n_audio))\n            # index_ts.append(torch.LongTensor((b['index'],)))\n        return {\n            # 'index': torch.cat(index_ts, 0),\n            'plain_text': [b['plain_text'] for b in batch],\n            'audio_path': [b['audio_path'] for b in batch],\n            'text': torch.cat(text_idxs, 0),\n            'text_emb': torch.cat(text_embs, 0) if len(text_embs) else None,\n            'audio': torch.cat(audio_ts, 0),\n            'text_mask': torch.stack(text_masks, 0),\n            'audio_mask': torch.stack(audio_masks, 0),\n        }\n\n    return collate\n</code></pre>"},{"location":"reference/tungnaa/util/#tungnaa.util.deep_update","title":"<code>deep_update(a, b)</code>","text":"<p>in-place update a with contents of b, recursively for nested Mapping objects.</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def deep_update(a, b):\n    \"\"\"\n    in-place update a with contents of b, recursively for nested Mapping objects.\n    \"\"\"\n    for k in b:\n        if k in a and isinstance(a[k], Mapping) and isinstance(b[k], Mapping):\n            deep_update(a[k], b[k])\n        else:\n            a[k] = b[k]\n</code></pre>"},{"location":"reference/tungnaa/util/#tungnaa.util.get_class_defaults","title":"<code>get_class_defaults(cls)</code>","text":"<p>get the default argument values of a class constructor</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def get_class_defaults(cls):\n    \"\"\"get the default argument values of a class constructor\"\"\"\n    d = get_function_defaults(getattr(cls, '__init__'))\n    # ignore `self` argument, insist on default values\n    try:\n        d.pop('self')\n    except KeyError:\n        raise ValueError(\"\"\"\n            no `self` argument found in class __init__\n        \"\"\")\n    assert [v is not inspect._empty for v in d.values()], \"\"\"\n            get_class_defaults should be used on constructors with keyword arguments only.\n        \"\"\"\n    return d\n</code></pre>"},{"location":"reference/tungnaa/util/#tungnaa.util.get_function_defaults","title":"<code>get_function_defaults(fn)</code>","text":"<p>get dict of name:default for a function's arguments</p> Source code in <code>src/tungnaa/util.py</code> <pre><code>def get_function_defaults(fn):\n    \"\"\"get dict of name:default for a function's arguments\"\"\"\n    s = inspect.signature(fn)\n    return {k:v.default for k,v in s.parameters.items()}\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/","title":"init","text":""},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend","title":"<code>Backend</code>","text":"Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>class Backend:\n    def __init__(self,\n        checkpoint:str,\n        rave_path:str|None=None,\n        audio_in:str|None=None,\n        audio_out:str|int=None,\n        audio_block:int|None=None,\n        audio_channels:int|None=None,\n        # sample_rate:int=None,\n        synth_audio:bool|None=None,\n        latent_audio:bool|None=None,\n        latent_osc:bool|None=None,\n        osc_sender=None,\n        buffer_frames:int=1,\n        profile:bool=True,\n        jit:bool=False,\n        max_model_state_storage:int=512,\n        ):\n        \"\"\"\n        Args:\n            checkpoint: path to tungnaa checkpoint file\n            rave_path: path to rave vocoder to use python sound\n            audio_in: sounddevice input name/index if using python audio\n            audio_out: sounddevice output name/index if using python audio\n            audio_block: block size if using python audio\n            synth_audio: if True, vocode in python and send stereo audio\n            latent_audio: if True, pack latents into a mono audio channel\n            latent_osc: if True, send latents over OSC\n            buffer_frames: process ahead this many model steps\n            profile: if True, print performance profiling info\n            jit: if True, compile tungnaa model with torchscript\n        \"\"\"\n        self.jit = jit\n        self.profile = Profiler(profile)\n        # default output modes\n        if synth_audio is None:\n            synth_audio = rave_path is not None\n        if latent_audio is None:\n            latent_audio = rave_path is None\n        if latent_osc is None:\n            latent_osc = False\n\n        self.reset_values = {}\n\n        out_mode = OutMode(0)\n        if synth_audio: out_mode |= OutMode.SYNTH_AUDIO\n        if latent_audio: out_mode |= OutMode.LATENT_AUDIO\n        if latent_osc: out_mode |= OutMode.LATENT_OSC\n        self.out_mode = out_mode\n        print(f'{self.out_mode=}')\n\n        if audio_channels is None:\n            audio_channels = 0\n            if OutMode.SYNTH_AUDIO in self.out_mode:\n                self.synth_channels = (0,1)\n                audio_channels += 2\n            else:\n                self.synth_channels = tuple()\n            if OutMode.LATENT_AUDIO in self.out_mode:\n                self.latent_channels = (audio_channels,)\n                audio_channels += 1\n            else:\n                self.latent_channels = tuple()\n        else:\n            raise NotImplementedError(\n                \"setting audio_channels not currently supported\")\n        print(f'{audio_channels=}')\n\n        self.step_mode = StepMode.PAUSE\n        self.generate_stop_at_end = False\n        self.sampler_stop_at_end = False\n\n        self.latent_biases = []\n\n        # self.temperature = 0.5\n        self.temperature = 1.\n\n        # generation\n        self.gen_loop_start = 0\n        self.gen_loop_end = None\n        # sampler\n        self.sampler_loop_start = 0\n        self.sampler_loop_end = None\n        # self.sampler_utterance = -1\n        self.sampler_step = 0\n\n        self.osc_sender = osc_sender\n\n        self.frontend_conn = None\n\n        self.max_model_state_storage = max_model_state_storage\n\n        ### move heavy init out of __init__, so it only runs in child process\n        ### (Backend.run is called by Proxy._run)\n        self.init_args = (buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels)\n\n        # call this from both __init__ and run\n        # torch.multiprocessing.set_sharing_strategy('file_system')\n\n\n    def run(self, conn):\n        \"\"\"run method expected by Proxy\"\"\"\n        # call this from both __init__ and run\n        # torch.multiprocessing.set_sharing_strategy('file_system')\n\n        self.frontend_conn = conn\n        # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n        buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels = self.init_args\n\n        self.load_tts_model(checkpoint=checkpoint)\n\n        ### synthesis in python:\n        # load RAVE model\n        self.load_vocoder_model(rave_path=rave_path)\n\n        ### audio output:\n        # make sounddevice stream\n        if self.out_mode:\n            devicelist = sd.query_devices()\n\n            # None throws an error on linux, on mac it uses the default device\n            # Let's make this behavior explicit.\n            if audio_out is None:\n                audio_out = sd.default.device[1]\n\n            audio_device = None\n            for dev in devicelist:\n                if audio_out in [dev['index'], dev['name']]:\n                    audio_device = dev\n                    # audio_device_sr = dev['default_samplerate']\n                print(f\"{dev['index']}: '{dev['name']}' {dev['hostapi']} (I/O {dev['max_input_channels']}/{dev['max_input_channels']}) (SR: {dev['default_samplerate']})\")\n\n            # this should not be an error\n            # None uses the default device on macOS\n            # if audio_device is None:\n                # raise RuntimeError(f\"Audio device '{audio_out}' does not exist.\")\n\n            print(f\"USING AUDIO OUTPUT DEVICE {audio_out}:{audio_device}\")\n\n            self.active_frame:torch.Tensor = None \n            self.future_frame:Thread = None\n            self.frame_counter:int = 0\n\n            sd.default.device = audio_out\n            if self.rave_sr:\n                if audio_device and audio_device['default_samplerate'] != self.rave_sr:\n                    # this should not be an error. On OSX/CoreAudio you can set the device sample rate to the model sample rate.\n                    # however on Linux/JACK this throws a fatal error and stops program execution, requiring you to restart Jack to change the sampling rate\n\n                    # TODO: also check RAVE block size vs. audio device block size if possible\n                    print(\"\\n------------------------------------\");\n                    print(f\"WARNING: Device default sample rate ({audio_device['default_samplerate']}) and RAVE model sample rate ({self.rave_sr}) mismatch! You may need to change device sample rate manually on some platforms.\")\n                    print(\"------------------------------------\\n\");\n\n                sd.default.samplerate = self.rave_sr # could cause an error if device uses a different sr from model\n                print(f\"RAVE SAMPLING RATE: {self.rave_sr}\")\n                print(f\"DEVICE SAMPLING RATE: {audio_device['default_samplerate']}\")\n\n            # TODO: Tungnaa only uses audio output. Shouldn't we always be using sd.OutputStream?\n            try:\n                assert len(audio_out)==2\n                stream_cls = sd.Stream\n            except Exception:\n                stream_cls = sd.OutputStream\n\n            self.stream = stream_cls(\n                callback=self.audio_callback,\n                samplerate=self.rave_sr, \n                blocksize=audio_block, \n                #device=(audio_in, audio_out)\n                device=audio_out,\n                channels=audio_channels\n            )\n\n            if self.rave_sr:\n                assert self.stream.samplerate == self.rave_sr, f\"\"\"\n                failed to set sample rate to {self.rave_sr} from sounddevice\n                \"\"\"\n        else:\n            self.stream = None\n\n        self.text = None\n        self.text_rep = None\n        self.align_params = None\n        self.momentary_align_params = None\n        self.step_to_set = None\n\n        self.one_shot_paint = False\n        self.latent_feedback = False\n\n        self.lock = RLock()\n        self.text_update_thread = None\n\n        # self.frontend_q = Queue()\n        self.audio_q = Queue(buffer_frames)     \n        self.trigger_q = Queue(buffer_frames)     \n\n        self.states = []\n\n        self.needs_reset = False\n\n        self.step_thread = Thread(target=self.step_loop, daemon=True)\n        self.step_thread.start()\n\n    def step_loop(self):\n        \"\"\"model stepping thread\n\n        for each timestamp received in trigger_q\n        send audio frames in audio_q\n        \"\"\"\n        while True:\n            t = self.trigger_q.get()\n            with self.profile('step'):\n                frame = self.step(t)\n            if frame is not None:\n                # with self.profile('frame.numpy'):\n                frame = frame.numpy()\n            self.audio_q.put(frame)\n\n    def load_tts_model(self, checkpoint):\n        \"\"\"helper for loading TTS model, called on initialization but can also be called from GUI\"\"\"\n        self.model = TacotronDecoder.from_checkpoint(checkpoint)\n\n        # not scripting text encoder for now\n        # if self.model.text_encoder is None:\n            # self.text_model = TextEncoder()\n        # else:\n        self.text_model = self.model.text_encoder\n        self.model.text_encoder = None\n\n        # def _debug(m):\n        #     # print({(k,type(v)) for k,v in m.__dict__.items()})\n        #     for k,v in m.__dict__.items():\n        #         # print('ATTR', k)\n        #         if 'tensor' in str(type(v)).lower(): \n        #             print('TENSOR', k)\n        #     for m_ in m.modules():\n        #         if m_ != m:\n        #             # print('MODULE', m_)\n        #             _debug(m_)\n        # _debug(self.model)\n\n        if self.jit:\n            for m in self.model.modules():\n                if hasattr(m, 'parametrizations'):\n                    torch.nn.utils.parametrize.remove_parametrizations(\n                        m,'weight')\n            self.model = torch.jit.script(self.model)\n        self.model.eval()\n        # self.model.train()\n\n        print(f'{self.num_latents=}')\n\n        # print(f'{self.model.frame_channels=}')\n        self.use_pitch = hasattr(self.model, 'pitch_xform') and self.model.pitch_xform\n\n    def load_vocoder_model(self, rave_path):\n        \"\"\"helper for loading RAVE vocoder model, called on initialization but can also be called from GUI\"\"\"\n        # TODO: The audio engine sampling rate gets set depending on the sampling rate from the vocoder. \n        #   When loading a new vocoder model we need to add some logic to make sure the new vocoder has the same sample rate as the audio system.\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            assert rave_path is not None\n            self.rave = torch.jit.load(rave_path, map_location='cpu')\n            self.rave.eval()\n            self.block_size = int(self.rave.decode_params[1])\n            try:\n                self.rave_sr = int(self.rave.sampling_rate)\n            except Exception:\n                self.rave_sr = int(self.rave.sr)\n            with torch.inference_mode():\n                # warmup\n                if hasattr(self.rave, 'full_latent_size'):\n                    latent_size = self.rave.latent_size + int(\n                        hasattr(self.rave, 'pitch_encoder'))\n                else:\n                    latent_size = self.rave.cropped_latent_size\n                self.rave.decode(torch.zeros(1,latent_size,1))\n        else:\n            self.rave = None\n            self.rave_sr = None\n\n    @property\n    def num_latents(self):\n        return self.model.frame_channels\n\n    def start_stream(self):\n        \"\"\"helper for start/sampler\"\"\"\n        # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.LATENT_OSC):\n        #     if not self.loop_thread.is_alive():\n        #         self.run_thread = True\n        #         self.loop_thread.start()\n        # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.SYNTH_AUDIO):\n        # if self.out_mode is not None:\n        if not self.stream.active:\n            self.stream.start()\n\n    def generate(self):\n        \"\"\"\n        start autoregressive alignment &amp; latent frame generation\n        \"\"\"\n        # if self.step_mode != StepMode.GENERATION:\n            # self.needs_reset = True\n        self.step_mode = StepMode.GENERATION\n        self.start_stream()\n\n    def pause(self):\n        \"\"\"\n        pause generation or sampler\n        \"\"\"\n        self.step_mode = StepMode.PAUSE\n\n    def sampler(self):\n        \"\"\"\n        start sampler mode\n        \"\"\"\n        self.step_mode = StepMode.SAMPLER\n        self.start_stream()\n\n    def reset(self):\n        \"\"\"\n        reset the model state and alignments history\n        \"\"\"\n        self.needs_reset = True\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup any resources\n        \"\"\"\n        self.step_mode = StepMode.PAUSE\n        self.run_thread = False\n        # should probably do this more gracefully\n        exit(0) # exit the backend process\n\n    def set_text(self, text:str) -&gt; int:\n        \"\"\"\n        Compute embeddings for &amp; store a new text, replacing the old text.\n        Returns the number of embedding tokens\n\n        Args:\n            text: input text as a string\n\n        Returns:\n            length of text in tokens\n        \"\"\"\n        if (\n            self.text_update_thread is not None \n            and self.text_update_thread.is_alive()\n        ):\n            print('warning: text update still pending')\n\n        # TODO: more general text preprocessing\n        text, start, end = self.extract_loop_points(text)\n        tokens, text, idx_map = self.text_model.tokenize(text)\n        # print(start, end, idx_map)\n        # NOTE: positions in text may change fron tokenization (end tokens)\n        if start &lt; len(idx_map):\n            start = idx_map[start]\n        else:\n            start = 0\n        if end is not None and end &lt; len(idx_map):\n            end = idx_map[end]\n        else:\n            end = None\n\n        # text processing runs in its own thread\n        self.text_update_thread = Thread(\n            target=self._update_text, \n            args=(text, tokens, start, end), daemon=True)\n        self.text_update_thread.start()\n\n        return text\n\n    def extract_loop_points(self, text, tokens='&lt;&gt;'):\n        \"\"\"helper for `set_text`\"\"\"\n        start_tok, end_tok = tokens\n        # TODO: could look for matched brackets, have multiple loops...\n        start = text.find(start_tok) # -1 if not found\n        # if not len(text):\n        #     return text, None, None\n        if start &lt; 0:\n            start = 0\n        else:\n            text = text[:start]+text[start+1:]\n        end = text.find(end_tok) # -1 if not found\n        if end &lt; 0: \n            end = None\n        else:\n            text = text[:end]+text[end+1:]\n            end = max(0, end - 1)\n        # print(text, end)\n        return text, start, end\n\n    def _update_text(self, text, tokens, start, end):\n        \"\"\"runs in a thread\"\"\"\n        # store the length of the common prefix between old/new text\n        # if self.text is None:\n        #     self.prefix_len = 0\n        # else:\n        #     for i,(a,b) in enumerate(zip(text, self.text)):\n        #         print(i, a, b)\n        #         if a!=b: break\n        #     self.prefix_len = i\n\n        # store a mapping from old text positions to new\n        if self.text is None:\n            text_index_map = lambda x:x\n        else:\n            _,_,tm = lev(self.text, text)\n            text_index_map = lambda x: tm[x] if x &lt; len(tm) else len(text)-1\n\n        # lock should keep multiple threads from trying to run the text encoder\n        # at once in case `input_text` is called rapidly\n        with self.lock:\n            with torch.inference_mode():\n                self.reset_values = dict(\n                    text_rep = self.text_model.encode(tokens),\n                    text_index_map = text_index_map,\n                    text_tokens = tokens,\n                    text = text,\n                    gen_loop_start = start,\n                    gen_loop_end = end,\n                )\n            self.reset()\n\n    def set_biases(self, biases:List[float]):\n        self.latent_biases = biases\n\n    def set_alignment(self, \n            align_params:Optional[Tuple[float, float]]\n        ) -&gt; None:\n        \"\"\"\n        Send alignment parameters to the backend. If None is passed, alignment painting is off.\n\n        Args:\n            align_params: [loc, scale] or None\n        \"\"\"\n        self.align_params = align_params\n\n    def set_momentary_alignment(self,             \n            align_params:Optional[Tuple[float, float]]\n        ) -&gt; None:\n        \"\"\"\n        Alignment will be set on the next frame only\n        \"\"\"\n        self.momentary_align_params = align_params\n\n    def set_state_by_step(self, step):\n        self.step_to_set = step\n\n    def set_latent_feedback(self, b:bool):\n       self.latent_feedback = b\n\n    def set_generate_stop_at_end(self, b:bool):\n        self.generate_stop_at_end = b\n\n    def set_sampler_stop_at_end(self, b:bool):\n        self.sampler_stop_at_end = b\n\n    def set_sampler_step(self, step:int):\n        self.sampler_step = step % self.total_steps()\n\n    def set_sampler_loop_index(self, \n            start:int=None, end:int=None, \n            utterance:int=None, \n            reset:bool=True):\n        \"\"\"\n        Args:\n            start: loop start step\n            end: loop end step\n            utterance: sampler utterance\n            reset: if True, immediately go to loop start\n        \"\"\"\n        if len(self.states)==0: return\n\n        def wrap(x, n, tag):\n            if x is None: return x\n            if x &lt; 0: \n                x += n\n            if x &lt; 0 or x &gt;= n:\n                print(f'warning: out of bounds {tag} {x}')\n                x %= n\n            return x\n\n        if utterance is not None:\n            if start is not None:\n                start = self.utterance_to_global_step(utterance, start)\n            if end is not None:\n                end = self.utterance_to_global_step(utterance, end)\n\n        if start is None: start = self.sampler_loop_start        \n        if end is None: end = self.sampler_loop_end\n\n        # utterance = wrap(utterance, len(self.states), 'utterance')\n        start = wrap(start, len(self.states[utterance]), 'loop start')\n        end = wrap(end, len(self.states[utterance]), 'loop end')\n\n        # changed = utterance != self.sampler_utterance\n        # self.sampler_utterance = utterance\n\n        self.sampler_loop_start = start\n        self.sampler_loop_end = end\n\n        # must reset if changing utterance\n        if reset:\n            self.reset_sampler()\n\n    def set_sampler_loop_text(self, \n            text:str, n:int=-1, \n            start:bool=True, end:bool=True, \n            reset:bool=True):\n        \"\"\"\n        Args:\n            text: a regular expression string\n            n: index of occurrence in history\n            start: if True, set the loop start\n            end: if True, set the loop end\n            reset: if True, immediately go to loop start\n        \"\"\"\n        if not (start or end): return\n        r = text\n\n        # get all occurences of all matching strings,\n        #   and index by order in history\n\n        # first find matches for the regex in utterance texts\n        matches = []\n        utts = self.states #if n&gt;=0 else reversed(self.states)\n        index = 0\n        for utt_index, utt in enumerate(utts):\n            if len(utt)==0: continue\n            # text = utt[0]['text']\n            text = utt.text\n            for match in re.findall(r, text):\n                # error if multiple capture groups in regex\n                if not isinstance(match, str):\n                    raise ValueError(\"`set_sampler_loop`: multiple capture groups not supported\")\n                # add utterance, text index of match\n                index = index + text[index:].find(match)\n                matches.append((utt_index, index, index+len(match), match))\n                # print(f'{utt_index=}, {index=}, {match=}')\n\n        if len(matches)==0:\n            print(f'warning: no text matching \"{r}\" in `set_sampler_loop`')\n            return\n\n        # for matches, get occurences and flatten into list\n        #   simple: occurrence is from entering start to exiting end\n        #   TODO better: occurence is when there is a run which\n        #       enters the first token / leaves the last token\n        #       without any major jumps or reversals\n        #   TODO?: allow skips of first / last token?\n        occurrences = []\n        for u, i, j, m in matches:\n            occ_start = None\n            utt = self.states[u]\n            for k, state in enumerate(utt):\n                align = state['align_hard']\n                # print(align)\n\n                if align['enter_index']==i:\n                    # open/replace occurrence\n                    if occ_start is not None:\n                        print(f'warning: double open {m=} {occ_start=} {u=} {k=}')\n                    occ_start = k\n                if align['leave_index']==j:\n                    # close occurrence (or ignore)\n                    if occ_start is None: \n                        print(f'warning: close without open {m=}')\n                        continue\n                    occurrences.append((u, occ_start, k))\n                    # print(f'{(u, occ_start, k)=}')\n                    occ_start = None\n\n        if len(occurrences)==0:\n            print(f'warning: no occurrences of {matches} in `set_sampler_loop`')\n            return\n\n        # index into list using n\n        u, loop_start, loop_end = occurrences[n]\n\n        loop_start = self.utterance_to_global_step(u, loop_start)\n        loop_end = self.utterance_to_global_step(u, loop_end)\n        # TODO: efficient search for index of occurence\n\n        # set loop points\n        # if utterance changes, unset points which aren't set\n        # changed = u != self.sampler_utterance\n        # self.sampler_utterance = u\n\n        if start:\n            self.sampler_loop_start = loop_start\n        # elif changed:\n            # self.sampler_loop_start = 0\n\n        if end:\n            self.sampler_loop_end = loop_end\n        # elif changed:\n            # self.sampler_loop_end = None\n\n        # must reset if changing utterance\n        # if changed or reset:\n        if reset:\n            self.reset_sampler()\n\n        # print(f'{self.sampler_utterance=}, {self.sampler_step=}, {self.sampler_loop_start=}, {self.sampler_loop_end=}')\n\n\n    def reset_sampler(self):\n        self.step_mode = StepMode.SAMPLER\n        self.sampler_step = self.sampler_loop_start\n\n\n    def process_frame(self, \n            latent_t:Optional[Tensor]=None, \n            align_t:Optional[Tensor]=None\n        ) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Generate an audio(rave latents) and alignment frame.\n\n        Args:\n            latent_t: [batch, RAVE latent size]\n                last frame of audio feature if using `latent feedback` mode\n            align_t: [batch, text length in tokens]\n                explicit alignments if using `paint` mode\n        Returns:\n            latent_t: [batch, RAVE latent size]\n                next frame of audio feature\n            align_t: [batch, text length in tokens]\n                alignments to text\n        \"\"\"\n        with torch.inference_mode():\n            with self.profile('tts', detail=False):\n                r = self.model.step(\n                    alignment=align_t, audio_frame=latent_t, \n                    temperature=self.temperature)\n                latent_t, align_t, \n        # use low precision for storage\n        return r['output'].half(), r['alignment'].half()\n\n    def set_temperature(self, t):\n        self.temperature = t\n\n    def do_audio_block(self, outdata, sdtime):\n        \"\"\"loop over samples of output, requesting frames from the audio thread\n        and pulling them from the queue as needed\n        \"\"\"\n        outdata[:,:] = 0\n        c = self.synth_channels+self.latent_channels\n        for i in range(outdata.shape[0]):\n            # if the current frame is exhausted, delete it\n            if self.active_frame is not None:\n                if self.frame_counter &gt;= self.active_frame.shape[-1]:\n                    self.active_frame = None\n                    self.frame_counter = 0\n\n            # if no active frame, try to get one from step thread\n            if self.active_frame is None:\n                if not self.audio_q.empty(): \n                    self.active_frame = self.audio_q.get()\n\n            if not self.trigger_q.full():\n                # use ADC input time as timestamp\n                timestamp = sdtime.inputBufferAdcTime\n                self.trigger_q.put(timestamp)\n\n            if self.active_frame is None:\n                if self.playing:\n                    print(f'audio: dropped frame')\n                return\n            else:\n                # read next audio sample out of active model frame \n                # TODO: batch/multichannel handling\n                outdata[i,c] = self.active_frame[:, self.frame_counter]\n                self.frame_counter += 1\n\n    def audio_callback(self,*a):\n        \"\"\"sounddevice callback main loop\"\"\"\n        if len(a)==4: # output device only case\n            (\n                outdata,#: np.ndarray, #[frames x channels]\n                frames, sdtime, status\n            ) = a\n        elif len(a)==5: # input and output device\n            (\n                indata, outdata, #np.ndarray, #[frames x channels]\n                frames, sdtime, status\n            ) = a\n\n        self.do_audio_block(outdata, sdtime)\n\n    def utterance_empty(self, utterance=-1):\n        try:\n            return len(self.states[utterance])==0\n        except IndexError:\n            return True\n\n    def utterance_len(self, utterance=-1):\n        return len(self.states[utterance])\n\n    def global_step_to_utterance(self, global_step):\n        utterance = 0\n        step = global_step\n        while True:\n            if utterance &gt;= len(self.states):\n                raise ValueError('warning: global_step out of bounds')\n            ul = len(self.states[utterance])\n            if step &lt; ul:\n                break\n            step -= ul\n            utterance += 1\n\n        return utterance, step\n\n    def utterance_to_global_step(self, utterance, step):\n        if utterance &gt;= len(self.states):\n            raise ValueError('warning: utterance out of bounds')\n        return step + sum(len(self.states[i]) for i in range(utterance-1))\n\n    def prev_state(self, *a, step=-1, utterance=-1, global_step=None):\n        \"\"\"convenient access to previous states\"\"\"\n        if global_step is not None:\n            try:\n                utterance, step = self.global_step_to_utterance(global_step)\n            except ValueError:\n                return None\n\n        s = self.states[utterance][step]\n        for k in a:\n            s = s[k]\n        return s\n\n    def total_steps(self):\n        return sum(len(s) for s in self.states)\n\n    def hard_alignment(self, align_t):\n        \"\"\"return index and character of hard alignment\"\"\"\n        i = align_t.argmax().item()\n        # print(i)\n        c = self.text[i] if i &lt; len(self.text) else None\n\n        i_enter = None\n        i_leave = None\n        c_enter = None\n        c_leave = None\n        if not self.utterance_empty():\n            # print(self.states)\n            prev_align = self.prev_state('align_hard')\n\n            if i&gt;prev_align['index']:\n                i_enter = i\n                i_leave = prev_align['index']\n                c_enter = c\n                c_leave = prev_align['char']\n\n        return {\n            'index':i, \n            'char':c, \n            'enter_index':i_enter,\n            'leave_index':i_leave,\n            'enter_char':c_enter,\n            'leave_char':c_leave\n        }\n\n    def do_reset(self):\n        \"\"\"perform reset of model states/text (called from `step`)\"\"\"\n        print('RESET')\n        for k,v in self.reset_values.items():\n            setattr(self, k, v)\n\n        if self.text_rep is not None:\n            self.model.reset(self.text_rep)\n            if self.align_params is None:\n                # go to loop start after reset\n                self.momentary_align_params = (self.gen_loop_start, 1)\n            elif not self.utterance_empty():\n                # unless painting alignments -- then try to stay\n                # in approximately the same spot\n                # TODO this doesn't work because the frontend just sets it again based on the slider -- need to add bidirectional control\n                self.align_params = (\n                    self.text_index_map(round(self.align_params[0])), 1)\n\n            # remove old RNN states\n            if len(self.states):\n                for state in self.states[-1]:\n                    self.strip_states(state)\n\n            # start a new utterance\n            self.states.append(Utterance(self.text))\n            ## for now, only sampler of current utterance is supported\n            self.sampler_step = 0\n            # in the future, it might be useful to encode texts without yet starting a new utterance. for now, it makes more sense if hitting encode \n            # always starts generation\n            self.generate()\n\n            self.needs_reset = False\n\n\n        return {\n            'reset': True, \n            'text': self.text, \n            'num_latents': self.num_latents, \n            'use_pitch': self.use_pitch\n            }\n\n    def paint_alignment(self):\n        \"\"\"helper for `step_gen`\"\"\"\n        align_params = self.momentary_align_params or self.align_params\n        self.momentary_align_params = None\n        if align_params is None:\n            return None\n        loc, scale = align_params\n        n_tokens = self.text_rep.shape[1]\n        loc = max(0, min(n_tokens, loc))\n        deltas = torch.arange(n_tokens) - loc\n        deltas = deltas / (0.5 + scale) # sharpness modulated by speed \n        # discrete gaussian, sums exactly to 1 over text positions\n        logits = -deltas**2\n        res = logits.exp() \n        res = res / res.sum()\n        return res[None]\n\n\n    def convert_numpy(self, item):\n        if hasattr(item, 'numpy'):\n            return item.numpy()\n        # elif isinstance(item, dict):\n            # return {k:self.convert_numpy(v) for k,v in item.items()}\n        # elif hasattr(item, '__len__'):\n            # return [self.convert_numpy(i) for i in item]\n        else:\n            return item\n\n    def send_state(self, state):   \n        state = {\n            k:self.convert_numpy(v)\n            for k,v in state.items()\n            if k!='model_state'\n        }\n        self.frontend_conn.send(state)\n\n    def step(self, timestamp):\n        \"\"\"compute one vocoder frame of generation or sampler\"\"\"\n        # if self.frontend_q.qsize() &gt; 100:\n            # self.frontend_q.get()\n            # print('frontend queue full, dropping')\n\n        state = {'text':self.text}\n\n        # reset text and model states\n        if self.needs_reset:\n            state |= self.do_reset()\n\n        # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n        if self.text_rep is None or self.step_mode==StepMode.PAUSE:\n            if len(state):\n                # self.frontend_q.put(state)\n                if self.frontend_conn is not None:\n                    self.send_state(state)\n                else:\n                    print('warning: frontend_conn does not exist')\n            return None\n        if self.step_mode==StepMode.GENERATION:\n            # with self.profile('step_gen'):\n            state |= self.step_gen(timestamp)\n        elif self.step_mode==StepMode.SAMPLER:\n            with self.profile('step_sampler'):\n                state |= self.step_sampler(timestamp)\n        # else:\n            # print(f'WARNING: {self.step_mode=} in step')\n\n        if self.osc_sender is not None:\n            self.osc_sender(state)\n\n        if len(self.latent_biases):\n            with torch.inference_mode():\n                bias = torch.tensor(self.latent_biases)[None]\n                state['latent_t'] += bias\n\n        # send to frontend\n        # self.frontend_q.put(state)\n        if self.frontend_conn is not None:\n            self.send_state(state)\n        else:\n            # print(state)\n            print('warning: frontend_conn does not exist')\n\n        latent = state['latent_t'] # batch, channel\n        batch, n_latent = latent.shape\n        assert batch==1\n\n        c_synth, c_latent = len(self.synth_channels), len(self.latent_channels)\n\n        with torch.inference_mode():\n            audio_frame = torch.zeros(\n                c_synth+c_latent, \n                self.model.block_size) # channel, time\n\n            if OutMode.SYNTH_AUDIO in self.out_mode:\n                # allow creative use of vocoders with mismatched sizes\n                rave_dim = self.rave.decode_params[0]\n                common_dim = min(rave_dim, latent.shape[-1])\n                latent_to_rave = torch.zeros(latent.shape[0], rave_dim)\n                latent_to_rave[:,:common_dim] = latent[:,:common_dim]\n                ### run the vocoder\n                with self.profile('vocoder'):\n                    audio = self.rave.decode(\n                        latent_to_rave[...,None])[0] # channel, time\n                ###\n                audio_frame[:c_synth, :] = audio\n\n            if OutMode.LATENT_AUDIO in self.out_mode:\n                latent = torch.cat((\n                    torch.zeros(batch, 1), # zero to ensure trigger\n                    torch.full((batch, 1), n_latent), # trigger, number of latents\n                    latent\n                    ), dim=1) / 1024 # scale down in case audio gets sent to speakers\n                # latent x batch\n\n                audio_frame[c_synth:, :2+n_latent] = latent\n\n        return audio_frame\n        # send to audio thread\n        # print(f'DEBUG: queuing audio {id(audio)}')\n        # self.audio_q.put(audio_frame)\n\n    def step_sampler(self, timestamp):\n        \"\"\"sampler branch of main `step` method\"\"\"\n        # print(f'{self.sampler_step=}')\n\n        # if self.utterance_empty(utterance=self.sampler_utterance):\n        if self.total_steps()==0:\n            print('nothing to play back')\n            self.step_mode = StepMode.PAUSE\n            return None\n\n        state = self.prev_state(global_step=self.sampler_step) or {}\n\n        self.sampler_step += 1\n\n        if (\n            self.sampler_step == self.sampler_loop_end \n            or self.sampler_step &gt;= self.total_steps()\n            ):\n            self.sampler_step = self.sampler_loop_start\n            if self.sampler_stop_at_end:\n                self.step_mode = StepMode.PAUSE\n\n\n        state = {**state}\n        state['latent_t'] = state['latent_t'].clone()\n        state['timestamp'] = timestamp\n        state['sampler'] = True\n\n        return state\n\n\n    def step_gen(self, timestamp):\n        \"\"\"generation branch of main `step` method\"\"\"\n        # loop end\n        if (\n            self.align_params is None \n            and self.gen_loop_end is not None \n            and not self.utterance_empty()\n            and self.prev_state('align_hard', 'index') in (\n                self.gen_loop_end, self.gen_loop_end+1)\n        ):\n            self.momentary_align_params = (self.gen_loop_start, 1)\n\n        # set just model states\n        # TODO: possibility to set to previous utterance?\n        if self.step_to_set is not None:\n            step = self.step_to_set\n            with torch.inference_mode():\n                try:\n                    self.model.set_state(\n                        self.prev_state('model_state', step=step))\n                except Exception:\n                    print(f'WARNING: failed to set {step=} ')\n                    raise\n            self.step_to_set = None\n\n        if self.model.memory is None:\n            print('skipping: model not initialized')\n            if self.rave is not None:\n                self.audio_q.put(None)\n            return None\n\n        if not self.utterance_empty() and self.latent_feedback:\n            # pass the last vocoder frame back in\n            latent_t = self.prev_state('latent_t')\n        else:\n            latent_t = None\n\n        align_t = self.paint_alignment()\n\n        ##### run the TTS model\n        latent_t, align_t = self.process_frame(\n            # self.mode, \n            latent_t=latent_t, align_t=align_t)\n        #####\n\n        # NOTE\n        ### EXPERIMENTAL juice knob\n        # latent_t = latent_t * 1.1\n        ###\n\n        state = {\n            # 'text':self.text,\n            'latent_t':latent_t, \n            'align_t':align_t,\n            'timestamp':timestamp,\n            'model_state':self.model.get_state(),\n            'align_hard':self.hard_alignment(align_t)\n            }\n\n        if (\n            self.generate_stop_at_end \n            and state['align_hard']['index']==(len(self.text)-1)\n        ):\n            self.step_mode = StepMode.PAUSE\n            # self.reset()\n\n        utt = self.states[-1]\n        utt.append(state)\n        # remove RNN states after certain number of steps\n        if len(utt) &gt;= self.max_model_state_storage:\n            self.strip_states(utt[-self.max_model_state_storage])\n        return state\n\n    def strip_states(self, state):\n        \"\"\"remove RNN states\"\"\"\n        state.pop('model_state', None)\n\n    @property\n    def playing(self):\n        return self.step_mode!=StepMode.PAUSE\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.__init__","title":"<code>__init__(checkpoint, rave_path=None, audio_in=None, audio_out=None, audio_block=None, audio_channels=None, synth_audio=None, latent_audio=None, latent_osc=None, osc_sender=None, buffer_frames=1, profile=True, jit=False, max_model_state_storage=512)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>str</code> <p>path to tungnaa checkpoint file</p> required <code>rave_path</code> <code>str | None</code> <p>path to rave vocoder to use python sound</p> <code>None</code> <code>audio_in</code> <code>str | None</code> <p>sounddevice input name/index if using python audio</p> <code>None</code> <code>audio_out</code> <code>str | int</code> <p>sounddevice output name/index if using python audio</p> <code>None</code> <code>audio_block</code> <code>int | None</code> <p>block size if using python audio</p> <code>None</code> <code>synth_audio</code> <code>bool | None</code> <p>if True, vocode in python and send stereo audio</p> <code>None</code> <code>latent_audio</code> <code>bool | None</code> <p>if True, pack latents into a mono audio channel</p> <code>None</code> <code>latent_osc</code> <code>bool | None</code> <p>if True, send latents over OSC</p> <code>None</code> <code>buffer_frames</code> <code>int</code> <p>process ahead this many model steps</p> <code>1</code> <code>profile</code> <code>bool</code> <p>if True, print performance profiling info</p> <code>True</code> <code>jit</code> <code>bool</code> <p>if True, compile tungnaa model with torchscript</p> <code>False</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def __init__(self,\n    checkpoint:str,\n    rave_path:str|None=None,\n    audio_in:str|None=None,\n    audio_out:str|int=None,\n    audio_block:int|None=None,\n    audio_channels:int|None=None,\n    # sample_rate:int=None,\n    synth_audio:bool|None=None,\n    latent_audio:bool|None=None,\n    latent_osc:bool|None=None,\n    osc_sender=None,\n    buffer_frames:int=1,\n    profile:bool=True,\n    jit:bool=False,\n    max_model_state_storage:int=512,\n    ):\n    \"\"\"\n    Args:\n        checkpoint: path to tungnaa checkpoint file\n        rave_path: path to rave vocoder to use python sound\n        audio_in: sounddevice input name/index if using python audio\n        audio_out: sounddevice output name/index if using python audio\n        audio_block: block size if using python audio\n        synth_audio: if True, vocode in python and send stereo audio\n        latent_audio: if True, pack latents into a mono audio channel\n        latent_osc: if True, send latents over OSC\n        buffer_frames: process ahead this many model steps\n        profile: if True, print performance profiling info\n        jit: if True, compile tungnaa model with torchscript\n    \"\"\"\n    self.jit = jit\n    self.profile = Profiler(profile)\n    # default output modes\n    if synth_audio is None:\n        synth_audio = rave_path is not None\n    if latent_audio is None:\n        latent_audio = rave_path is None\n    if latent_osc is None:\n        latent_osc = False\n\n    self.reset_values = {}\n\n    out_mode = OutMode(0)\n    if synth_audio: out_mode |= OutMode.SYNTH_AUDIO\n    if latent_audio: out_mode |= OutMode.LATENT_AUDIO\n    if latent_osc: out_mode |= OutMode.LATENT_OSC\n    self.out_mode = out_mode\n    print(f'{self.out_mode=}')\n\n    if audio_channels is None:\n        audio_channels = 0\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            self.synth_channels = (0,1)\n            audio_channels += 2\n        else:\n            self.synth_channels = tuple()\n        if OutMode.LATENT_AUDIO in self.out_mode:\n            self.latent_channels = (audio_channels,)\n            audio_channels += 1\n        else:\n            self.latent_channels = tuple()\n    else:\n        raise NotImplementedError(\n            \"setting audio_channels not currently supported\")\n    print(f'{audio_channels=}')\n\n    self.step_mode = StepMode.PAUSE\n    self.generate_stop_at_end = False\n    self.sampler_stop_at_end = False\n\n    self.latent_biases = []\n\n    # self.temperature = 0.5\n    self.temperature = 1.\n\n    # generation\n    self.gen_loop_start = 0\n    self.gen_loop_end = None\n    # sampler\n    self.sampler_loop_start = 0\n    self.sampler_loop_end = None\n    # self.sampler_utterance = -1\n    self.sampler_step = 0\n\n    self.osc_sender = osc_sender\n\n    self.frontend_conn = None\n\n    self.max_model_state_storage = max_model_state_storage\n\n    ### move heavy init out of __init__, so it only runs in child process\n    ### (Backend.run is called by Proxy._run)\n    self.init_args = (buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.audio_callback","title":"<code>audio_callback(*a)</code>","text":"<p>sounddevice callback main loop</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def audio_callback(self,*a):\n    \"\"\"sounddevice callback main loop\"\"\"\n    if len(a)==4: # output device only case\n        (\n            outdata,#: np.ndarray, #[frames x channels]\n            frames, sdtime, status\n        ) = a\n    elif len(a)==5: # input and output device\n        (\n            indata, outdata, #np.ndarray, #[frames x channels]\n            frames, sdtime, status\n        ) = a\n\n    self.do_audio_block(outdata, sdtime)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup any resources</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup any resources\n    \"\"\"\n    self.step_mode = StepMode.PAUSE\n    self.run_thread = False\n    # should probably do this more gracefully\n    exit(0) # exit the backend process\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.do_audio_block","title":"<code>do_audio_block(outdata, sdtime)</code>","text":"<p>loop over samples of output, requesting frames from the audio thread and pulling them from the queue as needed</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def do_audio_block(self, outdata, sdtime):\n    \"\"\"loop over samples of output, requesting frames from the audio thread\n    and pulling them from the queue as needed\n    \"\"\"\n    outdata[:,:] = 0\n    c = self.synth_channels+self.latent_channels\n    for i in range(outdata.shape[0]):\n        # if the current frame is exhausted, delete it\n        if self.active_frame is not None:\n            if self.frame_counter &gt;= self.active_frame.shape[-1]:\n                self.active_frame = None\n                self.frame_counter = 0\n\n        # if no active frame, try to get one from step thread\n        if self.active_frame is None:\n            if not self.audio_q.empty(): \n                self.active_frame = self.audio_q.get()\n\n        if not self.trigger_q.full():\n            # use ADC input time as timestamp\n            timestamp = sdtime.inputBufferAdcTime\n            self.trigger_q.put(timestamp)\n\n        if self.active_frame is None:\n            if self.playing:\n                print(f'audio: dropped frame')\n            return\n        else:\n            # read next audio sample out of active model frame \n            # TODO: batch/multichannel handling\n            outdata[i,c] = self.active_frame[:, self.frame_counter]\n            self.frame_counter += 1\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.do_reset","title":"<code>do_reset()</code>","text":"<p>perform reset of model states/text (called from <code>step</code>)</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def do_reset(self):\n    \"\"\"perform reset of model states/text (called from `step`)\"\"\"\n    print('RESET')\n    for k,v in self.reset_values.items():\n        setattr(self, k, v)\n\n    if self.text_rep is not None:\n        self.model.reset(self.text_rep)\n        if self.align_params is None:\n            # go to loop start after reset\n            self.momentary_align_params = (self.gen_loop_start, 1)\n        elif not self.utterance_empty():\n            # unless painting alignments -- then try to stay\n            # in approximately the same spot\n            # TODO this doesn't work because the frontend just sets it again based on the slider -- need to add bidirectional control\n            self.align_params = (\n                self.text_index_map(round(self.align_params[0])), 1)\n\n        # remove old RNN states\n        if len(self.states):\n            for state in self.states[-1]:\n                self.strip_states(state)\n\n        # start a new utterance\n        self.states.append(Utterance(self.text))\n        ## for now, only sampler of current utterance is supported\n        self.sampler_step = 0\n        # in the future, it might be useful to encode texts without yet starting a new utterance. for now, it makes more sense if hitting encode \n        # always starts generation\n        self.generate()\n\n        self.needs_reset = False\n\n\n    return {\n        'reset': True, \n        'text': self.text, \n        'num_latents': self.num_latents, \n        'use_pitch': self.use_pitch\n        }\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.extract_loop_points","title":"<code>extract_loop_points(text, tokens='&lt;&gt;')</code>","text":"<p>helper for <code>set_text</code></p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def extract_loop_points(self, text, tokens='&lt;&gt;'):\n    \"\"\"helper for `set_text`\"\"\"\n    start_tok, end_tok = tokens\n    # TODO: could look for matched brackets, have multiple loops...\n    start = text.find(start_tok) # -1 if not found\n    # if not len(text):\n    #     return text, None, None\n    if start &lt; 0:\n        start = 0\n    else:\n        text = text[:start]+text[start+1:]\n    end = text.find(end_tok) # -1 if not found\n    if end &lt; 0: \n        end = None\n    else:\n        text = text[:end]+text[end+1:]\n        end = max(0, end - 1)\n    # print(text, end)\n    return text, start, end\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.generate","title":"<code>generate()</code>","text":"<p>start autoregressive alignment &amp; latent frame generation</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def generate(self):\n    \"\"\"\n    start autoregressive alignment &amp; latent frame generation\n    \"\"\"\n    # if self.step_mode != StepMode.GENERATION:\n        # self.needs_reset = True\n    self.step_mode = StepMode.GENERATION\n    self.start_stream()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.hard_alignment","title":"<code>hard_alignment(align_t)</code>","text":"<p>return index and character of hard alignment</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def hard_alignment(self, align_t):\n    \"\"\"return index and character of hard alignment\"\"\"\n    i = align_t.argmax().item()\n    # print(i)\n    c = self.text[i] if i &lt; len(self.text) else None\n\n    i_enter = None\n    i_leave = None\n    c_enter = None\n    c_leave = None\n    if not self.utterance_empty():\n        # print(self.states)\n        prev_align = self.prev_state('align_hard')\n\n        if i&gt;prev_align['index']:\n            i_enter = i\n            i_leave = prev_align['index']\n            c_enter = c\n            c_leave = prev_align['char']\n\n    return {\n        'index':i, \n        'char':c, \n        'enter_index':i_enter,\n        'leave_index':i_leave,\n        'enter_char':c_enter,\n        'leave_char':c_leave\n    }\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.load_tts_model","title":"<code>load_tts_model(checkpoint)</code>","text":"<p>helper for loading TTS model, called on initialization but can also be called from GUI</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def load_tts_model(self, checkpoint):\n    \"\"\"helper for loading TTS model, called on initialization but can also be called from GUI\"\"\"\n    self.model = TacotronDecoder.from_checkpoint(checkpoint)\n\n    # not scripting text encoder for now\n    # if self.model.text_encoder is None:\n        # self.text_model = TextEncoder()\n    # else:\n    self.text_model = self.model.text_encoder\n    self.model.text_encoder = None\n\n    # def _debug(m):\n    #     # print({(k,type(v)) for k,v in m.__dict__.items()})\n    #     for k,v in m.__dict__.items():\n    #         # print('ATTR', k)\n    #         if 'tensor' in str(type(v)).lower(): \n    #             print('TENSOR', k)\n    #     for m_ in m.modules():\n    #         if m_ != m:\n    #             # print('MODULE', m_)\n    #             _debug(m_)\n    # _debug(self.model)\n\n    if self.jit:\n        for m in self.model.modules():\n            if hasattr(m, 'parametrizations'):\n                torch.nn.utils.parametrize.remove_parametrizations(\n                    m,'weight')\n        self.model = torch.jit.script(self.model)\n    self.model.eval()\n    # self.model.train()\n\n    print(f'{self.num_latents=}')\n\n    # print(f'{self.model.frame_channels=}')\n    self.use_pitch = hasattr(self.model, 'pitch_xform') and self.model.pitch_xform\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.load_vocoder_model","title":"<code>load_vocoder_model(rave_path)</code>","text":"<p>helper for loading RAVE vocoder model, called on initialization but can also be called from GUI</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def load_vocoder_model(self, rave_path):\n    \"\"\"helper for loading RAVE vocoder model, called on initialization but can also be called from GUI\"\"\"\n    # TODO: The audio engine sampling rate gets set depending on the sampling rate from the vocoder. \n    #   When loading a new vocoder model we need to add some logic to make sure the new vocoder has the same sample rate as the audio system.\n    if OutMode.SYNTH_AUDIO in self.out_mode:\n        assert rave_path is not None\n        self.rave = torch.jit.load(rave_path, map_location='cpu')\n        self.rave.eval()\n        self.block_size = int(self.rave.decode_params[1])\n        try:\n            self.rave_sr = int(self.rave.sampling_rate)\n        except Exception:\n            self.rave_sr = int(self.rave.sr)\n        with torch.inference_mode():\n            # warmup\n            if hasattr(self.rave, 'full_latent_size'):\n                latent_size = self.rave.latent_size + int(\n                    hasattr(self.rave, 'pitch_encoder'))\n            else:\n                latent_size = self.rave.cropped_latent_size\n            self.rave.decode(torch.zeros(1,latent_size,1))\n    else:\n        self.rave = None\n        self.rave_sr = None\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.paint_alignment","title":"<code>paint_alignment()</code>","text":"<p>helper for <code>step_gen</code></p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def paint_alignment(self):\n    \"\"\"helper for `step_gen`\"\"\"\n    align_params = self.momentary_align_params or self.align_params\n    self.momentary_align_params = None\n    if align_params is None:\n        return None\n    loc, scale = align_params\n    n_tokens = self.text_rep.shape[1]\n    loc = max(0, min(n_tokens, loc))\n    deltas = torch.arange(n_tokens) - loc\n    deltas = deltas / (0.5 + scale) # sharpness modulated by speed \n    # discrete gaussian, sums exactly to 1 over text positions\n    logits = -deltas**2\n    res = logits.exp() \n    res = res / res.sum()\n    return res[None]\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.pause","title":"<code>pause()</code>","text":"<p>pause generation or sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def pause(self):\n    \"\"\"\n    pause generation or sampler\n    \"\"\"\n    self.step_mode = StepMode.PAUSE\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.prev_state","title":"<code>prev_state(*a, step=-1, utterance=-1, global_step=None)</code>","text":"<p>convenient access to previous states</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def prev_state(self, *a, step=-1, utterance=-1, global_step=None):\n    \"\"\"convenient access to previous states\"\"\"\n    if global_step is not None:\n        try:\n            utterance, step = self.global_step_to_utterance(global_step)\n        except ValueError:\n            return None\n\n    s = self.states[utterance][step]\n    for k in a:\n        s = s[k]\n    return s\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.process_frame","title":"<code>process_frame(latent_t=None, align_t=None)</code>","text":"<p>Generate an audio(rave latents) and alignment frame.</p> <p>Parameters:</p> Name Type Description Default <code>latent_t</code> <code>Optional[Tensor]</code> <p>[batch, RAVE latent size] last frame of audio feature if using <code>latent feedback</code> mode</p> <code>None</code> <code>align_t</code> <code>Optional[Tensor]</code> <p>[batch, text length in tokens] explicit alignments if using <code>paint</code> mode</p> <code>None</code> <p>Returns:     latent_t: [batch, RAVE latent size]         next frame of audio feature     align_t: [batch, text length in tokens]         alignments to text</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def process_frame(self, \n        latent_t:Optional[Tensor]=None, \n        align_t:Optional[Tensor]=None\n    ) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Generate an audio(rave latents) and alignment frame.\n\n    Args:\n        latent_t: [batch, RAVE latent size]\n            last frame of audio feature if using `latent feedback` mode\n        align_t: [batch, text length in tokens]\n            explicit alignments if using `paint` mode\n    Returns:\n        latent_t: [batch, RAVE latent size]\n            next frame of audio feature\n        align_t: [batch, text length in tokens]\n            alignments to text\n    \"\"\"\n    with torch.inference_mode():\n        with self.profile('tts', detail=False):\n            r = self.model.step(\n                alignment=align_t, audio_frame=latent_t, \n                temperature=self.temperature)\n            latent_t, align_t, \n    # use low precision for storage\n    return r['output'].half(), r['alignment'].half()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.reset","title":"<code>reset()</code>","text":"<p>reset the model state and alignments history</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def reset(self):\n    \"\"\"\n    reset the model state and alignments history\n    \"\"\"\n    self.needs_reset = True\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.run","title":"<code>run(conn)</code>","text":"<p>run method expected by Proxy</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def run(self, conn):\n    \"\"\"run method expected by Proxy\"\"\"\n    # call this from both __init__ and run\n    # torch.multiprocessing.set_sharing_strategy('file_system')\n\n    self.frontend_conn = conn\n    # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n    buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels = self.init_args\n\n    self.load_tts_model(checkpoint=checkpoint)\n\n    ### synthesis in python:\n    # load RAVE model\n    self.load_vocoder_model(rave_path=rave_path)\n\n    ### audio output:\n    # make sounddevice stream\n    if self.out_mode:\n        devicelist = sd.query_devices()\n\n        # None throws an error on linux, on mac it uses the default device\n        # Let's make this behavior explicit.\n        if audio_out is None:\n            audio_out = sd.default.device[1]\n\n        audio_device = None\n        for dev in devicelist:\n            if audio_out in [dev['index'], dev['name']]:\n                audio_device = dev\n                # audio_device_sr = dev['default_samplerate']\n            print(f\"{dev['index']}: '{dev['name']}' {dev['hostapi']} (I/O {dev['max_input_channels']}/{dev['max_input_channels']}) (SR: {dev['default_samplerate']})\")\n\n        # this should not be an error\n        # None uses the default device on macOS\n        # if audio_device is None:\n            # raise RuntimeError(f\"Audio device '{audio_out}' does not exist.\")\n\n        print(f\"USING AUDIO OUTPUT DEVICE {audio_out}:{audio_device}\")\n\n        self.active_frame:torch.Tensor = None \n        self.future_frame:Thread = None\n        self.frame_counter:int = 0\n\n        sd.default.device = audio_out\n        if self.rave_sr:\n            if audio_device and audio_device['default_samplerate'] != self.rave_sr:\n                # this should not be an error. On OSX/CoreAudio you can set the device sample rate to the model sample rate.\n                # however on Linux/JACK this throws a fatal error and stops program execution, requiring you to restart Jack to change the sampling rate\n\n                # TODO: also check RAVE block size vs. audio device block size if possible\n                print(\"\\n------------------------------------\");\n                print(f\"WARNING: Device default sample rate ({audio_device['default_samplerate']}) and RAVE model sample rate ({self.rave_sr}) mismatch! You may need to change device sample rate manually on some platforms.\")\n                print(\"------------------------------------\\n\");\n\n            sd.default.samplerate = self.rave_sr # could cause an error if device uses a different sr from model\n            print(f\"RAVE SAMPLING RATE: {self.rave_sr}\")\n            print(f\"DEVICE SAMPLING RATE: {audio_device['default_samplerate']}\")\n\n        # TODO: Tungnaa only uses audio output. Shouldn't we always be using sd.OutputStream?\n        try:\n            assert len(audio_out)==2\n            stream_cls = sd.Stream\n        except Exception:\n            stream_cls = sd.OutputStream\n\n        self.stream = stream_cls(\n            callback=self.audio_callback,\n            samplerate=self.rave_sr, \n            blocksize=audio_block, \n            #device=(audio_in, audio_out)\n            device=audio_out,\n            channels=audio_channels\n        )\n\n        if self.rave_sr:\n            assert self.stream.samplerate == self.rave_sr, f\"\"\"\n            failed to set sample rate to {self.rave_sr} from sounddevice\n            \"\"\"\n    else:\n        self.stream = None\n\n    self.text = None\n    self.text_rep = None\n    self.align_params = None\n    self.momentary_align_params = None\n    self.step_to_set = None\n\n    self.one_shot_paint = False\n    self.latent_feedback = False\n\n    self.lock = RLock()\n    self.text_update_thread = None\n\n    # self.frontend_q = Queue()\n    self.audio_q = Queue(buffer_frames)     \n    self.trigger_q = Queue(buffer_frames)     \n\n    self.states = []\n\n    self.needs_reset = False\n\n    self.step_thread = Thread(target=self.step_loop, daemon=True)\n    self.step_thread.start()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.sampler","title":"<code>sampler()</code>","text":"<p>start sampler mode</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def sampler(self):\n    \"\"\"\n    start sampler mode\n    \"\"\"\n    self.step_mode = StepMode.SAMPLER\n    self.start_stream()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.set_alignment","title":"<code>set_alignment(align_params)</code>","text":"<p>Send alignment parameters to the backend. If None is passed, alignment painting is off.</p> <p>Parameters:</p> Name Type Description Default <code>align_params</code> <code>Optional[Tuple[float, float]]</code> <p>[loc, scale] or None</p> required Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_alignment(self, \n        align_params:Optional[Tuple[float, float]]\n    ) -&gt; None:\n    \"\"\"\n    Send alignment parameters to the backend. If None is passed, alignment painting is off.\n\n    Args:\n        align_params: [loc, scale] or None\n    \"\"\"\n    self.align_params = align_params\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.set_momentary_alignment","title":"<code>set_momentary_alignment(align_params)</code>","text":"<p>Alignment will be set on the next frame only</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_momentary_alignment(self,             \n        align_params:Optional[Tuple[float, float]]\n    ) -&gt; None:\n    \"\"\"\n    Alignment will be set on the next frame only\n    \"\"\"\n    self.momentary_align_params = align_params\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.set_sampler_loop_index","title":"<code>set_sampler_loop_index(start=None, end=None, utterance=None, reset=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>loop start step</p> <code>None</code> <code>end</code> <code>int</code> <p>loop end step</p> <code>None</code> <code>utterance</code> <code>int</code> <p>sampler utterance</p> <code>None</code> <code>reset</code> <code>bool</code> <p>if True, immediately go to loop start</p> <code>True</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_sampler_loop_index(self, \n        start:int=None, end:int=None, \n        utterance:int=None, \n        reset:bool=True):\n    \"\"\"\n    Args:\n        start: loop start step\n        end: loop end step\n        utterance: sampler utterance\n        reset: if True, immediately go to loop start\n    \"\"\"\n    if len(self.states)==0: return\n\n    def wrap(x, n, tag):\n        if x is None: return x\n        if x &lt; 0: \n            x += n\n        if x &lt; 0 or x &gt;= n:\n            print(f'warning: out of bounds {tag} {x}')\n            x %= n\n        return x\n\n    if utterance is not None:\n        if start is not None:\n            start = self.utterance_to_global_step(utterance, start)\n        if end is not None:\n            end = self.utterance_to_global_step(utterance, end)\n\n    if start is None: start = self.sampler_loop_start        \n    if end is None: end = self.sampler_loop_end\n\n    # utterance = wrap(utterance, len(self.states), 'utterance')\n    start = wrap(start, len(self.states[utterance]), 'loop start')\n    end = wrap(end, len(self.states[utterance]), 'loop end')\n\n    # changed = utterance != self.sampler_utterance\n    # self.sampler_utterance = utterance\n\n    self.sampler_loop_start = start\n    self.sampler_loop_end = end\n\n    # must reset if changing utterance\n    if reset:\n        self.reset_sampler()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.set_sampler_loop_text","title":"<code>set_sampler_loop_text(text, n=-1, start=True, end=True, reset=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>a regular expression string</p> required <code>n</code> <code>int</code> <p>index of occurrence in history</p> <code>-1</code> <code>start</code> <code>bool</code> <p>if True, set the loop start</p> <code>True</code> <code>end</code> <code>bool</code> <p>if True, set the loop end</p> <code>True</code> <code>reset</code> <code>bool</code> <p>if True, immediately go to loop start</p> <code>True</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_sampler_loop_text(self, \n        text:str, n:int=-1, \n        start:bool=True, end:bool=True, \n        reset:bool=True):\n    \"\"\"\n    Args:\n        text: a regular expression string\n        n: index of occurrence in history\n        start: if True, set the loop start\n        end: if True, set the loop end\n        reset: if True, immediately go to loop start\n    \"\"\"\n    if not (start or end): return\n    r = text\n\n    # get all occurences of all matching strings,\n    #   and index by order in history\n\n    # first find matches for the regex in utterance texts\n    matches = []\n    utts = self.states #if n&gt;=0 else reversed(self.states)\n    index = 0\n    for utt_index, utt in enumerate(utts):\n        if len(utt)==0: continue\n        # text = utt[0]['text']\n        text = utt.text\n        for match in re.findall(r, text):\n            # error if multiple capture groups in regex\n            if not isinstance(match, str):\n                raise ValueError(\"`set_sampler_loop`: multiple capture groups not supported\")\n            # add utterance, text index of match\n            index = index + text[index:].find(match)\n            matches.append((utt_index, index, index+len(match), match))\n            # print(f'{utt_index=}, {index=}, {match=}')\n\n    if len(matches)==0:\n        print(f'warning: no text matching \"{r}\" in `set_sampler_loop`')\n        return\n\n    # for matches, get occurences and flatten into list\n    #   simple: occurrence is from entering start to exiting end\n    #   TODO better: occurence is when there is a run which\n    #       enters the first token / leaves the last token\n    #       without any major jumps or reversals\n    #   TODO?: allow skips of first / last token?\n    occurrences = []\n    for u, i, j, m in matches:\n        occ_start = None\n        utt = self.states[u]\n        for k, state in enumerate(utt):\n            align = state['align_hard']\n            # print(align)\n\n            if align['enter_index']==i:\n                # open/replace occurrence\n                if occ_start is not None:\n                    print(f'warning: double open {m=} {occ_start=} {u=} {k=}')\n                occ_start = k\n            if align['leave_index']==j:\n                # close occurrence (or ignore)\n                if occ_start is None: \n                    print(f'warning: close without open {m=}')\n                    continue\n                occurrences.append((u, occ_start, k))\n                # print(f'{(u, occ_start, k)=}')\n                occ_start = None\n\n    if len(occurrences)==0:\n        print(f'warning: no occurrences of {matches} in `set_sampler_loop`')\n        return\n\n    # index into list using n\n    u, loop_start, loop_end = occurrences[n]\n\n    loop_start = self.utterance_to_global_step(u, loop_start)\n    loop_end = self.utterance_to_global_step(u, loop_end)\n    # TODO: efficient search for index of occurence\n\n    # set loop points\n    # if utterance changes, unset points which aren't set\n    # changed = u != self.sampler_utterance\n    # self.sampler_utterance = u\n\n    if start:\n        self.sampler_loop_start = loop_start\n    # elif changed:\n        # self.sampler_loop_start = 0\n\n    if end:\n        self.sampler_loop_end = loop_end\n    # elif changed:\n        # self.sampler_loop_end = None\n\n    # must reset if changing utterance\n    # if changed or reset:\n    if reset:\n        self.reset_sampler()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.set_text","title":"<code>set_text(text)</code>","text":"<p>Compute embeddings for &amp; store a new text, replacing the old text. Returns the number of embedding tokens</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text as a string</p> required <p>Returns:</p> Type Description <code>int</code> <p>length of text in tokens</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_text(self, text:str) -&gt; int:\n    \"\"\"\n    Compute embeddings for &amp; store a new text, replacing the old text.\n    Returns the number of embedding tokens\n\n    Args:\n        text: input text as a string\n\n    Returns:\n        length of text in tokens\n    \"\"\"\n    if (\n        self.text_update_thread is not None \n        and self.text_update_thread.is_alive()\n    ):\n        print('warning: text update still pending')\n\n    # TODO: more general text preprocessing\n    text, start, end = self.extract_loop_points(text)\n    tokens, text, idx_map = self.text_model.tokenize(text)\n    # print(start, end, idx_map)\n    # NOTE: positions in text may change fron tokenization (end tokens)\n    if start &lt; len(idx_map):\n        start = idx_map[start]\n    else:\n        start = 0\n    if end is not None and end &lt; len(idx_map):\n        end = idx_map[end]\n    else:\n        end = None\n\n    # text processing runs in its own thread\n    self.text_update_thread = Thread(\n        target=self._update_text, \n        args=(text, tokens, start, end), daemon=True)\n    self.text_update_thread.start()\n\n    return text\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.start_stream","title":"<code>start_stream()</code>","text":"<p>helper for start/sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def start_stream(self):\n    \"\"\"helper for start/sampler\"\"\"\n    # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.LATENT_OSC):\n    #     if not self.loop_thread.is_alive():\n    #         self.run_thread = True\n    #         self.loop_thread.start()\n    # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.SYNTH_AUDIO):\n    # if self.out_mode is not None:\n    if not self.stream.active:\n        self.stream.start()\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.step","title":"<code>step(timestamp)</code>","text":"<p>compute one vocoder frame of generation or sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step(self, timestamp):\n    \"\"\"compute one vocoder frame of generation or sampler\"\"\"\n    # if self.frontend_q.qsize() &gt; 100:\n        # self.frontend_q.get()\n        # print('frontend queue full, dropping')\n\n    state = {'text':self.text}\n\n    # reset text and model states\n    if self.needs_reset:\n        state |= self.do_reset()\n\n    # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n    if self.text_rep is None or self.step_mode==StepMode.PAUSE:\n        if len(state):\n            # self.frontend_q.put(state)\n            if self.frontend_conn is not None:\n                self.send_state(state)\n            else:\n                print('warning: frontend_conn does not exist')\n        return None\n    if self.step_mode==StepMode.GENERATION:\n        # with self.profile('step_gen'):\n        state |= self.step_gen(timestamp)\n    elif self.step_mode==StepMode.SAMPLER:\n        with self.profile('step_sampler'):\n            state |= self.step_sampler(timestamp)\n    # else:\n        # print(f'WARNING: {self.step_mode=} in step')\n\n    if self.osc_sender is not None:\n        self.osc_sender(state)\n\n    if len(self.latent_biases):\n        with torch.inference_mode():\n            bias = torch.tensor(self.latent_biases)[None]\n            state['latent_t'] += bias\n\n    # send to frontend\n    # self.frontend_q.put(state)\n    if self.frontend_conn is not None:\n        self.send_state(state)\n    else:\n        # print(state)\n        print('warning: frontend_conn does not exist')\n\n    latent = state['latent_t'] # batch, channel\n    batch, n_latent = latent.shape\n    assert batch==1\n\n    c_synth, c_latent = len(self.synth_channels), len(self.latent_channels)\n\n    with torch.inference_mode():\n        audio_frame = torch.zeros(\n            c_synth+c_latent, \n            self.model.block_size) # channel, time\n\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            # allow creative use of vocoders with mismatched sizes\n            rave_dim = self.rave.decode_params[0]\n            common_dim = min(rave_dim, latent.shape[-1])\n            latent_to_rave = torch.zeros(latent.shape[0], rave_dim)\n            latent_to_rave[:,:common_dim] = latent[:,:common_dim]\n            ### run the vocoder\n            with self.profile('vocoder'):\n                audio = self.rave.decode(\n                    latent_to_rave[...,None])[0] # channel, time\n            ###\n            audio_frame[:c_synth, :] = audio\n\n        if OutMode.LATENT_AUDIO in self.out_mode:\n            latent = torch.cat((\n                torch.zeros(batch, 1), # zero to ensure trigger\n                torch.full((batch, 1), n_latent), # trigger, number of latents\n                latent\n                ), dim=1) / 1024 # scale down in case audio gets sent to speakers\n            # latent x batch\n\n            audio_frame[c_synth:, :2+n_latent] = latent\n\n    return audio_frame\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.step_gen","title":"<code>step_gen(timestamp)</code>","text":"<p>generation branch of main <code>step</code> method</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_gen(self, timestamp):\n    \"\"\"generation branch of main `step` method\"\"\"\n    # loop end\n    if (\n        self.align_params is None \n        and self.gen_loop_end is not None \n        and not self.utterance_empty()\n        and self.prev_state('align_hard', 'index') in (\n            self.gen_loop_end, self.gen_loop_end+1)\n    ):\n        self.momentary_align_params = (self.gen_loop_start, 1)\n\n    # set just model states\n    # TODO: possibility to set to previous utterance?\n    if self.step_to_set is not None:\n        step = self.step_to_set\n        with torch.inference_mode():\n            try:\n                self.model.set_state(\n                    self.prev_state('model_state', step=step))\n            except Exception:\n                print(f'WARNING: failed to set {step=} ')\n                raise\n        self.step_to_set = None\n\n    if self.model.memory is None:\n        print('skipping: model not initialized')\n        if self.rave is not None:\n            self.audio_q.put(None)\n        return None\n\n    if not self.utterance_empty() and self.latent_feedback:\n        # pass the last vocoder frame back in\n        latent_t = self.prev_state('latent_t')\n    else:\n        latent_t = None\n\n    align_t = self.paint_alignment()\n\n    ##### run the TTS model\n    latent_t, align_t = self.process_frame(\n        # self.mode, \n        latent_t=latent_t, align_t=align_t)\n    #####\n\n    # NOTE\n    ### EXPERIMENTAL juice knob\n    # latent_t = latent_t * 1.1\n    ###\n\n    state = {\n        # 'text':self.text,\n        'latent_t':latent_t, \n        'align_t':align_t,\n        'timestamp':timestamp,\n        'model_state':self.model.get_state(),\n        'align_hard':self.hard_alignment(align_t)\n        }\n\n    if (\n        self.generate_stop_at_end \n        and state['align_hard']['index']==(len(self.text)-1)\n    ):\n        self.step_mode = StepMode.PAUSE\n        # self.reset()\n\n    utt = self.states[-1]\n    utt.append(state)\n    # remove RNN states after certain number of steps\n    if len(utt) &gt;= self.max_model_state_storage:\n        self.strip_states(utt[-self.max_model_state_storage])\n    return state\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.step_loop","title":"<code>step_loop()</code>","text":"<p>model stepping thread</p> <p>for each timestamp received in trigger_q send audio frames in audio_q</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_loop(self):\n    \"\"\"model stepping thread\n\n    for each timestamp received in trigger_q\n    send audio frames in audio_q\n    \"\"\"\n    while True:\n        t = self.trigger_q.get()\n        with self.profile('step'):\n            frame = self.step(t)\n        if frame is not None:\n            # with self.profile('frame.numpy'):\n            frame = frame.numpy()\n        self.audio_q.put(frame)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.step_sampler","title":"<code>step_sampler(timestamp)</code>","text":"<p>sampler branch of main <code>step</code> method</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_sampler(self, timestamp):\n    \"\"\"sampler branch of main `step` method\"\"\"\n    # print(f'{self.sampler_step=}')\n\n    # if self.utterance_empty(utterance=self.sampler_utterance):\n    if self.total_steps()==0:\n        print('nothing to play back')\n        self.step_mode = StepMode.PAUSE\n        return None\n\n    state = self.prev_state(global_step=self.sampler_step) or {}\n\n    self.sampler_step += 1\n\n    if (\n        self.sampler_step == self.sampler_loop_end \n        or self.sampler_step &gt;= self.total_steps()\n        ):\n        self.sampler_step = self.sampler_loop_start\n        if self.sampler_stop_at_end:\n            self.step_mode = StepMode.PAUSE\n\n\n    state = {**state}\n    state['latent_t'] = state['latent_t'].clone()\n    state['timestamp'] = timestamp\n    state['sampler'] = True\n\n    return state\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.Backend.strip_states","title":"<code>strip_states(state)</code>","text":"<p>remove RNN states</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def strip_states(self, state):\n    \"\"\"remove RNN states\"\"\"\n    state.pop('model_state', None)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.GenericOSCSender","title":"<code>GenericOSCSender</code>","text":"<p>               Bases: <code>Sender</code></p> Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>class GenericOSCSender(Sender):\n    def __init__(self, \n            host:str='127.0.0.1', \n            port:int=57120, \n            lroute:str='/tungnaa/latents',\n            sroute:str='/tungnaa/status'\n            ):\n        self.client = udp_client.SimpleUDPClient(host, port)\n        self.latents_route = lroute\n        self.status_route = sroute\n\n    def __call__(self, d:Dict[str,Any]):\n        \"\"\"\n        Args:\n            d: dictionary with 'audio_t' key, other keys are ignored in this case\n\n            this should work as a callback for the backend:\n            the input dict should be returned to later be enqueued\n        \"\"\"\n        latents = [t.float().item() for t in d['latent_t'].squeeze()]\n        self.client.send_message(self.latents_route, latents)\n        return d\n\n    def send_status(self, cmd:str, value:Union[str,float]):\n        \"\"\"\n        Send a non-data message to SuperCollider\n\n        cmd     the status type/command &lt;mute|maybe more...&gt;\n        value   the value of the command, e.g. 1.0/0.0 for mute on/off\n        \"\"\"\n        self.client.send_message(self.status_route, value)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.GenericOSCSender.__call__","title":"<code>__call__(d)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Any]</code> <p>dictionary with 'audio_t' key, other keys are ignored in this case</p> required <code>this</code> <code>should work as a callback for the backend</code> required Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def __call__(self, d:Dict[str,Any]):\n    \"\"\"\n    Args:\n        d: dictionary with 'audio_t' key, other keys are ignored in this case\n\n        this should work as a callback for the backend:\n        the input dict should be returned to later be enqueued\n    \"\"\"\n    latents = [t.float().item() for t in d['latent_t'].squeeze()]\n    self.client.send_message(self.latents_route, latents)\n    return d\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.GenericOSCSender.send_status","title":"<code>send_status(cmd, value)</code>","text":"<p>Send a non-data message to SuperCollider</p> <p>cmd     the status type/command  value   the value of the command, e.g. 1.0/0.0 for mute on/off Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def send_status(self, cmd:str, value:Union[str,float]):\n    \"\"\"\n    Send a non-data message to SuperCollider\n\n    cmd     the status type/command &lt;mute|maybe more...&gt;\n    value   the value of the command, e.g. 1.0/0.0 for mute on/off\n    \"\"\"\n    self.client.send_message(self.status_route, value)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.SCSynthDirectOSCSender","title":"<code>SCSynthDirectOSCSender</code>","text":"<p>               Bases: <code>Sender</code></p> <p>sends timed bundles directly to scsynth to set a control bus</p> Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>class SCSynthDirectOSCSender(Sender):\n    \"\"\"sends timed bundles directly to scsynth to set a control bus\"\"\"\n    def __init__(self, \n            host:str='127.0.0.1', \n            port:int=57110, \n            bus_index:int=64,\n            latency:float=0.2,\n            ):\n        self.client = udp_client.SimpleUDPClient(host, port)\n        self.bus_index = bus_index\n        self.latency = latency\n\n    def __call__(self, d:Dict[str,Any]):\n        \"\"\"\n        Args:\n            d: dictionary with 'timestamp' and 'audio_t' keys,\n                other keys are ignored in this case\n        \"\"\"\n        bb = OscBundleBuilder(d['timestamp'] + self.latency)\n        mb = OscMessageBuilder('/c_set')\n\n        latents = [t.float().item() for t in d['latent_t'].squeeze()]\n        for i,l in enumerate(latents):\n            mb.add_arg(i+self.bus_index)\n            mb.add_arg(l)\n\n        bb.add_content(mb.build())\n        bundle = bb.build()\n\n        self.client.send(bundle)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.SCSynthDirectOSCSender.__call__","title":"<code>__call__(d)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Any]</code> <p>dictionary with 'timestamp' and 'audio_t' keys, other keys are ignored in this case</p> required Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def __call__(self, d:Dict[str,Any]):\n    \"\"\"\n    Args:\n        d: dictionary with 'timestamp' and 'audio_t' keys,\n            other keys are ignored in this case\n    \"\"\"\n    bb = OscBundleBuilder(d['timestamp'] + self.latency)\n    mb = OscMessageBuilder('/c_set')\n\n    latents = [t.float().item() for t in d['latent_t'].squeeze()]\n    for i,l in enumerate(latents):\n        mb.add_arg(i+self.bus_index)\n        mb.add_arg(l)\n\n    bb.add_content(mb.build())\n    bundle = bb.build()\n\n    self.client.send(bundle)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder","title":"<code>TacotronDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/tungnaa/model.py</code> <pre><code>class TacotronDecoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=None, # text embedding dim\n        frame_channels=None, # RAVE latent dim\n        dropout=0.1,\n        likelihood_type='nsf',#'normal'#'mixture'#'ged'\n        mixture_n=16,\n        flow_context=256,\n        flow_hidden=256,\n        flow_layers=2,\n        flow_blocks=2,\n        nsf_bins=16,\n        ged_hidden=256,\n        ged_layers=4,\n        ged_unfold=None,\n        ged_multiply_params=False,\n        ged_project_params=None,\n        ged_glu=False,\n        ged_dropout=None, #None follows main dropout, 0 turns off\n        dropout_type='dropout', #'zoneout'\n        prenet_type='original', # disabled\n        prenet_dropout=0.2,\n        prenet_layers=2,\n        prenet_size=256,\n        prenet_wn=False,\n        hidden_dropout=0,\n        separate_stopnet=True, # disabled\n        max_decoder_steps=10000,\n        text_encoder:Dict=None,\n        rnn_size=1200,\n        rnn_bias=True,\n        rnn_layers=1,\n        noise_channels=0,\n        decoder_type='lstm',\n        decoder_layers=1,\n        decoder_size=None,\n        hidden_to_decoder=True,\n        memory_to_decoder=False,\n        init_proj=1.0,\n        proj_wn=False,\n        attn_wn=False,\n        learn_go_frame=False,\n        pitch_xform=False,\n        length_reparam=False,\n        text_encoder_type='canine',\n        block_size=2048,\n        max_batch=8,\n        max_tokens=1024,\n        prior_filter_len=11,\n        tokens_per_frame=1.0,\n        attention_type='dca',\n        script=False\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): number of input channels.\n            frame_channels (int): number of feature frame channels.\n            dropout (float): dropout rate (except prenet).\n            prenet_dropout (float): prenet dropout rate.\n            max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n            text_encoder: dict of text encoder kwargs\n        \"\"\"\n        super().__init__()\n        assert frame_channels is not None\n\n        if length_reparam:\n            frame_channels = frame_channels + 1\n\n        self.B = max_batch\n        self.T = max_tokens\n\n        if text_encoder_type not in [None, 'none']:\n            if text_encoder is None: text_encoder = {}\n            if text_encoder_type=='zero':\n                self.text_encoder = ZeroEncoder(**text_encoder)\n            elif text_encoder_type=='baseline':\n                self.text_encoder = TacotronEncoder(**text_encoder)\n            elif text_encoder_type=='canine':\n                self.text_encoder = CanineEncoder(**text_encoder)\n            elif text_encoder_type=='canine_embedding':\n                self.text_encoder = CanineEmbeddings(**text_encoder)\n            else:\n                raise ValueError(text_encoder_type)\n            if in_channels is None:\n                in_channels = self.text_encoder.channels\n            elif in_channels != self.text_encoder.channels:\n                raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n        else:\n            self.text_encoder = None\n            assert in_channels is not None\n\n        self.max_decoder_steps = max_decoder_steps\n        self.block_size = block_size\n        self.frame_channels = frame_channels\n        # self.pitch_xform = pitch_xform\n        # # self.r_init = r\n        # # self.r = r\n        # self.encoder_embedding_dim = in_channels\n        # self.separate_stopnet = separate_stopnet\n        # self.max_decoder_steps = max_decoder_steps\n        self.stop_threshold = 0.5\n\n        # decoder_size = decoder_size or rnn_size\n\n        # # model dimensions\n        # self.decoder_layers = decoder_layers\n        # self.attention_hidden_dim = rnn_size\n        # self.decoder_rnn_dim = decoder_size\n        # self.prenet_dim = prenet_size\n        # # self.attn_dim = 128\n        # self.p_attention_dropout = dropout\n        # self.p_decoder_dropout = dropout\n        # self.dropout_type = dropout_type\n\n        self.prenet_dropout = prenet_dropout\n\n        # the likelihood converts a hidden state to a probability distribution\n        # over each vocoder frame.\n        # in the simpler cases this is just unpacking location and scale from\n        # the hidden state.\n        # in other cases the likelihood can be a normalizing flow with its own\n        # trainable parameters.\n        if likelihood_type=='normal':\n            self.likelihood = StandardNormal()\n        elif likelihood_type=='diagonal':\n            self.likelihood = DiagonalNormal()\n        elif likelihood_type=='mixture':\n            self.likelihood = DiagonalNormalMixture(mixture_n)\n        elif likelihood_type=='ged':\n            ged_dropout = dropout if ged_dropout is None else ged_dropout\n            self.likelihood = GED(\n                self.frame_channels,\n                hidden_size=ged_hidden, hidden_layers=ged_layers, \n                dropout=ged_dropout, unfold=ged_unfold, \n                multiply_params=ged_multiply_params,\n                project_params=ged_project_params,\n                glu=ged_glu\n                )\n        elif likelihood_type=='nsf':\n            self.likelihood = NSF(\n                self.frame_channels, context_size=flow_context,\n                hidden_size=flow_hidden, hidden_layers=flow_layers,\n                blocks=flow_blocks, bins=nsf_bins,\n                dropout=dropout)\n        else:\n            raise ValueError(likelihood_type)\n\n        if script and likelihood_type!='nsf':\n            self.likelihood = torch.jit.script(self.likelihood)\n\n        self.core = TacotronCore(\n            in_channels=in_channels, # text embedding dim\n            out_channels=self.likelihood.n_params(self.frame_channels),\n            frame_channels=frame_channels, # RAVE latent dim\n            dropout=dropout,\n            dropout_type=dropout_type, #'zoneout'\n            prenet_type=prenet_type, # disabled\n            prenet_dropout=prenet_dropout,\n            prenet_layers=prenet_layers,\n            prenet_size=prenet_size,\n            prenet_wn=prenet_wn,\n            hidden_dropout=hidden_dropout,\n            separate_stopnet=separate_stopnet, # disabled\n            rnn_size=rnn_size,\n            rnn_bias=rnn_bias,\n            rnn_layers=rnn_layers,\n            noise_channels=noise_channels,\n            decoder_type=decoder_type,\n            decoder_layers=decoder_layers,\n            decoder_size=decoder_size,\n            hidden_to_decoder=hidden_to_decoder,\n            memory_to_decoder=memory_to_decoder,\n            init_proj=init_proj,\n            proj_wn=proj_wn,\n            attn_wn=attn_wn,\n            learn_go_frame=learn_go_frame,\n            pitch_xform=pitch_xform,\n            block_size=block_size,\n            max_batch=max_batch,\n            max_tokens=max_tokens,\n            prior_filter_len=prior_filter_len,\n            tokens_per_frame=tokens_per_frame,\n            attention_type=attention_type,\n            length_reparam=length_reparam\n        )\n\n        if script:\n            self.core = torch.jit.script(self.core)\n\n    @property\n    def memory(self):\n        return self.core.memory\n\n    @classmethod\n    def from_checkpoint(cls, path_or_dict):\n        if isinstance(path_or_dict, dict):\n            ckpt = path_or_dict\n        else:\n            ckpt = torch.load(\n                path_or_dict, map_location='cpu', weights_only=False)\n\n        kw = ckpt['kw']\n        model_kw = cls.update_kw_dict(kw['model'])\n\n        model = cls(**model_kw)\n        try:\n            model.load_state_dict(ckpt['model_state'], strict=True)\n        except Exception as e:\n            print(e.__traceback__)\n            model.load_state_dict(ckpt['model_state'], strict=False)\n\n\n        return model\n\n    @classmethod\n    def update_kw_dict(cls, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        b = d.pop('text_bottleneck', None)\n        if b is not None:\n            if d['text_encoder'] is None:\n                d['text_encoder'] = {}\n            d['text_encoder']['bottleneck'] = b\n        return d\n\n    @torch.jit.ignore\n    def update_state_dict(self, d):\n        \"\"\"backward compatibility with older checkpoints\"\"\"\n        # TODO: core.\n        def replace(old, new):\n            t = d.pop(old, None)\n            if t is not None:\n                d[new] = t\n        replace('go_attention_rnn_cell_state', 'go_attention_cell')\n        replace('go_query', 'go_attention_hidden')\n        # rnncell -&gt; dropoutrnn\n        replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n        replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n        replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n        replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n        # dropoutrnn -&gt; residualrnn\n        replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n        replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n        replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n        replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n        for name in (\n            'go_attention_hidden', 'go_attention_cell',\n            'attention_hidden', 'attention_cell'\n            ):\n            if name in d and d[name].ndim==2:\n                d[name] = d[name][None]\n        # move into core\n        for name in list(d):\n            if any(name.startswith(s) for s in (\n                \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n                replace(name, f'core.{name}')\n\n        # text bottleneck -&gt; text encoder\n        replace('core.text_proj.weight', 'text_encoder.proj.weight')\n        replace('core.text_proj.bias', 'text_encoder.proj.bias')\n        return d\n\n    @torch.jit.ignore\n    def load_state_dict(self, d, **kw):\n        super().load_state_dict(self.update_state_dict(d), **kw)\n\n    @torch.jit.export\n    def reset(self, inputs):\n        r\"\"\"\n        populates buffers with initial states and text inputs\n\n        call with encoded text, before using `step`\n\n        Args:\n            inputs: (B, T_text, D_text)\n\n        \"\"\"\n        self.core.reset(inputs)\n\n    # TODO: should there be option to include acoustic memory here?\n    @torch.jit.export\n    def get_state(self):\n        return self.core.get_state()\n\n    @torch.jit.export\n    def set_state(self, \n            state:Dict[str,Tensor|Tuple[Tensor, Tensor, Tensor, Tensor]]):\n        self.core.set_state(state)\n\n    def latent_map(self, z):\n        return self.core.latent_map(z)\n\n    def latent_unmap(self, z):\n        return self.core.latent_unmap(z)\n\n    def chunk_pad(self, inputs, mask, c=128):\n        b, t = mask.shape\n        p = math.ceil(t / c) * c - t\n        if p&gt;0:\n            inputs = torch.cat(\n                (inputs, inputs.new_zeros(b, p, *inputs.shape[2:])), 1)\n            mask = torch.cat(\n                (mask, mask.new_zeros(b, p)), 1)\n        return inputs, mask\n\n\n    @torch.jit.ignore\n    def forward(self, inputs, audio, mask, audio_mask,\n            audio_lengths:Optional[Tensor]=None,\n            prenet_dropout:Optional[float]=None,\n            chunk_pad_text:int|None=None,\n            chunk_pad_audio:int|None=None,\n            temperature:float=1\n            ):\n        r\"\"\"Train Decoder with teacher forcing.\n        Args:\n            inputs: raw or encoded text.\n            audio: audio frames for teacher-forcing.\n            mask: text mask for sequence padding.\n            audio_mask: audio mask for loss computation.\n            prenet_dropout: if not None, override original value\n                (to implement e.g. annealing)\n            temperature: no effect on training, only on returned output/MSE\n        Shapes:\n            - inputs: \n                FloatTensor (B, T_text, D_text)\n                or LongTensor (B, T_text)\n            - audio: (B, T_audio, D_audio)\n            - mask: (B, T_text)\n            - audio_mask: (B, T_audio)\n            - stop_target TODO\n\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_audio, T_text)\n            - stop_tokens: (B, T_audio)\n\n        \"\"\"\n        if chunk_pad_audio:\n            audio, audio_mask = self.chunk_pad(\n                audio, audio_mask, chunk_pad_audio)\n\n        if audio_lengths is None:\n            audio_lengths = audio_mask.sum(-1).cpu()\n        ground_truth = audio\n        if prenet_dropout is None:\n            prenet_dropout = self.prenet_dropout\n\n        # print(f'{audio[...,0].min()=}')\n        audio = self.latent_map(audio)\n        # print(f'{audio[...,0].min()=}')\n\n        if inputs.dtype==torch.long:\n            assert self.text_encoder is not None\n            assert inputs.ndim==2\n            inputs = self.text_encoder.encode(inputs, mask)\n        if chunk_pad_text:\n            inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n        (\n            memory, context, alignment,\n            attention_hidden, attention_cell, \n            decoder_hidden, decoder_cell \n        ) = self.core.init_states(inputs)  \n\n        # concat the initial audio frame with training data\n        memories = torch.cat((memory[None], audio.transpose(0, 1)))\n        memories = self.core.prenet(memories, prenet_dropout)\n\n        # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n        hidden, contexts, alignments = self.core.decode_loop(\n            inputs, context, memories, alignment,\n            attention_hidden, attention_cell,\n            mask)\n\n        # compute the additional decoder layers \n        hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n            hidden, contexts, memories[:-1].transpose(0,1),\n            decoder_hidden, decoder_cell, \n            audio_lengths)\n\n        r, outputs = self.run_likelihood(\n            audio, audio_mask, output_params, temperature=temperature)\n\n        stop_loss = None\n        # TODO\n        # stop_tokens = self.predict_stop(hidden, outputs)\n        # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n        outputs = self.latent_unmap(outputs)\n\n        r.update({\n            'text': inputs,\n            # 'stop_loss': stop_loss,\n            'predicted': outputs,\n            'ground_truth': ground_truth,\n            'alignment': alignments,\n            'params': output_params,\n            # 'stop': stop_tokens,\n            'audio_mask': audio_mask,\n            'text_mask': mask,\n            **self.likelihood.metrics(output_params),\n            **self.alignment_metrics(alignments, mask, audio_mask)\n        })\n\n        return r\n\n    def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n        \"\"\"\n        alignments: (B, T_audio, T_text)\n        mask: (B, T_text)\n        \"\"\"\n        # TODO: could normalize concentration by logT and subtract from 1,\n        # so it represents a proportion of the entropy 'unused'\n        # then could have a cutoff parameter\n        # alignment should hit every token: max. entropy of mean token probs\n        # alignment should be sharp: min. mean of token entropy\n        concentration = []\n        concentration_norm = []\n        dispersion = []\n        # alternatively:\n        # max average length of token prob vectors, and of time-curve vectors\n        # if using L2, this enourages token energy to concentrate in few token\n        # dimensions per vector, but to spread across multiple time steps \n        # concentration_l2 = []\n        # dispersion_l2 = []\n        for a,mt,ma in zip(alignments, mask, audio_mask):\n            a = a[ma][:,mt]\n            mean_probs = a.mean(0)\n            ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n            # ent_mean = torch.special.entr(mean_probs).sum()\n            concentration.append(ent_mean)\n            concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n            dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n            # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n            # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n            # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n        return {\n            'concentration': torch.stack(concentration),#.mean(),\n            'concentration_norm': torch.stack(concentration_norm),#.mean(),\n            'dispersion': torch.stack(dispersion),#.mean(),\n            # 'concentration_l2': torch.stack(concentration_l2).mean(),\n            # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n        }\n\n    @torch.jit.ignore\n    def run_likelihood(self, \n            audio, audio_mask, output_params, temperature:float=1):\n        r = {}\n        m = audio_mask[...,None]\n        audio_m = audio*m\n        params_m = output_params*m\n        # nll = self.likelihood(audio*m, output_params*m)\n        # nll = nll.masked_select(audio_mask).mean()\n\n        # NOTE: could improve training performance here?\n        #   use audio_mask before likelihood instead of after\n        r['nll'] = (\n            self.likelihood(audio_m, params_m)\n            .masked_select(audio_mask).mean())\n\n        if isinstance(self.likelihood, DiagonalNormalMixture):\n            r['nll_fixed'] = (\n                self.likelihood(audio_m, params_m, mode='fixed')\n                .masked_select(audio_mask).mean())\n            r['nll_posthoc'] = (\n                self.likelihood(audio_m, params_m, mode='posthoc')\n                .masked_select(audio_mask).mean())\n\n        with torch.no_grad():\n            # return self.likelihood.sample(output_params, temperature=0)\n            outputs = self.likelihood.sample(\n                output_params, temperature=temperature) # low memory\n\n        d = audio_m - outputs*m\n        r['mse'] = (d*d).mean() * m.numel() / m.float().sum()\n        return r, outputs\n\n    # @torch.jit.ignore\n    @torch.jit.export\n    def step(self, \n            alignment:Optional[Tensor]=None,\n            audio_frame:Optional[Tensor]=None, \n            temperature:float=1.0\n        ):\n        r\"\"\"\n        single step of inference.\n\n        optionally supply `alignment` to force the alignments.\n        optionally supply `audio_frame` to set the previous frame.\n\n        Args:\n            alignment: B x T_text\n            audio_frame: B x D_audio\n            temperature: optional sampling temperature\n        Returns:\n            output: B x D_audio\n            alignment: B x T_text\n            stop_token: None (not implemented)\n        \"\"\"\n        alignment, output_params = self.core.step_pre(alignment, audio_frame)\n        decoder_output = self.likelihood.sample(\n            output_params, temperature=temperature).squeeze(1)\n\n        decoder_output = self.core.step_post(alignment, decoder_output)\n\n        # if debug:\n        return dict(\n            output=decoder_output,\n            alignment=alignment,\n            stop_prob=torch.tensor(0.),\n            params=output_params\n        )\n        # else: \n            # return decoder_output, alignment, 0\n\n    # TODO rewrite this or don't script it?\n    # @torch.jit.export\n    @torch.jit.ignore\n    def inference(self, inputs, \n            stop:bool=True, \n            max_steps:Optional[int]=None, \n            temperature:float=1.0,\n            alignments:Optional[Tensor]=None,\n            audio:Optional[Tensor]=None,\n            ):\n        r\"\"\"Decoder inference\n        Can supply forced alignments to text;\n        Can also supply audio for teacher forcing, \n        to test consistency with the training code or implement prompting\n        Args:\n            inputs: Text Encoder outputs.\n            stop: use stop gate\n            max_steps: stop after this many decoder steps\n            temperature: for the sampling distribution\n            alignments: forced alignment\n            audio: forced last-frame\n                (will be offset, you don't need to prepend a start frame)\n        Shapes:\n            - inputs: (B, T_text, D_text)\n            - outputs: (B, T_audio, D_audio)\n            - alignments: (B, T_text, T_audio)\n            - audio: (B, T_audio, D_audio)\n            - stop_tokens: (B, T_audio)\n        \"\"\"\n        # max_steps = max_steps or self.max_decoder_steps\n        max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n        self.reset(inputs)\n\n        outputs = []\n        if alignments is None:\n            alignments = []\n            feed_alignment = False\n        else:\n            feed_alignment = True\n\n        for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n            alignment = alignments[:,:,i] if feed_alignment else None\n            audio_frame = (\n                audio[:,i-1,:] \n                if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n                else None)\n            r = self.step(\n                temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n            outputs.append(r['output'])\n            if not feed_alignment:\n                alignments.append(r['alignment'])\n\n            if stop and r['stop_prob']&gt;self.stop_threshold:\n                break\n            if len(outputs) == max_steps:\n                if stop:\n                    print(f\"   &gt; Decoder stopped with {max_steps=}\")\n                break\n\n        outputs = torch.stack(outputs, 1)\n        if not feed_alignment:\n            alignments = torch.stack(alignments, 1)\n\n        return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.__init__","title":"<code>__init__(in_channels=None, frame_channels=None, dropout=0.1, likelihood_type='nsf', mixture_n=16, flow_context=256, flow_hidden=256, flow_layers=2, flow_blocks=2, nsf_bins=16, ged_hidden=256, ged_layers=4, ged_unfold=None, ged_multiply_params=False, ged_project_params=None, ged_glu=False, ged_dropout=None, dropout_type='dropout', prenet_type='original', prenet_dropout=0.2, prenet_layers=2, prenet_size=256, prenet_wn=False, hidden_dropout=0, separate_stopnet=True, max_decoder_steps=10000, text_encoder=None, rnn_size=1200, rnn_bias=True, rnn_layers=1, noise_channels=0, decoder_type='lstm', decoder_layers=1, decoder_size=None, hidden_to_decoder=True, memory_to_decoder=False, init_proj=1.0, proj_wn=False, attn_wn=False, learn_go_frame=False, pitch_xform=False, length_reparam=False, text_encoder_type='canine', block_size=2048, max_batch=8, max_tokens=1024, prior_filter_len=11, tokens_per_frame=1.0, attention_type='dca', script=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels.</p> <code>None</code> <code>frame_channels</code> <code>int</code> <p>number of feature frame channels.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>dropout rate (except prenet).</p> <code>0.1</code> <code>prenet_dropout</code> <code>float</code> <p>prenet dropout rate.</p> <code>0.2</code> <code>max_decoder_steps</code> <code>int</code> <p>Maximum number of steps allowed for the decoder. Defaults to 10000.</p> <code>10000</code> <code>text_encoder</code> <code>Dict</code> <p>dict of text encoder kwargs</p> <code>None</code> Source code in <code>src/tungnaa/model.py</code> <pre><code>def __init__(\n    self,\n    in_channels=None, # text embedding dim\n    frame_channels=None, # RAVE latent dim\n    dropout=0.1,\n    likelihood_type='nsf',#'normal'#'mixture'#'ged'\n    mixture_n=16,\n    flow_context=256,\n    flow_hidden=256,\n    flow_layers=2,\n    flow_blocks=2,\n    nsf_bins=16,\n    ged_hidden=256,\n    ged_layers=4,\n    ged_unfold=None,\n    ged_multiply_params=False,\n    ged_project_params=None,\n    ged_glu=False,\n    ged_dropout=None, #None follows main dropout, 0 turns off\n    dropout_type='dropout', #'zoneout'\n    prenet_type='original', # disabled\n    prenet_dropout=0.2,\n    prenet_layers=2,\n    prenet_size=256,\n    prenet_wn=False,\n    hidden_dropout=0,\n    separate_stopnet=True, # disabled\n    max_decoder_steps=10000,\n    text_encoder:Dict=None,\n    rnn_size=1200,\n    rnn_bias=True,\n    rnn_layers=1,\n    noise_channels=0,\n    decoder_type='lstm',\n    decoder_layers=1,\n    decoder_size=None,\n    hidden_to_decoder=True,\n    memory_to_decoder=False,\n    init_proj=1.0,\n    proj_wn=False,\n    attn_wn=False,\n    learn_go_frame=False,\n    pitch_xform=False,\n    length_reparam=False,\n    text_encoder_type='canine',\n    block_size=2048,\n    max_batch=8,\n    max_tokens=1024,\n    prior_filter_len=11,\n    tokens_per_frame=1.0,\n    attention_type='dca',\n    script=False\n):\n    \"\"\"\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        dropout (float): dropout rate (except prenet).\n        prenet_dropout (float): prenet dropout rate.\n        max_decoder_steps (int): Maximum number of steps allowed for the decoder. Defaults to 10000.\n        text_encoder: dict of text encoder kwargs\n    \"\"\"\n    super().__init__()\n    assert frame_channels is not None\n\n    if length_reparam:\n        frame_channels = frame_channels + 1\n\n    self.B = max_batch\n    self.T = max_tokens\n\n    if text_encoder_type not in [None, 'none']:\n        if text_encoder is None: text_encoder = {}\n        if text_encoder_type=='zero':\n            self.text_encoder = ZeroEncoder(**text_encoder)\n        elif text_encoder_type=='baseline':\n            self.text_encoder = TacotronEncoder(**text_encoder)\n        elif text_encoder_type=='canine':\n            self.text_encoder = CanineEncoder(**text_encoder)\n        elif text_encoder_type=='canine_embedding':\n            self.text_encoder = CanineEmbeddings(**text_encoder)\n        else:\n            raise ValueError(text_encoder_type)\n        if in_channels is None:\n            in_channels = self.text_encoder.channels\n        elif in_channels != self.text_encoder.channels:\n            raise ValueError(f'{in_channels=} but {self.text_encoder.channels=}')\n    else:\n        self.text_encoder = None\n        assert in_channels is not None\n\n    self.max_decoder_steps = max_decoder_steps\n    self.block_size = block_size\n    self.frame_channels = frame_channels\n    # self.pitch_xform = pitch_xform\n    # # self.r_init = r\n    # # self.r = r\n    # self.encoder_embedding_dim = in_channels\n    # self.separate_stopnet = separate_stopnet\n    # self.max_decoder_steps = max_decoder_steps\n    self.stop_threshold = 0.5\n\n    # decoder_size = decoder_size or rnn_size\n\n    # # model dimensions\n    # self.decoder_layers = decoder_layers\n    # self.attention_hidden_dim = rnn_size\n    # self.decoder_rnn_dim = decoder_size\n    # self.prenet_dim = prenet_size\n    # # self.attn_dim = 128\n    # self.p_attention_dropout = dropout\n    # self.p_decoder_dropout = dropout\n    # self.dropout_type = dropout_type\n\n    self.prenet_dropout = prenet_dropout\n\n    # the likelihood converts a hidden state to a probability distribution\n    # over each vocoder frame.\n    # in the simpler cases this is just unpacking location and scale from\n    # the hidden state.\n    # in other cases the likelihood can be a normalizing flow with its own\n    # trainable parameters.\n    if likelihood_type=='normal':\n        self.likelihood = StandardNormal()\n    elif likelihood_type=='diagonal':\n        self.likelihood = DiagonalNormal()\n    elif likelihood_type=='mixture':\n        self.likelihood = DiagonalNormalMixture(mixture_n)\n    elif likelihood_type=='ged':\n        ged_dropout = dropout if ged_dropout is None else ged_dropout\n        self.likelihood = GED(\n            self.frame_channels,\n            hidden_size=ged_hidden, hidden_layers=ged_layers, \n            dropout=ged_dropout, unfold=ged_unfold, \n            multiply_params=ged_multiply_params,\n            project_params=ged_project_params,\n            glu=ged_glu\n            )\n    elif likelihood_type=='nsf':\n        self.likelihood = NSF(\n            self.frame_channels, context_size=flow_context,\n            hidden_size=flow_hidden, hidden_layers=flow_layers,\n            blocks=flow_blocks, bins=nsf_bins,\n            dropout=dropout)\n    else:\n        raise ValueError(likelihood_type)\n\n    if script and likelihood_type!='nsf':\n        self.likelihood = torch.jit.script(self.likelihood)\n\n    self.core = TacotronCore(\n        in_channels=in_channels, # text embedding dim\n        out_channels=self.likelihood.n_params(self.frame_channels),\n        frame_channels=frame_channels, # RAVE latent dim\n        dropout=dropout,\n        dropout_type=dropout_type, #'zoneout'\n        prenet_type=prenet_type, # disabled\n        prenet_dropout=prenet_dropout,\n        prenet_layers=prenet_layers,\n        prenet_size=prenet_size,\n        prenet_wn=prenet_wn,\n        hidden_dropout=hidden_dropout,\n        separate_stopnet=separate_stopnet, # disabled\n        rnn_size=rnn_size,\n        rnn_bias=rnn_bias,\n        rnn_layers=rnn_layers,\n        noise_channels=noise_channels,\n        decoder_type=decoder_type,\n        decoder_layers=decoder_layers,\n        decoder_size=decoder_size,\n        hidden_to_decoder=hidden_to_decoder,\n        memory_to_decoder=memory_to_decoder,\n        init_proj=init_proj,\n        proj_wn=proj_wn,\n        attn_wn=attn_wn,\n        learn_go_frame=learn_go_frame,\n        pitch_xform=pitch_xform,\n        block_size=block_size,\n        max_batch=max_batch,\n        max_tokens=max_tokens,\n        prior_filter_len=prior_filter_len,\n        tokens_per_frame=tokens_per_frame,\n        attention_type=attention_type,\n        length_reparam=length_reparam\n    )\n\n    if script:\n        self.core = torch.jit.script(self.core)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.alignment_metrics","title":"<code>alignment_metrics(alignments, mask, audio_mask, t=2)</code>","text":"<p>alignments: (B, T_audio, T_text) mask: (B, T_text)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>def alignment_metrics(self, alignments, mask, audio_mask, t=2):\n    \"\"\"\n    alignments: (B, T_audio, T_text)\n    mask: (B, T_text)\n    \"\"\"\n    # TODO: could normalize concentration by logT and subtract from 1,\n    # so it represents a proportion of the entropy 'unused'\n    # then could have a cutoff parameter\n    # alignment should hit every token: max. entropy of mean token probs\n    # alignment should be sharp: min. mean of token entropy\n    concentration = []\n    concentration_norm = []\n    dispersion = []\n    # alternatively:\n    # max average length of token prob vectors, and of time-curve vectors\n    # if using L2, this enourages token energy to concentrate in few token\n    # dimensions per vector, but to spread across multiple time steps \n    # concentration_l2 = []\n    # dispersion_l2 = []\n    for a,mt,ma in zip(alignments, mask, audio_mask):\n        a = a[ma][:,mt]\n        mean_probs = a.mean(0)\n        ent_mean = (mean_probs * mean_probs.clip(1e-7,1).log()).sum()\n        # ent_mean = torch.special.entr(mean_probs).sum()\n        concentration.append(ent_mean)\n        concentration_norm.append(1 + ent_mean / mt.float().sum().log())\n        dispersion.append(-(a*a.clip(1e-7,1).log()).sum(-1).mean())\n        # dispersion.append(-torch.special.entr(a).sum(-1).mean())\n        # concentration_l2.append(-a.pow(t).mean(0).pow(1/t).mean())\n        # dispersion_l2.append(-a.pow(t).mean(1).pow(1/t).mean())\n    return {\n        'concentration': torch.stack(concentration),#.mean(),\n        'concentration_norm': torch.stack(concentration_norm),#.mean(),\n        'dispersion': torch.stack(dispersion),#.mean(),\n        # 'concentration_l2': torch.stack(concentration_l2).mean(),\n        # 'dispersion_l2': torch.stack(dispersion_l2).mean()\n    }\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.forward","title":"<code>forward(inputs, audio, mask, audio_mask, audio_lengths=None, prenet_dropout=None, chunk_pad_text=None, chunk_pad_audio=None, temperature=1)</code>","text":"<p>Train Decoder with teacher forcing. Args:     inputs: raw or encoded text.     audio: audio frames for teacher-forcing.     mask: text mask for sequence padding.     audio_mask: audio mask for loss computation.     prenet_dropout: if not None, override original value         (to implement e.g. annealing)     temperature: no effect on training, only on returned output/MSE Shapes:     - inputs:          FloatTensor (B, T_text, D_text)         or LongTensor (B, T_text)     - audio: (B, T_audio, D_audio)     - mask: (B, T_text)     - audio_mask: (B, T_audio)     - stop_target TODO</p> <pre><code>- outputs: (B, T_audio, D_audio)\n- alignments: (B, T_audio, T_text)\n- stop_tokens: (B, T_audio)\n</code></pre> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef forward(self, inputs, audio, mask, audio_mask,\n        audio_lengths:Optional[Tensor]=None,\n        prenet_dropout:Optional[float]=None,\n        chunk_pad_text:int|None=None,\n        chunk_pad_audio:int|None=None,\n        temperature:float=1\n        ):\n    r\"\"\"Train Decoder with teacher forcing.\n    Args:\n        inputs: raw or encoded text.\n        audio: audio frames for teacher-forcing.\n        mask: text mask for sequence padding.\n        audio_mask: audio mask for loss computation.\n        prenet_dropout: if not None, override original value\n            (to implement e.g. annealing)\n        temperature: no effect on training, only on returned output/MSE\n    Shapes:\n        - inputs: \n            FloatTensor (B, T_text, D_text)\n            or LongTensor (B, T_text)\n        - audio: (B, T_audio, D_audio)\n        - mask: (B, T_text)\n        - audio_mask: (B, T_audio)\n        - stop_target TODO\n\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_audio, T_text)\n        - stop_tokens: (B, T_audio)\n\n    \"\"\"\n    if chunk_pad_audio:\n        audio, audio_mask = self.chunk_pad(\n            audio, audio_mask, chunk_pad_audio)\n\n    if audio_lengths is None:\n        audio_lengths = audio_mask.sum(-1).cpu()\n    ground_truth = audio\n    if prenet_dropout is None:\n        prenet_dropout = self.prenet_dropout\n\n    # print(f'{audio[...,0].min()=}')\n    audio = self.latent_map(audio)\n    # print(f'{audio[...,0].min()=}')\n\n    if inputs.dtype==torch.long:\n        assert self.text_encoder is not None\n        assert inputs.ndim==2\n        inputs = self.text_encoder.encode(inputs, mask)\n    if chunk_pad_text:\n        inputs, mask = self.chunk_pad(inputs, mask, chunk_pad_text)\n\n    (\n        memory, context, alignment,\n        attention_hidden, attention_cell, \n        decoder_hidden, decoder_cell \n    ) = self.core.init_states(inputs)  \n\n    # concat the initial audio frame with training data\n    memories = torch.cat((memory[None], audio.transpose(0, 1)))\n    memories = self.core.prenet(memories, prenet_dropout)\n\n    # print(f'{inputs.shape=}, {context.shape=}, {memories.shape=}, {alignment.shape=}, {mask.shape=}')\n    hidden, contexts, alignments = self.core.decode_loop(\n        inputs, context, memories, alignment,\n        attention_hidden, attention_cell,\n        mask)\n\n    # compute the additional decoder layers \n    hidden, output_params, decoder_hidden, decoder_cell = self.core.decode_post(\n        hidden, contexts, memories[:-1].transpose(0,1),\n        decoder_hidden, decoder_cell, \n        audio_lengths)\n\n    r, outputs = self.run_likelihood(\n        audio, audio_mask, output_params, temperature=temperature)\n\n    stop_loss = None\n    # TODO\n    # stop_tokens = self.predict_stop(hidden, outputs)\n    # stop_loss = compute_stop_loss(stop_target, stop_tokens)\n\n    outputs = self.latent_unmap(outputs)\n\n    r.update({\n        'text': inputs,\n        # 'stop_loss': stop_loss,\n        'predicted': outputs,\n        'ground_truth': ground_truth,\n        'alignment': alignments,\n        'params': output_params,\n        # 'stop': stop_tokens,\n        'audio_mask': audio_mask,\n        'text_mask': mask,\n        **self.likelihood.metrics(output_params),\n        **self.alignment_metrics(alignments, mask, audio_mask)\n    })\n\n    return r\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.inference","title":"<code>inference(inputs, stop=True, max_steps=None, temperature=1.0, alignments=None, audio=None)</code>","text":"<p>Decoder inference Can supply forced alignments to text; Can also supply audio for teacher forcing,  to test consistency with the training code or implement prompting Args:     inputs: Text Encoder outputs.     stop: use stop gate     max_steps: stop after this many decoder steps     temperature: for the sampling distribution     alignments: forced alignment     audio: forced last-frame         (will be offset, you don't need to prepend a start frame) Shapes:     - inputs: (B, T_text, D_text)     - outputs: (B, T_audio, D_audio)     - alignments: (B, T_text, T_audio)     - audio: (B, T_audio, D_audio)     - stop_tokens: (B, T_audio)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef inference(self, inputs, \n        stop:bool=True, \n        max_steps:Optional[int]=None, \n        temperature:float=1.0,\n        alignments:Optional[Tensor]=None,\n        audio:Optional[Tensor]=None,\n        ):\n    r\"\"\"Decoder inference\n    Can supply forced alignments to text;\n    Can also supply audio for teacher forcing, \n    to test consistency with the training code or implement prompting\n    Args:\n        inputs: Text Encoder outputs.\n        stop: use stop gate\n        max_steps: stop after this many decoder steps\n        temperature: for the sampling distribution\n        alignments: forced alignment\n        audio: forced last-frame\n            (will be offset, you don't need to prepend a start frame)\n    Shapes:\n        - inputs: (B, T_text, D_text)\n        - outputs: (B, T_audio, D_audio)\n        - alignments: (B, T_text, T_audio)\n        - audio: (B, T_audio, D_audio)\n        - stop_tokens: (B, T_audio)\n    \"\"\"\n    # max_steps = max_steps or self.max_decoder_steps\n    max_steps = max_steps if max_steps is not None else self.max_decoder_steps\n\n    self.reset(inputs)\n\n    outputs = []\n    if alignments is None:\n        alignments = []\n        feed_alignment = False\n    else:\n        feed_alignment = True\n\n    for i in range(alignments.shape[-1]) if feed_alignment else it.count():\n        alignment = alignments[:,:,i] if feed_alignment else None\n        audio_frame = (\n            audio[:,i-1,:] \n            if i&gt;0 and audio is not None and i-1&lt;audio.shape[1] \n            else None)\n        r = self.step(\n            temperature=temperature, alignment=alignment, audio_frame=audio_frame)\n        outputs.append(r['output'])\n        if not feed_alignment:\n            alignments.append(r['alignment'])\n\n        if stop and r['stop_prob']&gt;self.stop_threshold:\n            break\n        if len(outputs) == max_steps:\n            if stop:\n                print(f\"   &gt; Decoder stopped with {max_steps=}\")\n            break\n\n    outputs = torch.stack(outputs, 1)\n    if not feed_alignment:\n        alignments = torch.stack(alignments, 1)\n\n    return outputs, alignments\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.reset","title":"<code>reset(inputs)</code>","text":"<p>populates buffers with initial states and text inputs</p> <p>call with encoded text, before using <code>step</code></p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>(B, T_text, D_text)</p> required Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef reset(self, inputs):\n    r\"\"\"\n    populates buffers with initial states and text inputs\n\n    call with encoded text, before using `step`\n\n    Args:\n        inputs: (B, T_text, D_text)\n\n    \"\"\"\n    self.core.reset(inputs)\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.step","title":"<code>step(alignment=None, audio_frame=None, temperature=1.0)</code>","text":"<p>single step of inference.</p> <p>optionally supply <code>alignment</code> to force the alignments. optionally supply <code>audio_frame</code> to set the previous frame.</p> <p>Parameters:</p> Name Type Description Default <code>alignment</code> <code>Optional[Tensor]</code> <p>B x T_text</p> <code>None</code> <code>audio_frame</code> <code>Optional[Tensor]</code> <p>B x D_audio</p> <code>None</code> <code>temperature</code> <code>float</code> <p>optional sampling temperature</p> <code>1.0</code> <p>Returns:     output: B x D_audio     alignment: B x T_text     stop_token: None (not implemented)</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.export\ndef step(self, \n        alignment:Optional[Tensor]=None,\n        audio_frame:Optional[Tensor]=None, \n        temperature:float=1.0\n    ):\n    r\"\"\"\n    single step of inference.\n\n    optionally supply `alignment` to force the alignments.\n    optionally supply `audio_frame` to set the previous frame.\n\n    Args:\n        alignment: B x T_text\n        audio_frame: B x D_audio\n        temperature: optional sampling temperature\n    Returns:\n        output: B x D_audio\n        alignment: B x T_text\n        stop_token: None (not implemented)\n    \"\"\"\n    alignment, output_params = self.core.step_pre(alignment, audio_frame)\n    decoder_output = self.likelihood.sample(\n        output_params, temperature=temperature).squeeze(1)\n\n    decoder_output = self.core.step_post(alignment, decoder_output)\n\n    # if debug:\n    return dict(\n        output=decoder_output,\n        alignment=alignment,\n        stop_prob=torch.tensor(0.),\n        params=output_params\n    )\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.update_kw_dict","title":"<code>update_kw_dict(d)</code>  <code>classmethod</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@classmethod\ndef update_kw_dict(cls, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    b = d.pop('text_bottleneck', None)\n    if b is not None:\n        if d['text_encoder'] is None:\n            d['text_encoder'] = {}\n        d['text_encoder']['bottleneck'] = b\n    return d\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.TacotronDecoder.update_state_dict","title":"<code>update_state_dict(d)</code>","text":"<p>backward compatibility with older checkpoints</p> Source code in <code>src/tungnaa/model.py</code> <pre><code>@torch.jit.ignore\ndef update_state_dict(self, d):\n    \"\"\"backward compatibility with older checkpoints\"\"\"\n    # TODO: core.\n    def replace(old, new):\n        t = d.pop(old, None)\n        if t is not None:\n            d[new] = t\n    replace('go_attention_rnn_cell_state', 'go_attention_cell')\n    replace('go_query', 'go_attention_hidden')\n    # rnncell -&gt; dropoutrnn\n    replace('attention_rnn.weight_hh', 'attention_rnn.rnn.weight_hh')\n    replace('attention_rnn.weight_ih', 'attention_rnn.rnn.weight_ih')\n    replace('attention_rnn.bias_hh', 'attention_rnn.rnn.bias_hh')\n    replace('attention_rnn.bias_ih', 'attention_rnn.rnn.bias_ih')\n    # dropoutrnn -&gt; residualrnn\n    replace('attention_rnn.rnn.weight_hh', 'core.attention_rnn.net.0.rnn.weight_hh')\n    replace('attention_rnn.rnn.weight_ih', 'core.attention_rnn.net.0.rnn.weight_ih')\n    replace('attention_rnn.rnn.bias_hh', 'core.attention_rnn.net.0.rnn.bias_hh')\n    replace('attention_rnn.rnn.bias_ih','core.attention_rnn.net.0.rnn.bias_ih')\n    for name in (\n        'go_attention_hidden', 'go_attention_cell',\n        'attention_hidden', 'attention_cell'\n        ):\n        if name in d and d[name].ndim==2:\n            d[name] = d[name][None]\n    # move into core\n    for name in list(d):\n        if any(name.startswith(s) for s in (\n            \"go_\", \"memory\", \"context\", \"attention_\", \"decoder_\", \"alignment\", \"inputs\", \"prenet.\", \"attention.\", \"decoder_rnn.\", \"linear_projection.\", \"stopnet.\")):\n            replace(name, f'core.{name}')\n\n    # text bottleneck -&gt; text encoder\n    replace('core.text_proj.weight', 'text_encoder.proj.weight')\n    replace('core.text_proj.bias', 'text_encoder.proj.bias')\n    return d\n</code></pre>"},{"location":"reference/tungnaa/gui/__init__/#tungnaa.gui.lev","title":"<code>lev(t1, t2)</code>  <code>cached</code>","text":"<p>Returns:</p> Type Description <p>levenshtein distance between t1 and t2</p> <p>string representing edits from t1 to t2 - delete + insert ^ edit . no change</p> <p>list giving the corresponding index in t2 for each position in t1</p> Source code in <code>src/tungnaa/text.py</code> <pre><code>@ft.cache\ndef lev(t1, t2):\n    \"\"\"\n    Returns:\n        levenshtein distance between t1 and t2\n        string representing edits from t1 to t2\n            - delete\n            + insert\n            ^ edit\n            . no change\n        list giving the corresponding index in t2 for each position in t1\n    \"\"\"\n    # print(t1, t2)\n    if len(t1)==0:\n        return len(t2), '+'*len(t2), []\n    elif len(t2)==0:\n        return len(t1), '-'*len(t1), [0]*len(t1)\n\n    hd1, tl1 = t1[0], t1[1:]\n    hd2, tl2 = t2[0], t2[1:]\n\n    if hd1==hd2:\n        n, s, i = lev(tl1, tl2)\n        return n, '.'+s, [0]+[j+1 for j in i]\n    (n, s, i), c, f = min(\n        (lev(tl1, tl2), '^', lambda i: [0]+[j+1 for j in i]), \n        (lev(tl1, t2), '-', lambda i: [0]+i), \n        (lev(t1, tl2), '+', lambda i: [j+1 for j in i]))\n    return n+1, c+s, f(i)\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/","title":"Backend","text":""},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend","title":"<code>Backend</code>","text":"Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>class Backend:\n    def __init__(self,\n        checkpoint:str,\n        rave_path:str|None=None,\n        audio_in:str|None=None,\n        audio_out:str|int=None,\n        audio_block:int|None=None,\n        audio_channels:int|None=None,\n        # sample_rate:int=None,\n        synth_audio:bool|None=None,\n        latent_audio:bool|None=None,\n        latent_osc:bool|None=None,\n        osc_sender=None,\n        buffer_frames:int=1,\n        profile:bool=True,\n        jit:bool=False,\n        max_model_state_storage:int=512,\n        ):\n        \"\"\"\n        Args:\n            checkpoint: path to tungnaa checkpoint file\n            rave_path: path to rave vocoder to use python sound\n            audio_in: sounddevice input name/index if using python audio\n            audio_out: sounddevice output name/index if using python audio\n            audio_block: block size if using python audio\n            synth_audio: if True, vocode in python and send stereo audio\n            latent_audio: if True, pack latents into a mono audio channel\n            latent_osc: if True, send latents over OSC\n            buffer_frames: process ahead this many model steps\n            profile: if True, print performance profiling info\n            jit: if True, compile tungnaa model with torchscript\n        \"\"\"\n        self.jit = jit\n        self.profile = Profiler(profile)\n        # default output modes\n        if synth_audio is None:\n            synth_audio = rave_path is not None\n        if latent_audio is None:\n            latent_audio = rave_path is None\n        if latent_osc is None:\n            latent_osc = False\n\n        self.reset_values = {}\n\n        out_mode = OutMode(0)\n        if synth_audio: out_mode |= OutMode.SYNTH_AUDIO\n        if latent_audio: out_mode |= OutMode.LATENT_AUDIO\n        if latent_osc: out_mode |= OutMode.LATENT_OSC\n        self.out_mode = out_mode\n        print(f'{self.out_mode=}')\n\n        if audio_channels is None:\n            audio_channels = 0\n            if OutMode.SYNTH_AUDIO in self.out_mode:\n                self.synth_channels = (0,1)\n                audio_channels += 2\n            else:\n                self.synth_channels = tuple()\n            if OutMode.LATENT_AUDIO in self.out_mode:\n                self.latent_channels = (audio_channels,)\n                audio_channels += 1\n            else:\n                self.latent_channels = tuple()\n        else:\n            raise NotImplementedError(\n                \"setting audio_channels not currently supported\")\n        print(f'{audio_channels=}')\n\n        self.step_mode = StepMode.PAUSE\n        self.generate_stop_at_end = False\n        self.sampler_stop_at_end = False\n\n        self.latent_biases = []\n\n        # self.temperature = 0.5\n        self.temperature = 1.\n\n        # generation\n        self.gen_loop_start = 0\n        self.gen_loop_end = None\n        # sampler\n        self.sampler_loop_start = 0\n        self.sampler_loop_end = None\n        # self.sampler_utterance = -1\n        self.sampler_step = 0\n\n        self.osc_sender = osc_sender\n\n        self.frontend_conn = None\n\n        self.max_model_state_storage = max_model_state_storage\n\n        ### move heavy init out of __init__, so it only runs in child process\n        ### (Backend.run is called by Proxy._run)\n        self.init_args = (buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels)\n\n        # call this from both __init__ and run\n        # torch.multiprocessing.set_sharing_strategy('file_system')\n\n\n    def run(self, conn):\n        \"\"\"run method expected by Proxy\"\"\"\n        # call this from both __init__ and run\n        # torch.multiprocessing.set_sharing_strategy('file_system')\n\n        self.frontend_conn = conn\n        # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n        buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels = self.init_args\n\n        self.load_tts_model(checkpoint=checkpoint)\n\n        ### synthesis in python:\n        # load RAVE model\n        self.load_vocoder_model(rave_path=rave_path)\n\n        ### audio output:\n        # make sounddevice stream\n        if self.out_mode:\n            devicelist = sd.query_devices()\n\n            # None throws an error on linux, on mac it uses the default device\n            # Let's make this behavior explicit.\n            if audio_out is None:\n                audio_out = sd.default.device[1]\n\n            audio_device = None\n            for dev in devicelist:\n                if audio_out in [dev['index'], dev['name']]:\n                    audio_device = dev\n                    # audio_device_sr = dev['default_samplerate']\n                print(f\"{dev['index']}: '{dev['name']}' {dev['hostapi']} (I/O {dev['max_input_channels']}/{dev['max_input_channels']}) (SR: {dev['default_samplerate']})\")\n\n            # this should not be an error\n            # None uses the default device on macOS\n            # if audio_device is None:\n                # raise RuntimeError(f\"Audio device '{audio_out}' does not exist.\")\n\n            print(f\"USING AUDIO OUTPUT DEVICE {audio_out}:{audio_device}\")\n\n            self.active_frame:torch.Tensor = None \n            self.future_frame:Thread = None\n            self.frame_counter:int = 0\n\n            sd.default.device = audio_out\n            if self.rave_sr:\n                if audio_device and audio_device['default_samplerate'] != self.rave_sr:\n                    # this should not be an error. On OSX/CoreAudio you can set the device sample rate to the model sample rate.\n                    # however on Linux/JACK this throws a fatal error and stops program execution, requiring you to restart Jack to change the sampling rate\n\n                    # TODO: also check RAVE block size vs. audio device block size if possible\n                    print(\"\\n------------------------------------\");\n                    print(f\"WARNING: Device default sample rate ({audio_device['default_samplerate']}) and RAVE model sample rate ({self.rave_sr}) mismatch! You may need to change device sample rate manually on some platforms.\")\n                    print(\"------------------------------------\\n\");\n\n                sd.default.samplerate = self.rave_sr # could cause an error if device uses a different sr from model\n                print(f\"RAVE SAMPLING RATE: {self.rave_sr}\")\n                print(f\"DEVICE SAMPLING RATE: {audio_device['default_samplerate']}\")\n\n            # TODO: Tungnaa only uses audio output. Shouldn't we always be using sd.OutputStream?\n            try:\n                assert len(audio_out)==2\n                stream_cls = sd.Stream\n            except Exception:\n                stream_cls = sd.OutputStream\n\n            self.stream = stream_cls(\n                callback=self.audio_callback,\n                samplerate=self.rave_sr, \n                blocksize=audio_block, \n                #device=(audio_in, audio_out)\n                device=audio_out,\n                channels=audio_channels\n            )\n\n            if self.rave_sr:\n                assert self.stream.samplerate == self.rave_sr, f\"\"\"\n                failed to set sample rate to {self.rave_sr} from sounddevice\n                \"\"\"\n        else:\n            self.stream = None\n\n        self.text = None\n        self.text_rep = None\n        self.align_params = None\n        self.momentary_align_params = None\n        self.step_to_set = None\n\n        self.one_shot_paint = False\n        self.latent_feedback = False\n\n        self.lock = RLock()\n        self.text_update_thread = None\n\n        # self.frontend_q = Queue()\n        self.audio_q = Queue(buffer_frames)     \n        self.trigger_q = Queue(buffer_frames)     \n\n        self.states = []\n\n        self.needs_reset = False\n\n        self.step_thread = Thread(target=self.step_loop, daemon=True)\n        self.step_thread.start()\n\n    def step_loop(self):\n        \"\"\"model stepping thread\n\n        for each timestamp received in trigger_q\n        send audio frames in audio_q\n        \"\"\"\n        while True:\n            t = self.trigger_q.get()\n            with self.profile('step'):\n                frame = self.step(t)\n            if frame is not None:\n                # with self.profile('frame.numpy'):\n                frame = frame.numpy()\n            self.audio_q.put(frame)\n\n    def load_tts_model(self, checkpoint):\n        \"\"\"helper for loading TTS model, called on initialization but can also be called from GUI\"\"\"\n        self.model = TacotronDecoder.from_checkpoint(checkpoint)\n\n        # not scripting text encoder for now\n        # if self.model.text_encoder is None:\n            # self.text_model = TextEncoder()\n        # else:\n        self.text_model = self.model.text_encoder\n        self.model.text_encoder = None\n\n        # def _debug(m):\n        #     # print({(k,type(v)) for k,v in m.__dict__.items()})\n        #     for k,v in m.__dict__.items():\n        #         # print('ATTR', k)\n        #         if 'tensor' in str(type(v)).lower(): \n        #             print('TENSOR', k)\n        #     for m_ in m.modules():\n        #         if m_ != m:\n        #             # print('MODULE', m_)\n        #             _debug(m_)\n        # _debug(self.model)\n\n        if self.jit:\n            for m in self.model.modules():\n                if hasattr(m, 'parametrizations'):\n                    torch.nn.utils.parametrize.remove_parametrizations(\n                        m,'weight')\n            self.model = torch.jit.script(self.model)\n        self.model.eval()\n        # self.model.train()\n\n        print(f'{self.num_latents=}')\n\n        # print(f'{self.model.frame_channels=}')\n        self.use_pitch = hasattr(self.model, 'pitch_xform') and self.model.pitch_xform\n\n    def load_vocoder_model(self, rave_path):\n        \"\"\"helper for loading RAVE vocoder model, called on initialization but can also be called from GUI\"\"\"\n        # TODO: The audio engine sampling rate gets set depending on the sampling rate from the vocoder. \n        #   When loading a new vocoder model we need to add some logic to make sure the new vocoder has the same sample rate as the audio system.\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            assert rave_path is not None\n            self.rave = torch.jit.load(rave_path, map_location='cpu')\n            self.rave.eval()\n            self.block_size = int(self.rave.decode_params[1])\n            try:\n                self.rave_sr = int(self.rave.sampling_rate)\n            except Exception:\n                self.rave_sr = int(self.rave.sr)\n            with torch.inference_mode():\n                # warmup\n                if hasattr(self.rave, 'full_latent_size'):\n                    latent_size = self.rave.latent_size + int(\n                        hasattr(self.rave, 'pitch_encoder'))\n                else:\n                    latent_size = self.rave.cropped_latent_size\n                self.rave.decode(torch.zeros(1,latent_size,1))\n        else:\n            self.rave = None\n            self.rave_sr = None\n\n    @property\n    def num_latents(self):\n        return self.model.frame_channels\n\n    def start_stream(self):\n        \"\"\"helper for start/sampler\"\"\"\n        # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.LATENT_OSC):\n        #     if not self.loop_thread.is_alive():\n        #         self.run_thread = True\n        #         self.loop_thread.start()\n        # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.SYNTH_AUDIO):\n        # if self.out_mode is not None:\n        if not self.stream.active:\n            self.stream.start()\n\n    def generate(self):\n        \"\"\"\n        start autoregressive alignment &amp; latent frame generation\n        \"\"\"\n        # if self.step_mode != StepMode.GENERATION:\n            # self.needs_reset = True\n        self.step_mode = StepMode.GENERATION\n        self.start_stream()\n\n    def pause(self):\n        \"\"\"\n        pause generation or sampler\n        \"\"\"\n        self.step_mode = StepMode.PAUSE\n\n    def sampler(self):\n        \"\"\"\n        start sampler mode\n        \"\"\"\n        self.step_mode = StepMode.SAMPLER\n        self.start_stream()\n\n    def reset(self):\n        \"\"\"\n        reset the model state and alignments history\n        \"\"\"\n        self.needs_reset = True\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup any resources\n        \"\"\"\n        self.step_mode = StepMode.PAUSE\n        self.run_thread = False\n        # should probably do this more gracefully\n        exit(0) # exit the backend process\n\n    def set_text(self, text:str) -&gt; int:\n        \"\"\"\n        Compute embeddings for &amp; store a new text, replacing the old text.\n        Returns the number of embedding tokens\n\n        Args:\n            text: input text as a string\n\n        Returns:\n            length of text in tokens\n        \"\"\"\n        if (\n            self.text_update_thread is not None \n            and self.text_update_thread.is_alive()\n        ):\n            print('warning: text update still pending')\n\n        # TODO: more general text preprocessing\n        text, start, end = self.extract_loop_points(text)\n        tokens, text, idx_map = self.text_model.tokenize(text)\n        # print(start, end, idx_map)\n        # NOTE: positions in text may change fron tokenization (end tokens)\n        if start &lt; len(idx_map):\n            start = idx_map[start]\n        else:\n            start = 0\n        if end is not None and end &lt; len(idx_map):\n            end = idx_map[end]\n        else:\n            end = None\n\n        # text processing runs in its own thread\n        self.text_update_thread = Thread(\n            target=self._update_text, \n            args=(text, tokens, start, end), daemon=True)\n        self.text_update_thread.start()\n\n        return text\n\n    def extract_loop_points(self, text, tokens='&lt;&gt;'):\n        \"\"\"helper for `set_text`\"\"\"\n        start_tok, end_tok = tokens\n        # TODO: could look for matched brackets, have multiple loops...\n        start = text.find(start_tok) # -1 if not found\n        # if not len(text):\n        #     return text, None, None\n        if start &lt; 0:\n            start = 0\n        else:\n            text = text[:start]+text[start+1:]\n        end = text.find(end_tok) # -1 if not found\n        if end &lt; 0: \n            end = None\n        else:\n            text = text[:end]+text[end+1:]\n            end = max(0, end - 1)\n        # print(text, end)\n        return text, start, end\n\n    def _update_text(self, text, tokens, start, end):\n        \"\"\"runs in a thread\"\"\"\n        # store the length of the common prefix between old/new text\n        # if self.text is None:\n        #     self.prefix_len = 0\n        # else:\n        #     for i,(a,b) in enumerate(zip(text, self.text)):\n        #         print(i, a, b)\n        #         if a!=b: break\n        #     self.prefix_len = i\n\n        # store a mapping from old text positions to new\n        if self.text is None:\n            text_index_map = lambda x:x\n        else:\n            _,_,tm = lev(self.text, text)\n            text_index_map = lambda x: tm[x] if x &lt; len(tm) else len(text)-1\n\n        # lock should keep multiple threads from trying to run the text encoder\n        # at once in case `input_text` is called rapidly\n        with self.lock:\n            with torch.inference_mode():\n                self.reset_values = dict(\n                    text_rep = self.text_model.encode(tokens),\n                    text_index_map = text_index_map,\n                    text_tokens = tokens,\n                    text = text,\n                    gen_loop_start = start,\n                    gen_loop_end = end,\n                )\n            self.reset()\n\n    def set_biases(self, biases:List[float]):\n        self.latent_biases = biases\n\n    def set_alignment(self, \n            align_params:Optional[Tuple[float, float]]\n        ) -&gt; None:\n        \"\"\"\n        Send alignment parameters to the backend. If None is passed, alignment painting is off.\n\n        Args:\n            align_params: [loc, scale] or None\n        \"\"\"\n        self.align_params = align_params\n\n    def set_momentary_alignment(self,             \n            align_params:Optional[Tuple[float, float]]\n        ) -&gt; None:\n        \"\"\"\n        Alignment will be set on the next frame only\n        \"\"\"\n        self.momentary_align_params = align_params\n\n    def set_state_by_step(self, step):\n        self.step_to_set = step\n\n    def set_latent_feedback(self, b:bool):\n       self.latent_feedback = b\n\n    def set_generate_stop_at_end(self, b:bool):\n        self.generate_stop_at_end = b\n\n    def set_sampler_stop_at_end(self, b:bool):\n        self.sampler_stop_at_end = b\n\n    def set_sampler_step(self, step:int):\n        self.sampler_step = step % self.total_steps()\n\n    def set_sampler_loop_index(self, \n            start:int=None, end:int=None, \n            utterance:int=None, \n            reset:bool=True):\n        \"\"\"\n        Args:\n            start: loop start step\n            end: loop end step\n            utterance: sampler utterance\n            reset: if True, immediately go to loop start\n        \"\"\"\n        if len(self.states)==0: return\n\n        def wrap(x, n, tag):\n            if x is None: return x\n            if x &lt; 0: \n                x += n\n            if x &lt; 0 or x &gt;= n:\n                print(f'warning: out of bounds {tag} {x}')\n                x %= n\n            return x\n\n        if utterance is not None:\n            if start is not None:\n                start = self.utterance_to_global_step(utterance, start)\n            if end is not None:\n                end = self.utterance_to_global_step(utterance, end)\n\n        if start is None: start = self.sampler_loop_start        \n        if end is None: end = self.sampler_loop_end\n\n        # utterance = wrap(utterance, len(self.states), 'utterance')\n        start = wrap(start, len(self.states[utterance]), 'loop start')\n        end = wrap(end, len(self.states[utterance]), 'loop end')\n\n        # changed = utterance != self.sampler_utterance\n        # self.sampler_utterance = utterance\n\n        self.sampler_loop_start = start\n        self.sampler_loop_end = end\n\n        # must reset if changing utterance\n        if reset:\n            self.reset_sampler()\n\n    def set_sampler_loop_text(self, \n            text:str, n:int=-1, \n            start:bool=True, end:bool=True, \n            reset:bool=True):\n        \"\"\"\n        Args:\n            text: a regular expression string\n            n: index of occurrence in history\n            start: if True, set the loop start\n            end: if True, set the loop end\n            reset: if True, immediately go to loop start\n        \"\"\"\n        if not (start or end): return\n        r = text\n\n        # get all occurences of all matching strings,\n        #   and index by order in history\n\n        # first find matches for the regex in utterance texts\n        matches = []\n        utts = self.states #if n&gt;=0 else reversed(self.states)\n        index = 0\n        for utt_index, utt in enumerate(utts):\n            if len(utt)==0: continue\n            # text = utt[0]['text']\n            text = utt.text\n            for match in re.findall(r, text):\n                # error if multiple capture groups in regex\n                if not isinstance(match, str):\n                    raise ValueError(\"`set_sampler_loop`: multiple capture groups not supported\")\n                # add utterance, text index of match\n                index = index + text[index:].find(match)\n                matches.append((utt_index, index, index+len(match), match))\n                # print(f'{utt_index=}, {index=}, {match=}')\n\n        if len(matches)==0:\n            print(f'warning: no text matching \"{r}\" in `set_sampler_loop`')\n            return\n\n        # for matches, get occurences and flatten into list\n        #   simple: occurrence is from entering start to exiting end\n        #   TODO better: occurence is when there is a run which\n        #       enters the first token / leaves the last token\n        #       without any major jumps or reversals\n        #   TODO?: allow skips of first / last token?\n        occurrences = []\n        for u, i, j, m in matches:\n            occ_start = None\n            utt = self.states[u]\n            for k, state in enumerate(utt):\n                align = state['align_hard']\n                # print(align)\n\n                if align['enter_index']==i:\n                    # open/replace occurrence\n                    if occ_start is not None:\n                        print(f'warning: double open {m=} {occ_start=} {u=} {k=}')\n                    occ_start = k\n                if align['leave_index']==j:\n                    # close occurrence (or ignore)\n                    if occ_start is None: \n                        print(f'warning: close without open {m=}')\n                        continue\n                    occurrences.append((u, occ_start, k))\n                    # print(f'{(u, occ_start, k)=}')\n                    occ_start = None\n\n        if len(occurrences)==0:\n            print(f'warning: no occurrences of {matches} in `set_sampler_loop`')\n            return\n\n        # index into list using n\n        u, loop_start, loop_end = occurrences[n]\n\n        loop_start = self.utterance_to_global_step(u, loop_start)\n        loop_end = self.utterance_to_global_step(u, loop_end)\n        # TODO: efficient search for index of occurence\n\n        # set loop points\n        # if utterance changes, unset points which aren't set\n        # changed = u != self.sampler_utterance\n        # self.sampler_utterance = u\n\n        if start:\n            self.sampler_loop_start = loop_start\n        # elif changed:\n            # self.sampler_loop_start = 0\n\n        if end:\n            self.sampler_loop_end = loop_end\n        # elif changed:\n            # self.sampler_loop_end = None\n\n        # must reset if changing utterance\n        # if changed or reset:\n        if reset:\n            self.reset_sampler()\n\n        # print(f'{self.sampler_utterance=}, {self.sampler_step=}, {self.sampler_loop_start=}, {self.sampler_loop_end=}')\n\n\n    def reset_sampler(self):\n        self.step_mode = StepMode.SAMPLER\n        self.sampler_step = self.sampler_loop_start\n\n\n    def process_frame(self, \n            latent_t:Optional[Tensor]=None, \n            align_t:Optional[Tensor]=None\n        ) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Generate an audio(rave latents) and alignment frame.\n\n        Args:\n            latent_t: [batch, RAVE latent size]\n                last frame of audio feature if using `latent feedback` mode\n            align_t: [batch, text length in tokens]\n                explicit alignments if using `paint` mode\n        Returns:\n            latent_t: [batch, RAVE latent size]\n                next frame of audio feature\n            align_t: [batch, text length in tokens]\n                alignments to text\n        \"\"\"\n        with torch.inference_mode():\n            with self.profile('tts', detail=False):\n                r = self.model.step(\n                    alignment=align_t, audio_frame=latent_t, \n                    temperature=self.temperature)\n                latent_t, align_t, \n        # use low precision for storage\n        return r['output'].half(), r['alignment'].half()\n\n    def set_temperature(self, t):\n        self.temperature = t\n\n    def do_audio_block(self, outdata, sdtime):\n        \"\"\"loop over samples of output, requesting frames from the audio thread\n        and pulling them from the queue as needed\n        \"\"\"\n        outdata[:,:] = 0\n        c = self.synth_channels+self.latent_channels\n        for i in range(outdata.shape[0]):\n            # if the current frame is exhausted, delete it\n            if self.active_frame is not None:\n                if self.frame_counter &gt;= self.active_frame.shape[-1]:\n                    self.active_frame = None\n                    self.frame_counter = 0\n\n            # if no active frame, try to get one from step thread\n            if self.active_frame is None:\n                if not self.audio_q.empty(): \n                    self.active_frame = self.audio_q.get()\n\n            if not self.trigger_q.full():\n                # use ADC input time as timestamp\n                timestamp = sdtime.inputBufferAdcTime\n                self.trigger_q.put(timestamp)\n\n            if self.active_frame is None:\n                if self.playing:\n                    print(f'audio: dropped frame')\n                return\n            else:\n                # read next audio sample out of active model frame \n                # TODO: batch/multichannel handling\n                outdata[i,c] = self.active_frame[:, self.frame_counter]\n                self.frame_counter += 1\n\n    def audio_callback(self,*a):\n        \"\"\"sounddevice callback main loop\"\"\"\n        if len(a)==4: # output device only case\n            (\n                outdata,#: np.ndarray, #[frames x channels]\n                frames, sdtime, status\n            ) = a\n        elif len(a)==5: # input and output device\n            (\n                indata, outdata, #np.ndarray, #[frames x channels]\n                frames, sdtime, status\n            ) = a\n\n        self.do_audio_block(outdata, sdtime)\n\n    def utterance_empty(self, utterance=-1):\n        try:\n            return len(self.states[utterance])==0\n        except IndexError:\n            return True\n\n    def utterance_len(self, utterance=-1):\n        return len(self.states[utterance])\n\n    def global_step_to_utterance(self, global_step):\n        utterance = 0\n        step = global_step\n        while True:\n            if utterance &gt;= len(self.states):\n                raise ValueError('warning: global_step out of bounds')\n            ul = len(self.states[utterance])\n            if step &lt; ul:\n                break\n            step -= ul\n            utterance += 1\n\n        return utterance, step\n\n    def utterance_to_global_step(self, utterance, step):\n        if utterance &gt;= len(self.states):\n            raise ValueError('warning: utterance out of bounds')\n        return step + sum(len(self.states[i]) for i in range(utterance-1))\n\n    def prev_state(self, *a, step=-1, utterance=-1, global_step=None):\n        \"\"\"convenient access to previous states\"\"\"\n        if global_step is not None:\n            try:\n                utterance, step = self.global_step_to_utterance(global_step)\n            except ValueError:\n                return None\n\n        s = self.states[utterance][step]\n        for k in a:\n            s = s[k]\n        return s\n\n    def total_steps(self):\n        return sum(len(s) for s in self.states)\n\n    def hard_alignment(self, align_t):\n        \"\"\"return index and character of hard alignment\"\"\"\n        i = align_t.argmax().item()\n        # print(i)\n        c = self.text[i] if i &lt; len(self.text) else None\n\n        i_enter = None\n        i_leave = None\n        c_enter = None\n        c_leave = None\n        if not self.utterance_empty():\n            # print(self.states)\n            prev_align = self.prev_state('align_hard')\n\n            if i&gt;prev_align['index']:\n                i_enter = i\n                i_leave = prev_align['index']\n                c_enter = c\n                c_leave = prev_align['char']\n\n        return {\n            'index':i, \n            'char':c, \n            'enter_index':i_enter,\n            'leave_index':i_leave,\n            'enter_char':c_enter,\n            'leave_char':c_leave\n        }\n\n    def do_reset(self):\n        \"\"\"perform reset of model states/text (called from `step`)\"\"\"\n        print('RESET')\n        for k,v in self.reset_values.items():\n            setattr(self, k, v)\n\n        if self.text_rep is not None:\n            self.model.reset(self.text_rep)\n            if self.align_params is None:\n                # go to loop start after reset\n                self.momentary_align_params = (self.gen_loop_start, 1)\n            elif not self.utterance_empty():\n                # unless painting alignments -- then try to stay\n                # in approximately the same spot\n                # TODO this doesn't work because the frontend just sets it again based on the slider -- need to add bidirectional control\n                self.align_params = (\n                    self.text_index_map(round(self.align_params[0])), 1)\n\n            # remove old RNN states\n            if len(self.states):\n                for state in self.states[-1]:\n                    self.strip_states(state)\n\n            # start a new utterance\n            self.states.append(Utterance(self.text))\n            ## for now, only sampler of current utterance is supported\n            self.sampler_step = 0\n            # in the future, it might be useful to encode texts without yet starting a new utterance. for now, it makes more sense if hitting encode \n            # always starts generation\n            self.generate()\n\n            self.needs_reset = False\n\n\n        return {\n            'reset': True, \n            'text': self.text, \n            'num_latents': self.num_latents, \n            'use_pitch': self.use_pitch\n            }\n\n    def paint_alignment(self):\n        \"\"\"helper for `step_gen`\"\"\"\n        align_params = self.momentary_align_params or self.align_params\n        self.momentary_align_params = None\n        if align_params is None:\n            return None\n        loc, scale = align_params\n        n_tokens = self.text_rep.shape[1]\n        loc = max(0, min(n_tokens, loc))\n        deltas = torch.arange(n_tokens) - loc\n        deltas = deltas / (0.5 + scale) # sharpness modulated by speed \n        # discrete gaussian, sums exactly to 1 over text positions\n        logits = -deltas**2\n        res = logits.exp() \n        res = res / res.sum()\n        return res[None]\n\n\n    def convert_numpy(self, item):\n        if hasattr(item, 'numpy'):\n            return item.numpy()\n        # elif isinstance(item, dict):\n            # return {k:self.convert_numpy(v) for k,v in item.items()}\n        # elif hasattr(item, '__len__'):\n            # return [self.convert_numpy(i) for i in item]\n        else:\n            return item\n\n    def send_state(self, state):   \n        state = {\n            k:self.convert_numpy(v)\n            for k,v in state.items()\n            if k!='model_state'\n        }\n        self.frontend_conn.send(state)\n\n    def step(self, timestamp):\n        \"\"\"compute one vocoder frame of generation or sampler\"\"\"\n        # if self.frontend_q.qsize() &gt; 100:\n            # self.frontend_q.get()\n            # print('frontend queue full, dropping')\n\n        state = {'text':self.text}\n\n        # reset text and model states\n        if self.needs_reset:\n            state |= self.do_reset()\n\n        # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n        if self.text_rep is None or self.step_mode==StepMode.PAUSE:\n            if len(state):\n                # self.frontend_q.put(state)\n                if self.frontend_conn is not None:\n                    self.send_state(state)\n                else:\n                    print('warning: frontend_conn does not exist')\n            return None\n        if self.step_mode==StepMode.GENERATION:\n            # with self.profile('step_gen'):\n            state |= self.step_gen(timestamp)\n        elif self.step_mode==StepMode.SAMPLER:\n            with self.profile('step_sampler'):\n                state |= self.step_sampler(timestamp)\n        # else:\n            # print(f'WARNING: {self.step_mode=} in step')\n\n        if self.osc_sender is not None:\n            self.osc_sender(state)\n\n        if len(self.latent_biases):\n            with torch.inference_mode():\n                bias = torch.tensor(self.latent_biases)[None]\n                state['latent_t'] += bias\n\n        # send to frontend\n        # self.frontend_q.put(state)\n        if self.frontend_conn is not None:\n            self.send_state(state)\n        else:\n            # print(state)\n            print('warning: frontend_conn does not exist')\n\n        latent = state['latent_t'] # batch, channel\n        batch, n_latent = latent.shape\n        assert batch==1\n\n        c_synth, c_latent = len(self.synth_channels), len(self.latent_channels)\n\n        with torch.inference_mode():\n            audio_frame = torch.zeros(\n                c_synth+c_latent, \n                self.model.block_size) # channel, time\n\n            if OutMode.SYNTH_AUDIO in self.out_mode:\n                # allow creative use of vocoders with mismatched sizes\n                rave_dim = self.rave.decode_params[0]\n                common_dim = min(rave_dim, latent.shape[-1])\n                latent_to_rave = torch.zeros(latent.shape[0], rave_dim)\n                latent_to_rave[:,:common_dim] = latent[:,:common_dim]\n                ### run the vocoder\n                with self.profile('vocoder'):\n                    audio = self.rave.decode(\n                        latent_to_rave[...,None])[0] # channel, time\n                ###\n                audio_frame[:c_synth, :] = audio\n\n            if OutMode.LATENT_AUDIO in self.out_mode:\n                latent = torch.cat((\n                    torch.zeros(batch, 1), # zero to ensure trigger\n                    torch.full((batch, 1), n_latent), # trigger, number of latents\n                    latent\n                    ), dim=1) / 1024 # scale down in case audio gets sent to speakers\n                # latent x batch\n\n                audio_frame[c_synth:, :2+n_latent] = latent\n\n        return audio_frame\n        # send to audio thread\n        # print(f'DEBUG: queuing audio {id(audio)}')\n        # self.audio_q.put(audio_frame)\n\n    def step_sampler(self, timestamp):\n        \"\"\"sampler branch of main `step` method\"\"\"\n        # print(f'{self.sampler_step=}')\n\n        # if self.utterance_empty(utterance=self.sampler_utterance):\n        if self.total_steps()==0:\n            print('nothing to play back')\n            self.step_mode = StepMode.PAUSE\n            return None\n\n        state = self.prev_state(global_step=self.sampler_step) or {}\n\n        self.sampler_step += 1\n\n        if (\n            self.sampler_step == self.sampler_loop_end \n            or self.sampler_step &gt;= self.total_steps()\n            ):\n            self.sampler_step = self.sampler_loop_start\n            if self.sampler_stop_at_end:\n                self.step_mode = StepMode.PAUSE\n\n\n        state = {**state}\n        state['latent_t'] = state['latent_t'].clone()\n        state['timestamp'] = timestamp\n        state['sampler'] = True\n\n        return state\n\n\n    def step_gen(self, timestamp):\n        \"\"\"generation branch of main `step` method\"\"\"\n        # loop end\n        if (\n            self.align_params is None \n            and self.gen_loop_end is not None \n            and not self.utterance_empty()\n            and self.prev_state('align_hard', 'index') in (\n                self.gen_loop_end, self.gen_loop_end+1)\n        ):\n            self.momentary_align_params = (self.gen_loop_start, 1)\n\n        # set just model states\n        # TODO: possibility to set to previous utterance?\n        if self.step_to_set is not None:\n            step = self.step_to_set\n            with torch.inference_mode():\n                try:\n                    self.model.set_state(\n                        self.prev_state('model_state', step=step))\n                except Exception:\n                    print(f'WARNING: failed to set {step=} ')\n                    raise\n            self.step_to_set = None\n\n        if self.model.memory is None:\n            print('skipping: model not initialized')\n            if self.rave is not None:\n                self.audio_q.put(None)\n            return None\n\n        if not self.utterance_empty() and self.latent_feedback:\n            # pass the last vocoder frame back in\n            latent_t = self.prev_state('latent_t')\n        else:\n            latent_t = None\n\n        align_t = self.paint_alignment()\n\n        ##### run the TTS model\n        latent_t, align_t = self.process_frame(\n            # self.mode, \n            latent_t=latent_t, align_t=align_t)\n        #####\n\n        # NOTE\n        ### EXPERIMENTAL juice knob\n        # latent_t = latent_t * 1.1\n        ###\n\n        state = {\n            # 'text':self.text,\n            'latent_t':latent_t, \n            'align_t':align_t,\n            'timestamp':timestamp,\n            'model_state':self.model.get_state(),\n            'align_hard':self.hard_alignment(align_t)\n            }\n\n        if (\n            self.generate_stop_at_end \n            and state['align_hard']['index']==(len(self.text)-1)\n        ):\n            self.step_mode = StepMode.PAUSE\n            # self.reset()\n\n        utt = self.states[-1]\n        utt.append(state)\n        # remove RNN states after certain number of steps\n        if len(utt) &gt;= self.max_model_state_storage:\n            self.strip_states(utt[-self.max_model_state_storage])\n        return state\n\n    def strip_states(self, state):\n        \"\"\"remove RNN states\"\"\"\n        state.pop('model_state', None)\n\n    @property\n    def playing(self):\n        return self.step_mode!=StepMode.PAUSE\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.__init__","title":"<code>__init__(checkpoint, rave_path=None, audio_in=None, audio_out=None, audio_block=None, audio_channels=None, synth_audio=None, latent_audio=None, latent_osc=None, osc_sender=None, buffer_frames=1, profile=True, jit=False, max_model_state_storage=512)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>str</code> <p>path to tungnaa checkpoint file</p> required <code>rave_path</code> <code>str | None</code> <p>path to rave vocoder to use python sound</p> <code>None</code> <code>audio_in</code> <code>str | None</code> <p>sounddevice input name/index if using python audio</p> <code>None</code> <code>audio_out</code> <code>str | int</code> <p>sounddevice output name/index if using python audio</p> <code>None</code> <code>audio_block</code> <code>int | None</code> <p>block size if using python audio</p> <code>None</code> <code>synth_audio</code> <code>bool | None</code> <p>if True, vocode in python and send stereo audio</p> <code>None</code> <code>latent_audio</code> <code>bool | None</code> <p>if True, pack latents into a mono audio channel</p> <code>None</code> <code>latent_osc</code> <code>bool | None</code> <p>if True, send latents over OSC</p> <code>None</code> <code>buffer_frames</code> <code>int</code> <p>process ahead this many model steps</p> <code>1</code> <code>profile</code> <code>bool</code> <p>if True, print performance profiling info</p> <code>True</code> <code>jit</code> <code>bool</code> <p>if True, compile tungnaa model with torchscript</p> <code>False</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def __init__(self,\n    checkpoint:str,\n    rave_path:str|None=None,\n    audio_in:str|None=None,\n    audio_out:str|int=None,\n    audio_block:int|None=None,\n    audio_channels:int|None=None,\n    # sample_rate:int=None,\n    synth_audio:bool|None=None,\n    latent_audio:bool|None=None,\n    latent_osc:bool|None=None,\n    osc_sender=None,\n    buffer_frames:int=1,\n    profile:bool=True,\n    jit:bool=False,\n    max_model_state_storage:int=512,\n    ):\n    \"\"\"\n    Args:\n        checkpoint: path to tungnaa checkpoint file\n        rave_path: path to rave vocoder to use python sound\n        audio_in: sounddevice input name/index if using python audio\n        audio_out: sounddevice output name/index if using python audio\n        audio_block: block size if using python audio\n        synth_audio: if True, vocode in python and send stereo audio\n        latent_audio: if True, pack latents into a mono audio channel\n        latent_osc: if True, send latents over OSC\n        buffer_frames: process ahead this many model steps\n        profile: if True, print performance profiling info\n        jit: if True, compile tungnaa model with torchscript\n    \"\"\"\n    self.jit = jit\n    self.profile = Profiler(profile)\n    # default output modes\n    if synth_audio is None:\n        synth_audio = rave_path is not None\n    if latent_audio is None:\n        latent_audio = rave_path is None\n    if latent_osc is None:\n        latent_osc = False\n\n    self.reset_values = {}\n\n    out_mode = OutMode(0)\n    if synth_audio: out_mode |= OutMode.SYNTH_AUDIO\n    if latent_audio: out_mode |= OutMode.LATENT_AUDIO\n    if latent_osc: out_mode |= OutMode.LATENT_OSC\n    self.out_mode = out_mode\n    print(f'{self.out_mode=}')\n\n    if audio_channels is None:\n        audio_channels = 0\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            self.synth_channels = (0,1)\n            audio_channels += 2\n        else:\n            self.synth_channels = tuple()\n        if OutMode.LATENT_AUDIO in self.out_mode:\n            self.latent_channels = (audio_channels,)\n            audio_channels += 1\n        else:\n            self.latent_channels = tuple()\n    else:\n        raise NotImplementedError(\n            \"setting audio_channels not currently supported\")\n    print(f'{audio_channels=}')\n\n    self.step_mode = StepMode.PAUSE\n    self.generate_stop_at_end = False\n    self.sampler_stop_at_end = False\n\n    self.latent_biases = []\n\n    # self.temperature = 0.5\n    self.temperature = 1.\n\n    # generation\n    self.gen_loop_start = 0\n    self.gen_loop_end = None\n    # sampler\n    self.sampler_loop_start = 0\n    self.sampler_loop_end = None\n    # self.sampler_utterance = -1\n    self.sampler_step = 0\n\n    self.osc_sender = osc_sender\n\n    self.frontend_conn = None\n\n    self.max_model_state_storage = max_model_state_storage\n\n    ### move heavy init out of __init__, so it only runs in child process\n    ### (Backend.run is called by Proxy._run)\n    self.init_args = (buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels)\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.audio_callback","title":"<code>audio_callback(*a)</code>","text":"<p>sounddevice callback main loop</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def audio_callback(self,*a):\n    \"\"\"sounddevice callback main loop\"\"\"\n    if len(a)==4: # output device only case\n        (\n            outdata,#: np.ndarray, #[frames x channels]\n            frames, sdtime, status\n        ) = a\n    elif len(a)==5: # input and output device\n        (\n            indata, outdata, #np.ndarray, #[frames x channels]\n            frames, sdtime, status\n        ) = a\n\n    self.do_audio_block(outdata, sdtime)\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup any resources</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup any resources\n    \"\"\"\n    self.step_mode = StepMode.PAUSE\n    self.run_thread = False\n    # should probably do this more gracefully\n    exit(0) # exit the backend process\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.do_audio_block","title":"<code>do_audio_block(outdata, sdtime)</code>","text":"<p>loop over samples of output, requesting frames from the audio thread and pulling them from the queue as needed</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def do_audio_block(self, outdata, sdtime):\n    \"\"\"loop over samples of output, requesting frames from the audio thread\n    and pulling them from the queue as needed\n    \"\"\"\n    outdata[:,:] = 0\n    c = self.synth_channels+self.latent_channels\n    for i in range(outdata.shape[0]):\n        # if the current frame is exhausted, delete it\n        if self.active_frame is not None:\n            if self.frame_counter &gt;= self.active_frame.shape[-1]:\n                self.active_frame = None\n                self.frame_counter = 0\n\n        # if no active frame, try to get one from step thread\n        if self.active_frame is None:\n            if not self.audio_q.empty(): \n                self.active_frame = self.audio_q.get()\n\n        if not self.trigger_q.full():\n            # use ADC input time as timestamp\n            timestamp = sdtime.inputBufferAdcTime\n            self.trigger_q.put(timestamp)\n\n        if self.active_frame is None:\n            if self.playing:\n                print(f'audio: dropped frame')\n            return\n        else:\n            # read next audio sample out of active model frame \n            # TODO: batch/multichannel handling\n            outdata[i,c] = self.active_frame[:, self.frame_counter]\n            self.frame_counter += 1\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.do_reset","title":"<code>do_reset()</code>","text":"<p>perform reset of model states/text (called from <code>step</code>)</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def do_reset(self):\n    \"\"\"perform reset of model states/text (called from `step`)\"\"\"\n    print('RESET')\n    for k,v in self.reset_values.items():\n        setattr(self, k, v)\n\n    if self.text_rep is not None:\n        self.model.reset(self.text_rep)\n        if self.align_params is None:\n            # go to loop start after reset\n            self.momentary_align_params = (self.gen_loop_start, 1)\n        elif not self.utterance_empty():\n            # unless painting alignments -- then try to stay\n            # in approximately the same spot\n            # TODO this doesn't work because the frontend just sets it again based on the slider -- need to add bidirectional control\n            self.align_params = (\n                self.text_index_map(round(self.align_params[0])), 1)\n\n        # remove old RNN states\n        if len(self.states):\n            for state in self.states[-1]:\n                self.strip_states(state)\n\n        # start a new utterance\n        self.states.append(Utterance(self.text))\n        ## for now, only sampler of current utterance is supported\n        self.sampler_step = 0\n        # in the future, it might be useful to encode texts without yet starting a new utterance. for now, it makes more sense if hitting encode \n        # always starts generation\n        self.generate()\n\n        self.needs_reset = False\n\n\n    return {\n        'reset': True, \n        'text': self.text, \n        'num_latents': self.num_latents, \n        'use_pitch': self.use_pitch\n        }\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.extract_loop_points","title":"<code>extract_loop_points(text, tokens='&lt;&gt;')</code>","text":"<p>helper for <code>set_text</code></p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def extract_loop_points(self, text, tokens='&lt;&gt;'):\n    \"\"\"helper for `set_text`\"\"\"\n    start_tok, end_tok = tokens\n    # TODO: could look for matched brackets, have multiple loops...\n    start = text.find(start_tok) # -1 if not found\n    # if not len(text):\n    #     return text, None, None\n    if start &lt; 0:\n        start = 0\n    else:\n        text = text[:start]+text[start+1:]\n    end = text.find(end_tok) # -1 if not found\n    if end &lt; 0: \n        end = None\n    else:\n        text = text[:end]+text[end+1:]\n        end = max(0, end - 1)\n    # print(text, end)\n    return text, start, end\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.generate","title":"<code>generate()</code>","text":"<p>start autoregressive alignment &amp; latent frame generation</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def generate(self):\n    \"\"\"\n    start autoregressive alignment &amp; latent frame generation\n    \"\"\"\n    # if self.step_mode != StepMode.GENERATION:\n        # self.needs_reset = True\n    self.step_mode = StepMode.GENERATION\n    self.start_stream()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.hard_alignment","title":"<code>hard_alignment(align_t)</code>","text":"<p>return index and character of hard alignment</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def hard_alignment(self, align_t):\n    \"\"\"return index and character of hard alignment\"\"\"\n    i = align_t.argmax().item()\n    # print(i)\n    c = self.text[i] if i &lt; len(self.text) else None\n\n    i_enter = None\n    i_leave = None\n    c_enter = None\n    c_leave = None\n    if not self.utterance_empty():\n        # print(self.states)\n        prev_align = self.prev_state('align_hard')\n\n        if i&gt;prev_align['index']:\n            i_enter = i\n            i_leave = prev_align['index']\n            c_enter = c\n            c_leave = prev_align['char']\n\n    return {\n        'index':i, \n        'char':c, \n        'enter_index':i_enter,\n        'leave_index':i_leave,\n        'enter_char':c_enter,\n        'leave_char':c_leave\n    }\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.load_tts_model","title":"<code>load_tts_model(checkpoint)</code>","text":"<p>helper for loading TTS model, called on initialization but can also be called from GUI</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def load_tts_model(self, checkpoint):\n    \"\"\"helper for loading TTS model, called on initialization but can also be called from GUI\"\"\"\n    self.model = TacotronDecoder.from_checkpoint(checkpoint)\n\n    # not scripting text encoder for now\n    # if self.model.text_encoder is None:\n        # self.text_model = TextEncoder()\n    # else:\n    self.text_model = self.model.text_encoder\n    self.model.text_encoder = None\n\n    # def _debug(m):\n    #     # print({(k,type(v)) for k,v in m.__dict__.items()})\n    #     for k,v in m.__dict__.items():\n    #         # print('ATTR', k)\n    #         if 'tensor' in str(type(v)).lower(): \n    #             print('TENSOR', k)\n    #     for m_ in m.modules():\n    #         if m_ != m:\n    #             # print('MODULE', m_)\n    #             _debug(m_)\n    # _debug(self.model)\n\n    if self.jit:\n        for m in self.model.modules():\n            if hasattr(m, 'parametrizations'):\n                torch.nn.utils.parametrize.remove_parametrizations(\n                    m,'weight')\n        self.model = torch.jit.script(self.model)\n    self.model.eval()\n    # self.model.train()\n\n    print(f'{self.num_latents=}')\n\n    # print(f'{self.model.frame_channels=}')\n    self.use_pitch = hasattr(self.model, 'pitch_xform') and self.model.pitch_xform\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.load_vocoder_model","title":"<code>load_vocoder_model(rave_path)</code>","text":"<p>helper for loading RAVE vocoder model, called on initialization but can also be called from GUI</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def load_vocoder_model(self, rave_path):\n    \"\"\"helper for loading RAVE vocoder model, called on initialization but can also be called from GUI\"\"\"\n    # TODO: The audio engine sampling rate gets set depending on the sampling rate from the vocoder. \n    #   When loading a new vocoder model we need to add some logic to make sure the new vocoder has the same sample rate as the audio system.\n    if OutMode.SYNTH_AUDIO in self.out_mode:\n        assert rave_path is not None\n        self.rave = torch.jit.load(rave_path, map_location='cpu')\n        self.rave.eval()\n        self.block_size = int(self.rave.decode_params[1])\n        try:\n            self.rave_sr = int(self.rave.sampling_rate)\n        except Exception:\n            self.rave_sr = int(self.rave.sr)\n        with torch.inference_mode():\n            # warmup\n            if hasattr(self.rave, 'full_latent_size'):\n                latent_size = self.rave.latent_size + int(\n                    hasattr(self.rave, 'pitch_encoder'))\n            else:\n                latent_size = self.rave.cropped_latent_size\n            self.rave.decode(torch.zeros(1,latent_size,1))\n    else:\n        self.rave = None\n        self.rave_sr = None\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.paint_alignment","title":"<code>paint_alignment()</code>","text":"<p>helper for <code>step_gen</code></p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def paint_alignment(self):\n    \"\"\"helper for `step_gen`\"\"\"\n    align_params = self.momentary_align_params or self.align_params\n    self.momentary_align_params = None\n    if align_params is None:\n        return None\n    loc, scale = align_params\n    n_tokens = self.text_rep.shape[1]\n    loc = max(0, min(n_tokens, loc))\n    deltas = torch.arange(n_tokens) - loc\n    deltas = deltas / (0.5 + scale) # sharpness modulated by speed \n    # discrete gaussian, sums exactly to 1 over text positions\n    logits = -deltas**2\n    res = logits.exp() \n    res = res / res.sum()\n    return res[None]\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.pause","title":"<code>pause()</code>","text":"<p>pause generation or sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def pause(self):\n    \"\"\"\n    pause generation or sampler\n    \"\"\"\n    self.step_mode = StepMode.PAUSE\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.prev_state","title":"<code>prev_state(*a, step=-1, utterance=-1, global_step=None)</code>","text":"<p>convenient access to previous states</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def prev_state(self, *a, step=-1, utterance=-1, global_step=None):\n    \"\"\"convenient access to previous states\"\"\"\n    if global_step is not None:\n        try:\n            utterance, step = self.global_step_to_utterance(global_step)\n        except ValueError:\n            return None\n\n    s = self.states[utterance][step]\n    for k in a:\n        s = s[k]\n    return s\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.process_frame","title":"<code>process_frame(latent_t=None, align_t=None)</code>","text":"<p>Generate an audio(rave latents) and alignment frame.</p> <p>Parameters:</p> Name Type Description Default <code>latent_t</code> <code>Optional[Tensor]</code> <p>[batch, RAVE latent size] last frame of audio feature if using <code>latent feedback</code> mode</p> <code>None</code> <code>align_t</code> <code>Optional[Tensor]</code> <p>[batch, text length in tokens] explicit alignments if using <code>paint</code> mode</p> <code>None</code> <p>Returns:     latent_t: [batch, RAVE latent size]         next frame of audio feature     align_t: [batch, text length in tokens]         alignments to text</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def process_frame(self, \n        latent_t:Optional[Tensor]=None, \n        align_t:Optional[Tensor]=None\n    ) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Generate an audio(rave latents) and alignment frame.\n\n    Args:\n        latent_t: [batch, RAVE latent size]\n            last frame of audio feature if using `latent feedback` mode\n        align_t: [batch, text length in tokens]\n            explicit alignments if using `paint` mode\n    Returns:\n        latent_t: [batch, RAVE latent size]\n            next frame of audio feature\n        align_t: [batch, text length in tokens]\n            alignments to text\n    \"\"\"\n    with torch.inference_mode():\n        with self.profile('tts', detail=False):\n            r = self.model.step(\n                alignment=align_t, audio_frame=latent_t, \n                temperature=self.temperature)\n            latent_t, align_t, \n    # use low precision for storage\n    return r['output'].half(), r['alignment'].half()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.reset","title":"<code>reset()</code>","text":"<p>reset the model state and alignments history</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def reset(self):\n    \"\"\"\n    reset the model state and alignments history\n    \"\"\"\n    self.needs_reset = True\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.run","title":"<code>run(conn)</code>","text":"<p>run method expected by Proxy</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def run(self, conn):\n    \"\"\"run method expected by Proxy\"\"\"\n    # call this from both __init__ and run\n    # torch.multiprocessing.set_sharing_strategy('file_system')\n\n    self.frontend_conn = conn\n    # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n    buffer_frames, audio_out, checkpoint, rave_path, audio_block, audio_channels = self.init_args\n\n    self.load_tts_model(checkpoint=checkpoint)\n\n    ### synthesis in python:\n    # load RAVE model\n    self.load_vocoder_model(rave_path=rave_path)\n\n    ### audio output:\n    # make sounddevice stream\n    if self.out_mode:\n        devicelist = sd.query_devices()\n\n        # None throws an error on linux, on mac it uses the default device\n        # Let's make this behavior explicit.\n        if audio_out is None:\n            audio_out = sd.default.device[1]\n\n        audio_device = None\n        for dev in devicelist:\n            if audio_out in [dev['index'], dev['name']]:\n                audio_device = dev\n                # audio_device_sr = dev['default_samplerate']\n            print(f\"{dev['index']}: '{dev['name']}' {dev['hostapi']} (I/O {dev['max_input_channels']}/{dev['max_input_channels']}) (SR: {dev['default_samplerate']})\")\n\n        # this should not be an error\n        # None uses the default device on macOS\n        # if audio_device is None:\n            # raise RuntimeError(f\"Audio device '{audio_out}' does not exist.\")\n\n        print(f\"USING AUDIO OUTPUT DEVICE {audio_out}:{audio_device}\")\n\n        self.active_frame:torch.Tensor = None \n        self.future_frame:Thread = None\n        self.frame_counter:int = 0\n\n        sd.default.device = audio_out\n        if self.rave_sr:\n            if audio_device and audio_device['default_samplerate'] != self.rave_sr:\n                # this should not be an error. On OSX/CoreAudio you can set the device sample rate to the model sample rate.\n                # however on Linux/JACK this throws a fatal error and stops program execution, requiring you to restart Jack to change the sampling rate\n\n                # TODO: also check RAVE block size vs. audio device block size if possible\n                print(\"\\n------------------------------------\");\n                print(f\"WARNING: Device default sample rate ({audio_device['default_samplerate']}) and RAVE model sample rate ({self.rave_sr}) mismatch! You may need to change device sample rate manually on some platforms.\")\n                print(\"------------------------------------\\n\");\n\n            sd.default.samplerate = self.rave_sr # could cause an error if device uses a different sr from model\n            print(f\"RAVE SAMPLING RATE: {self.rave_sr}\")\n            print(f\"DEVICE SAMPLING RATE: {audio_device['default_samplerate']}\")\n\n        # TODO: Tungnaa only uses audio output. Shouldn't we always be using sd.OutputStream?\n        try:\n            assert len(audio_out)==2\n            stream_cls = sd.Stream\n        except Exception:\n            stream_cls = sd.OutputStream\n\n        self.stream = stream_cls(\n            callback=self.audio_callback,\n            samplerate=self.rave_sr, \n            blocksize=audio_block, \n            #device=(audio_in, audio_out)\n            device=audio_out,\n            channels=audio_channels\n        )\n\n        if self.rave_sr:\n            assert self.stream.samplerate == self.rave_sr, f\"\"\"\n            failed to set sample rate to {self.rave_sr} from sounddevice\n            \"\"\"\n    else:\n        self.stream = None\n\n    self.text = None\n    self.text_rep = None\n    self.align_params = None\n    self.momentary_align_params = None\n    self.step_to_set = None\n\n    self.one_shot_paint = False\n    self.latent_feedback = False\n\n    self.lock = RLock()\n    self.text_update_thread = None\n\n    # self.frontend_q = Queue()\n    self.audio_q = Queue(buffer_frames)     \n    self.trigger_q = Queue(buffer_frames)     \n\n    self.states = []\n\n    self.needs_reset = False\n\n    self.step_thread = Thread(target=self.step_loop, daemon=True)\n    self.step_thread.start()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.sampler","title":"<code>sampler()</code>","text":"<p>start sampler mode</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def sampler(self):\n    \"\"\"\n    start sampler mode\n    \"\"\"\n    self.step_mode = StepMode.SAMPLER\n    self.start_stream()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.set_alignment","title":"<code>set_alignment(align_params)</code>","text":"<p>Send alignment parameters to the backend. If None is passed, alignment painting is off.</p> <p>Parameters:</p> Name Type Description Default <code>align_params</code> <code>Optional[Tuple[float, float]]</code> <p>[loc, scale] or None</p> required Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_alignment(self, \n        align_params:Optional[Tuple[float, float]]\n    ) -&gt; None:\n    \"\"\"\n    Send alignment parameters to the backend. If None is passed, alignment painting is off.\n\n    Args:\n        align_params: [loc, scale] or None\n    \"\"\"\n    self.align_params = align_params\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.set_momentary_alignment","title":"<code>set_momentary_alignment(align_params)</code>","text":"<p>Alignment will be set on the next frame only</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_momentary_alignment(self,             \n        align_params:Optional[Tuple[float, float]]\n    ) -&gt; None:\n    \"\"\"\n    Alignment will be set on the next frame only\n    \"\"\"\n    self.momentary_align_params = align_params\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.set_sampler_loop_index","title":"<code>set_sampler_loop_index(start=None, end=None, utterance=None, reset=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>loop start step</p> <code>None</code> <code>end</code> <code>int</code> <p>loop end step</p> <code>None</code> <code>utterance</code> <code>int</code> <p>sampler utterance</p> <code>None</code> <code>reset</code> <code>bool</code> <p>if True, immediately go to loop start</p> <code>True</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_sampler_loop_index(self, \n        start:int=None, end:int=None, \n        utterance:int=None, \n        reset:bool=True):\n    \"\"\"\n    Args:\n        start: loop start step\n        end: loop end step\n        utterance: sampler utterance\n        reset: if True, immediately go to loop start\n    \"\"\"\n    if len(self.states)==0: return\n\n    def wrap(x, n, tag):\n        if x is None: return x\n        if x &lt; 0: \n            x += n\n        if x &lt; 0 or x &gt;= n:\n            print(f'warning: out of bounds {tag} {x}')\n            x %= n\n        return x\n\n    if utterance is not None:\n        if start is not None:\n            start = self.utterance_to_global_step(utterance, start)\n        if end is not None:\n            end = self.utterance_to_global_step(utterance, end)\n\n    if start is None: start = self.sampler_loop_start        \n    if end is None: end = self.sampler_loop_end\n\n    # utterance = wrap(utterance, len(self.states), 'utterance')\n    start = wrap(start, len(self.states[utterance]), 'loop start')\n    end = wrap(end, len(self.states[utterance]), 'loop end')\n\n    # changed = utterance != self.sampler_utterance\n    # self.sampler_utterance = utterance\n\n    self.sampler_loop_start = start\n    self.sampler_loop_end = end\n\n    # must reset if changing utterance\n    if reset:\n        self.reset_sampler()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.set_sampler_loop_text","title":"<code>set_sampler_loop_text(text, n=-1, start=True, end=True, reset=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>a regular expression string</p> required <code>n</code> <code>int</code> <p>index of occurrence in history</p> <code>-1</code> <code>start</code> <code>bool</code> <p>if True, set the loop start</p> <code>True</code> <code>end</code> <code>bool</code> <p>if True, set the loop end</p> <code>True</code> <code>reset</code> <code>bool</code> <p>if True, immediately go to loop start</p> <code>True</code> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_sampler_loop_text(self, \n        text:str, n:int=-1, \n        start:bool=True, end:bool=True, \n        reset:bool=True):\n    \"\"\"\n    Args:\n        text: a regular expression string\n        n: index of occurrence in history\n        start: if True, set the loop start\n        end: if True, set the loop end\n        reset: if True, immediately go to loop start\n    \"\"\"\n    if not (start or end): return\n    r = text\n\n    # get all occurences of all matching strings,\n    #   and index by order in history\n\n    # first find matches for the regex in utterance texts\n    matches = []\n    utts = self.states #if n&gt;=0 else reversed(self.states)\n    index = 0\n    for utt_index, utt in enumerate(utts):\n        if len(utt)==0: continue\n        # text = utt[0]['text']\n        text = utt.text\n        for match in re.findall(r, text):\n            # error if multiple capture groups in regex\n            if not isinstance(match, str):\n                raise ValueError(\"`set_sampler_loop`: multiple capture groups not supported\")\n            # add utterance, text index of match\n            index = index + text[index:].find(match)\n            matches.append((utt_index, index, index+len(match), match))\n            # print(f'{utt_index=}, {index=}, {match=}')\n\n    if len(matches)==0:\n        print(f'warning: no text matching \"{r}\" in `set_sampler_loop`')\n        return\n\n    # for matches, get occurences and flatten into list\n    #   simple: occurrence is from entering start to exiting end\n    #   TODO better: occurence is when there is a run which\n    #       enters the first token / leaves the last token\n    #       without any major jumps or reversals\n    #   TODO?: allow skips of first / last token?\n    occurrences = []\n    for u, i, j, m in matches:\n        occ_start = None\n        utt = self.states[u]\n        for k, state in enumerate(utt):\n            align = state['align_hard']\n            # print(align)\n\n            if align['enter_index']==i:\n                # open/replace occurrence\n                if occ_start is not None:\n                    print(f'warning: double open {m=} {occ_start=} {u=} {k=}')\n                occ_start = k\n            if align['leave_index']==j:\n                # close occurrence (or ignore)\n                if occ_start is None: \n                    print(f'warning: close without open {m=}')\n                    continue\n                occurrences.append((u, occ_start, k))\n                # print(f'{(u, occ_start, k)=}')\n                occ_start = None\n\n    if len(occurrences)==0:\n        print(f'warning: no occurrences of {matches} in `set_sampler_loop`')\n        return\n\n    # index into list using n\n    u, loop_start, loop_end = occurrences[n]\n\n    loop_start = self.utterance_to_global_step(u, loop_start)\n    loop_end = self.utterance_to_global_step(u, loop_end)\n    # TODO: efficient search for index of occurence\n\n    # set loop points\n    # if utterance changes, unset points which aren't set\n    # changed = u != self.sampler_utterance\n    # self.sampler_utterance = u\n\n    if start:\n        self.sampler_loop_start = loop_start\n    # elif changed:\n        # self.sampler_loop_start = 0\n\n    if end:\n        self.sampler_loop_end = loop_end\n    # elif changed:\n        # self.sampler_loop_end = None\n\n    # must reset if changing utterance\n    # if changed or reset:\n    if reset:\n        self.reset_sampler()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.set_text","title":"<code>set_text(text)</code>","text":"<p>Compute embeddings for &amp; store a new text, replacing the old text. Returns the number of embedding tokens</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text as a string</p> required <p>Returns:</p> Type Description <code>int</code> <p>length of text in tokens</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def set_text(self, text:str) -&gt; int:\n    \"\"\"\n    Compute embeddings for &amp; store a new text, replacing the old text.\n    Returns the number of embedding tokens\n\n    Args:\n        text: input text as a string\n\n    Returns:\n        length of text in tokens\n    \"\"\"\n    if (\n        self.text_update_thread is not None \n        and self.text_update_thread.is_alive()\n    ):\n        print('warning: text update still pending')\n\n    # TODO: more general text preprocessing\n    text, start, end = self.extract_loop_points(text)\n    tokens, text, idx_map = self.text_model.tokenize(text)\n    # print(start, end, idx_map)\n    # NOTE: positions in text may change fron tokenization (end tokens)\n    if start &lt; len(idx_map):\n        start = idx_map[start]\n    else:\n        start = 0\n    if end is not None and end &lt; len(idx_map):\n        end = idx_map[end]\n    else:\n        end = None\n\n    # text processing runs in its own thread\n    self.text_update_thread = Thread(\n        target=self._update_text, \n        args=(text, tokens, start, end), daemon=True)\n    self.text_update_thread.start()\n\n    return text\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.start_stream","title":"<code>start_stream()</code>","text":"<p>helper for start/sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def start_stream(self):\n    \"\"\"helper for start/sampler\"\"\"\n    # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.LATENT_OSC):\n    #     if not self.loop_thread.is_alive():\n    #         self.run_thread = True\n    #         self.loop_thread.start()\n    # if self.out_mode in (OutMode.LATENT_AUDIO, OutMode.SYNTH_AUDIO):\n    # if self.out_mode is not None:\n    if not self.stream.active:\n        self.stream.start()\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.step","title":"<code>step(timestamp)</code>","text":"<p>compute one vocoder frame of generation or sampler</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step(self, timestamp):\n    \"\"\"compute one vocoder frame of generation or sampler\"\"\"\n    # if self.frontend_q.qsize() &gt; 100:\n        # self.frontend_q.get()\n        # print('frontend queue full, dropping')\n\n    state = {'text':self.text}\n\n    # reset text and model states\n    if self.needs_reset:\n        state |= self.do_reset()\n\n    # print(f'{self.frontend_conn=} {threading.get_native_id()=} {os.getpid()=}')\n\n    if self.text_rep is None or self.step_mode==StepMode.PAUSE:\n        if len(state):\n            # self.frontend_q.put(state)\n            if self.frontend_conn is not None:\n                self.send_state(state)\n            else:\n                print('warning: frontend_conn does not exist')\n        return None\n    if self.step_mode==StepMode.GENERATION:\n        # with self.profile('step_gen'):\n        state |= self.step_gen(timestamp)\n    elif self.step_mode==StepMode.SAMPLER:\n        with self.profile('step_sampler'):\n            state |= self.step_sampler(timestamp)\n    # else:\n        # print(f'WARNING: {self.step_mode=} in step')\n\n    if self.osc_sender is not None:\n        self.osc_sender(state)\n\n    if len(self.latent_biases):\n        with torch.inference_mode():\n            bias = torch.tensor(self.latent_biases)[None]\n            state['latent_t'] += bias\n\n    # send to frontend\n    # self.frontend_q.put(state)\n    if self.frontend_conn is not None:\n        self.send_state(state)\n    else:\n        # print(state)\n        print('warning: frontend_conn does not exist')\n\n    latent = state['latent_t'] # batch, channel\n    batch, n_latent = latent.shape\n    assert batch==1\n\n    c_synth, c_latent = len(self.synth_channels), len(self.latent_channels)\n\n    with torch.inference_mode():\n        audio_frame = torch.zeros(\n            c_synth+c_latent, \n            self.model.block_size) # channel, time\n\n        if OutMode.SYNTH_AUDIO in self.out_mode:\n            # allow creative use of vocoders with mismatched sizes\n            rave_dim = self.rave.decode_params[0]\n            common_dim = min(rave_dim, latent.shape[-1])\n            latent_to_rave = torch.zeros(latent.shape[0], rave_dim)\n            latent_to_rave[:,:common_dim] = latent[:,:common_dim]\n            ### run the vocoder\n            with self.profile('vocoder'):\n                audio = self.rave.decode(\n                    latent_to_rave[...,None])[0] # channel, time\n            ###\n            audio_frame[:c_synth, :] = audio\n\n        if OutMode.LATENT_AUDIO in self.out_mode:\n            latent = torch.cat((\n                torch.zeros(batch, 1), # zero to ensure trigger\n                torch.full((batch, 1), n_latent), # trigger, number of latents\n                latent\n                ), dim=1) / 1024 # scale down in case audio gets sent to speakers\n            # latent x batch\n\n            audio_frame[c_synth:, :2+n_latent] = latent\n\n    return audio_frame\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.step_gen","title":"<code>step_gen(timestamp)</code>","text":"<p>generation branch of main <code>step</code> method</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_gen(self, timestamp):\n    \"\"\"generation branch of main `step` method\"\"\"\n    # loop end\n    if (\n        self.align_params is None \n        and self.gen_loop_end is not None \n        and not self.utterance_empty()\n        and self.prev_state('align_hard', 'index') in (\n            self.gen_loop_end, self.gen_loop_end+1)\n    ):\n        self.momentary_align_params = (self.gen_loop_start, 1)\n\n    # set just model states\n    # TODO: possibility to set to previous utterance?\n    if self.step_to_set is not None:\n        step = self.step_to_set\n        with torch.inference_mode():\n            try:\n                self.model.set_state(\n                    self.prev_state('model_state', step=step))\n            except Exception:\n                print(f'WARNING: failed to set {step=} ')\n                raise\n        self.step_to_set = None\n\n    if self.model.memory is None:\n        print('skipping: model not initialized')\n        if self.rave is not None:\n            self.audio_q.put(None)\n        return None\n\n    if not self.utterance_empty() and self.latent_feedback:\n        # pass the last vocoder frame back in\n        latent_t = self.prev_state('latent_t')\n    else:\n        latent_t = None\n\n    align_t = self.paint_alignment()\n\n    ##### run the TTS model\n    latent_t, align_t = self.process_frame(\n        # self.mode, \n        latent_t=latent_t, align_t=align_t)\n    #####\n\n    # NOTE\n    ### EXPERIMENTAL juice knob\n    # latent_t = latent_t * 1.1\n    ###\n\n    state = {\n        # 'text':self.text,\n        'latent_t':latent_t, \n        'align_t':align_t,\n        'timestamp':timestamp,\n        'model_state':self.model.get_state(),\n        'align_hard':self.hard_alignment(align_t)\n        }\n\n    if (\n        self.generate_stop_at_end \n        and state['align_hard']['index']==(len(self.text)-1)\n    ):\n        self.step_mode = StepMode.PAUSE\n        # self.reset()\n\n    utt = self.states[-1]\n    utt.append(state)\n    # remove RNN states after certain number of steps\n    if len(utt) &gt;= self.max_model_state_storage:\n        self.strip_states(utt[-self.max_model_state_storage])\n    return state\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.step_loop","title":"<code>step_loop()</code>","text":"<p>model stepping thread</p> <p>for each timestamp received in trigger_q send audio frames in audio_q</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_loop(self):\n    \"\"\"model stepping thread\n\n    for each timestamp received in trigger_q\n    send audio frames in audio_q\n    \"\"\"\n    while True:\n        t = self.trigger_q.get()\n        with self.profile('step'):\n            frame = self.step(t)\n        if frame is not None:\n            # with self.profile('frame.numpy'):\n            frame = frame.numpy()\n        self.audio_q.put(frame)\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.step_sampler","title":"<code>step_sampler(timestamp)</code>","text":"<p>sampler branch of main <code>step</code> method</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def step_sampler(self, timestamp):\n    \"\"\"sampler branch of main `step` method\"\"\"\n    # print(f'{self.sampler_step=}')\n\n    # if self.utterance_empty(utterance=self.sampler_utterance):\n    if self.total_steps()==0:\n        print('nothing to play back')\n        self.step_mode = StepMode.PAUSE\n        return None\n\n    state = self.prev_state(global_step=self.sampler_step) or {}\n\n    self.sampler_step += 1\n\n    if (\n        self.sampler_step == self.sampler_loop_end \n        or self.sampler_step &gt;= self.total_steps()\n        ):\n        self.sampler_step = self.sampler_loop_start\n        if self.sampler_stop_at_end:\n            self.step_mode = StepMode.PAUSE\n\n\n    state = {**state}\n    state['latent_t'] = state['latent_t'].clone()\n    state['timestamp'] = timestamp\n    state['sampler'] = True\n\n    return state\n</code></pre>"},{"location":"reference/tungnaa/gui/backend/#tungnaa.gui.backend.Backend.strip_states","title":"<code>strip_states(state)</code>","text":"<p>remove RNN states</p> Source code in <code>src/tungnaa/gui/backend.py</code> <pre><code>def strip_states(self, state):\n    \"\"\"remove RNN states\"\"\"\n    state.pop('model_state', None)\n</code></pre>"},{"location":"reference/tungnaa/gui/downloads/","title":"Downloads","text":""},{"location":"reference/tungnaa/gui/qtgui/","title":"Qtgui","text":""},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.AlignmentGraph","title":"<code>AlignmentGraph</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>Widget for displaying alignments</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class AlignmentGraph(QtWidgets.QWidget):\n    \"\"\"\n    Widget for displaying alignments\n    \"\"\"\n    def __init__(self, \n        parent:QtWidgets.QWidget=None, \n        image_clicked:typing.Callable=None):\n        super().__init__(parent)\n\n        self.max_display_steps = 200\n\n        pg.setConfigOption('imageAxisOrder', 'row-major') # best performance image data must be (height, width)\n        #pg.setConfigOption('useNumba', True) # supposedly better performance for image data\n\n        self.num_encodings = 40\n\n        self.frame = 0\n        self.prev_tok = 0\n\n        self.plot_widget = pg.GraphicsLayoutWidget(self)\n        self.plot_widget.ci.layout.setContentsMargins(0,0,0,4)\n        self.plot_widget.ci.layout.setSpacing(0)\n        self.plot = self.plot_widget.addPlot()\n\n        # self.plot = PlotWidget(parent=self)\n        # See: https://pyqtgraph.readthedocs.io/en/latest/api_reference/graphicsItems/imageitem.html#pyqtgraph.ImageItem\n        self.imageitem = AlignmentImageItem(image_clicked)\n        # self.imageitem.setImage(image=self.imagedata.T)\n        self.plot.addItem(self.imageitem)\n        self.plot.showAxes(True, showValues=(True, True, True, False))\n        self.plot.invertY(False) # vertical axis zero at the bottom\n        # no idea why, but fully transparent background is too light\n        # black with alpha of 40 seems to match\n        self.plot_widget.setBackground((0,0,0,40))\n\n        self.attn_slider_resolution = 512\n        self.attn_slider_max_value = (\n            self.num_encodings-1) * self.attn_slider_resolution\n        self.alignment_slider = QtWidgets.QSlider(\n            parent=self, orientation=QtCore.Qt.Horizontal)\n        self.alignment_slider.setMinimum(0)\n        self.alignment_slider.setMaximum( self.attn_slider_max_value )\n\n        self.layout = VBoxLayout(\n            self.alignment_slider, self.plot_widget, \n            spacing=0, margins=(0,0,0,0))\n        self.setLayout(self.layout)\n\n        self.text = None\n\n\n    def set_normalized_alignment(self, value:float):\n        \"\"\"\n        Sets alignment slider to a normalized position from 0-1\n\n        TODO: There's some kind of bug here when controlling via OSC, where the /set_alignment command doesn't scale properly from 0-1 to the range of tokens\n                my guess is that self.attn_slider_max_value isn't getting updated? Or something else weird is going on...\n        \"\"\"\n        val_as_token = int(value*self.attn_slider_max_value)\n        self.alignment_slider.setValue(val_as_token)\n        print(f\"Set normalized alignment {value} - as token: {val_as_token}/{self.attn_slider_max_value}/{self.num_encodings} \")\n\n    def set_alignment_as_token_idx(self, tok:int) -&gt; None:\n        \"\"\"\n        Set alignment slider to a position corresponding to a given token index\n        between 0 and self.num_encodings-1\n\n        tok     the target token index (must be between 0 and self.num_tokens-1)\n        \"\"\"\n\n        if tok &gt;= 0 and tok &lt; self.num_encodings:\n            tok_as_sliderval = tok * self.attn_slider_resolution\n            self.alignment_slider.setValue(tok_as_sliderval)\n        else:\n            print(f\"Error: token index {tok} out of range (0-{self.num_encodings-1})\")\n\n    def get_slidervalue_as_params(self) -&gt; np.ndarray:\n        \"\"\"slider value to attention parameters\"\"\"\n        tok = self.alignment_slider.value() / self.attn_slider_resolution \n\n        # smoothing:\n        tok = (tok + self.prev_tok) / 2\n        width = abs(tok - self.prev_tok)\n        self.prev_tok = tok\n\n        return tok, width\n\n    def addFrame(self, newframe:npt.ArrayLike, text:str):\n        # TODO: discard frames which are out of display range\n        # use a circular buffer instead of np.append\n        if text != self.text:\n            self.set_text(text)\n        self.frame +=1\n        start_step = max(0, self.frame-1-self.max_display_steps)\n        self.imagedata = np.append(self.imagedata, newframe, axis=0)\n        self.imageitem.setImage(image=self.imagedata)\n        self.imageitem.update()\n        self.plot.setYRange(start_step, self.frame-1, padding=0)\n\n    def set_text(self, text):\n        self.text = text\n\n        ticks = [(i+0.5,c) for i,c in enumerate(text)]\n        # print(ticks)\n        self.plot.getAxis('top').setTicks([ticks, []])\n        self.plot.getAxis('top').showLabel()\n        # self.plot.getAxis('bottom').showLabel(True)\n        self.plot.setXRange(0, len(text), padding=0)\n\n    def reset(self, text:str=None):\n        \"\"\"\n        Reset the attention graph with a given number of encoded tokens (y-axis)\n        Usually called after new text is encoded / attention recalculated.\n\n        num_encodings sets the number of encoded tokens to scale the y-axis by\n            if not set the current number of encodings is left as-is\n        \"\"\"\n        print(f\"AlignmentGraph: reset\")\n        self.frame = 1\n        if text is not None:\n            self.num_encodings = len(text)\n            self.set_text(text)\n        self.attn_slider_max_value = (\n            self.num_encodings * self.attn_slider_resolution)\n        self.alignment_slider.setMinimum(0)\n        self.alignment_slider.setMaximum(self.attn_slider_max_value)\n        self.imagedata = np.zeros((1,self.num_encodings), dtype=np.float32)\n        # self.imageitem.setImage(image=self.imagedata.T)\n        self.plot.setYRange(0, 1, padding=0)\n        self.imageitem.setImage(image=self.imagedata)\n        self.imageitem.update()\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.AlignmentGraph.get_slidervalue_as_params","title":"<code>get_slidervalue_as_params()</code>","text":"<p>slider value to attention parameters</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def get_slidervalue_as_params(self) -&gt; np.ndarray:\n    \"\"\"slider value to attention parameters\"\"\"\n    tok = self.alignment_slider.value() / self.attn_slider_resolution \n\n    # smoothing:\n    tok = (tok + self.prev_tok) / 2\n    width = abs(tok - self.prev_tok)\n    self.prev_tok = tok\n\n    return tok, width\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.AlignmentGraph.reset","title":"<code>reset(text=None)</code>","text":"<p>Reset the attention graph with a given number of encoded tokens (y-axis) Usually called after new text is encoded / attention recalculated.</p> <p>num_encodings sets the number of encoded tokens to scale the y-axis by     if not set the current number of encodings is left as-is</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def reset(self, text:str=None):\n    \"\"\"\n    Reset the attention graph with a given number of encoded tokens (y-axis)\n    Usually called after new text is encoded / attention recalculated.\n\n    num_encodings sets the number of encoded tokens to scale the y-axis by\n        if not set the current number of encodings is left as-is\n    \"\"\"\n    print(f\"AlignmentGraph: reset\")\n    self.frame = 1\n    if text is not None:\n        self.num_encodings = len(text)\n        self.set_text(text)\n    self.attn_slider_max_value = (\n        self.num_encodings * self.attn_slider_resolution)\n    self.alignment_slider.setMinimum(0)\n    self.alignment_slider.setMaximum(self.attn_slider_max_value)\n    self.imagedata = np.zeros((1,self.num_encodings), dtype=np.float32)\n    # self.imageitem.setImage(image=self.imagedata.T)\n    self.plot.setYRange(0, 1, padding=0)\n    self.imageitem.setImage(image=self.imagedata)\n    self.imageitem.update()\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.AlignmentGraph.set_alignment_as_token_idx","title":"<code>set_alignment_as_token_idx(tok)</code>","text":"<p>Set alignment slider to a position corresponding to a given token index between 0 and self.num_encodings-1</p> <p>tok     the target token index (must be between 0 and self.num_tokens-1)</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_alignment_as_token_idx(self, tok:int) -&gt; None:\n    \"\"\"\n    Set alignment slider to a position corresponding to a given token index\n    between 0 and self.num_encodings-1\n\n    tok     the target token index (must be between 0 and self.num_tokens-1)\n    \"\"\"\n\n    if tok &gt;= 0 and tok &lt; self.num_encodings:\n        tok_as_sliderval = tok * self.attn_slider_resolution\n        self.alignment_slider.setValue(tok_as_sliderval)\n    else:\n        print(f\"Error: token index {tok} out of range (0-{self.num_encodings-1})\")\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.AlignmentGraph.set_normalized_alignment","title":"<code>set_normalized_alignment(value)</code>","text":"<p>Sets alignment slider to a normalized position from 0-1</p> There's some kind of bug here when controlling via OSC, where the /set_alignment command doesn't scale properly from 0-1 to the range of tokens <p>my guess is that self.attn_slider_max_value isn't getting updated? Or something else weird is going on...</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_normalized_alignment(self, value:float):\n    \"\"\"\n    Sets alignment slider to a normalized position from 0-1\n\n    TODO: There's some kind of bug here when controlling via OSC, where the /set_alignment command doesn't scale properly from 0-1 to the range of tokens\n            my guess is that self.attn_slider_max_value isn't getting updated? Or something else weird is going on...\n    \"\"\"\n    val_as_token = int(value*self.attn_slider_max_value)\n    self.alignment_slider.setValue(val_as_token)\n    print(f\"Set normalized alignment {value} - as token: {val_as_token}/{self.attn_slider_max_value}/{self.num_encodings} \")\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.DoubleSlider","title":"<code>DoubleSlider</code>","text":"<p>               Bases: <code>QSlider</code></p> <p>Continuous value QSlider that uses native double values instead of integers</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class DoubleSlider(QtWidgets.QSlider):\n    \"\"\"\n    Continuous value QSlider that uses native double values instead of integers\n    \"\"\"\n\n    # define a signal which enables Qt's callback system of signals/slots\n    doubleValueChanged = QtCore.Signal(float)\n\n    def __init__(self, decimals=3, *args, **kargs):\n        super(DoubleSlider, self).__init__( *args, **kargs)\n        self._multi = 10 ** decimals\n        self.valueChanged.connect(self.emitDoubleValueChanged)\n\n    def emitDoubleValueChanged(self):\n        value = float(super(DoubleSlider, self).value())/self._multi\n        self.doubleValueChanged.emit(value)\n\n    def value(self):\n        return float(super(DoubleSlider, self).value()) / self._multi\n\n    def setMinimum(self, value):\n        return super(DoubleSlider, self).setMinimum(value * self._multi)\n\n    def setMaximum(self, value):\n        return super(DoubleSlider, self).setMaximum(value * self._multi)\n\n    def setSingleStep(self, value):\n        return super(DoubleSlider, self).setSingleStep(value * self._multi)\n\n    def singleStep(self):\n        return float(super(DoubleSlider, self).singleStep()) / self._multi\n\n    def setValue(self, value:float):\n        super(DoubleSlider, self).setValue(int(value * self._multi))\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow","title":"<code>MainWindow</code>","text":"<p>               Bases: <code>QMainWindow</code></p> <p>Main Application Window</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class MainWindow(QtWidgets.QMainWindow):\n    \"\"\"\n    Main Application Window\n    \"\"\"\n\n    def __init__(self, \n        parent:QtWidgets.QWidget=None, \n        use_backend:bool=True, \n        backend:tungnaa.gui.backend.Backend=None, \n        sender:tungnaa.gui.senders.Sender=None,\n        update_fps:int=100,\n        osc_listen_addr:tuple[str,int]=('localhost', 1337),\n        stress_gui=None,\n        text=None,\n        sampler_text=None,\n        ):\n        super().__init__(parent)\n        self.version= \"Alpha v0.1.0\" # TODO: There's probably a tidier way to do this using Qt or python package management\n        self.appname= f\"T\u0334u\u032en\u0335g\u0334na\u0360a\u0301 {self.version}\"\n\n        self.stress_gui = stress_gui\n\n        if sys.platform == 'darwin':\n            self.setUnifiedTitleAndToolBarOnMac(True)\n\n        self.mode = 'infer'\n\n        self.update_fps = update_fps\n        self.osc_listen_addr = osc_listen_addr\n        self.use_backend = use_backend\n        self.backend = backend\n        self.sender = sender\n        if self.backend is None:\n            print(\"No backend provided, disabling backend\")\n            self.use_backend = False\n\n        # OSC\n        self.osc_controller = OSCController(context=self)\n\n        # TODO: this line auto-runs the OSC server and creates a bug\n        #     Currently needs to be activated from the gui, running this line starts the OSC server but does not reflect this status in the gui\n        #self.serve_osc(tuple(osc_listen_addr))\n\n        # Signals used by other threads to communicate with GUI objects defined in the main thread.\n\n        # ---------------------------------------------------------------------\n        # BUILD GUI\n        # ---------------------------------------------------------------------\n        #self.setWindowTitle(QtCore.QCoreApplication.applicationName())\n\n        self.setWindowTitle(self.appname)\n        self.main = QtWidgets.QWidget(self)\n        # self.main.setStyleSheet(\"color: grey; background-color: black\")\n        self.setCentralWidget(self.main)\n        self.settings_dialog = None # Settings modal dialogue\n        self.app_icon = QtGui.QIcon()\n        with resources.path(\"tungnaa.resources\", \"tungnaa_icon.png\") as path:\n            self.app_icon.addFile(path.as_posix())\n        self.setWindowIcon(self.app_icon)\n        self.setWindowIconText(self.appname)\n\n        self.tungnaa_logo = QtWidgets.QLabel(parent=self.main)\n        with resources.path(\"tungnaa.resources\", \"tungnaa_logo.png\") as path:\n            logo_pixmap= QtGui.QPixmap(path.as_posix())\n        self.tungnaa_logo.setPixmap(logo_pixmap)\n        self.tungnaa_logo.setMargin(1)\n        self.tungnaa_logo.setAlignment(QtCore.Qt.AlignmentFlag.AlignLeft | QtCore.Qt.AlignmentFlag.AlignTop)\n        self.tungnaa_version = QtWidgets.QLabel(parent=self.main)\n        self.tungnaa_version.setText(self.version)\n        version_font = QtGui.QFont()\n        version_font.setPointSize(8)\n        self.tungnaa_version.setFont(version_font)\n        self.tungnaa_version.setMargin(0)\n        self.tungnaa_version.setAlignment(QtCore.Qt.AlignmentFlag.AlignRight | QtCore.Qt.AlignmentFlag.AlignTop)\n\n\n\n\n        # Main App Toolbar\n        self.toolbar_top = QtWidgets.QToolBar(self.main)\n        self.toolbar_combined = QtWidgets.QWidget(self.main)\n        self.toolbar_controls = QtWidgets.QToolBar(self.toolbar_combined)\n        controls_layout = HBoxLayout()\n        controls_layout.addWidget(self.toolbar_controls)\n        controls_layout.addStretch(1)\n        controls_layout.addLayout(VBoxLayout(self.tungnaa_logo, self.tungnaa_version))\n        self.toolbar_combined.setLayout(controls_layout)\n        self.addToolBar(self.toolbar_top) # Toolbar gets added to VLayout below\n\n        self._generate_action = QtGui.QAction(\"Generate\", self)\n        self._generate_action.setStatusTip(\"Start autoregression\")\n        self._generate_action.triggered.connect(self.play_generate)\n\n        self._sampler_action = QtGui.QAction(\"Sampler\", self)\n        self._sampler_action.setStatusTip(\"Sample generated latents\")\n        self._sampler_action.triggered.connect(self.play_sampler)\n\n        self._pause_action = QtGui.QAction(\"Pause\", self)\n        self._pause_action.setStatusTip(\"Pause generation or sampler\")\n        self._pause_action.triggered.connect(self.pause)\n\n        self._reset_action = QtGui.QAction(\"Reset\", self)\n        self._reset_action.setStatusTip(\"Reset autoregression history\")\n        self._reset_action.triggered.connect(self.reset_autoregression)\n\n        self._alignment_paint_toggle_action = QtGui.QAction(\"Attention Painting\", self)\n        self._alignment_paint_toggle_action.setCheckable(True)\n        self._alignment_paint_toggle_action.setStatusTip(\"Toggle Attention Painting\")\n        self._alignment_paint_toggle_action.toggled.connect(self.toggle_alignment_paint)\n\n        self._latent_feedback_toggle_action = QtGui.QAction(\"Latent Feedback\", self)\n        self._latent_feedback_toggle_action.setCheckable(True)\n        self._latent_feedback_toggle_action.setStatusTip(\"Feed latent manipulation back to model\")\n        self._latent_feedback_toggle_action.toggled.connect(self.toggle_latent_feedback)\n\n        self._generate_stop_at_end_toggle_action = QtGui.QAction(\"Generator Stop At End\", self)\n        self._generate_stop_at_end_toggle_action.setCheckable(True)\n        self._generate_stop_at_end_toggle_action.setStatusTip(\"Pause automatically at end of utterance text in generate mode\")\n        self._generate_stop_at_end_toggle_action.toggled.connect(self.toggle_generate_stop_at_end)\n\n        self._sampler_stop_at_end_toggle_action = QtGui.QAction(\"Sampler Stop At End\", self)\n        self._sampler_stop_at_end_toggle_action.setCheckable(True)\n        self._sampler_stop_at_end_toggle_action.setStatusTip(\"Pause automatically at loop end point in sampler mode\")\n        self._sampler_stop_at_end_toggle_action.toggled.connect(self.toggle_sampler_stop_at_end)\n\n        self._latent_bias_reset_action = QtGui.QAction(\"Bias Reset\", self)\n        self._latent_bias_reset_action.setStatusTip(\"Reset all latent biases to 0.0\")\n        self._latent_bias_reset_action.triggered.connect(self._reset_latent_biases)\n\n        self._settings_action = QtGui.QAction(\"Settings\", self)\n        self._settings_action.setStatusTip(\"Adjust OSC, MIDI and Audio settings\")\n        self._settings_action.triggered.connect(self.open_settings)\n        self.temperature_slider = DoubleSlider(orientation=QtCore.Qt.Horizontal, decimals=3, parent=self)\n        self.temperature_slider.setMaximum(2.0)\n        self.temperature_slider.setMinimum(0.0)\n        self.temperature_slider.setValue(1)\n        self.temperature_slider.doubleValueChanged.connect(lambda val: self._temperature_adjust(val))\n        self.temperature_slider.setStatusTip(\"sampling temperature\")\n\n\n        self.toolbar_top.addAction(self._alignment_paint_toggle_action)\n        self.toolbar_top.addAction(self._latent_feedback_toggle_action)\n        self.toolbar_top.addAction(self._sampler_stop_at_end_toggle_action)\n        self.toolbar_top.addAction(self._generate_stop_at_end_toggle_action)\n        self.toolbar_top.addAction(self._settings_action)\n\n        self.toolbar_controls.addAction(self._generate_action)\n        self.toolbar_controls.addAction(self._sampler_action)\n        self.toolbar_controls.addAction(self._pause_action)\n        self.toolbar_controls.addAction(self._reset_action)\n        self.toolbar_controls.addAction(self._latent_bias_reset_action)\n\n\n\n        # Tabbed Interface for Generator/Sampler\n        self.tabwidget = QtWidgets.QTabWidget(self.main);\n\n        # Generator Tab\n        self.generator_widget = QtWidgets.QWidget(self.tabwidget);\n\n        # See https://doc.qt.io/qt-6/qtextedit.html for all the fun slots for textedit\n        self.gen_text_input = QtWidgets.QTextEdit(parent=self.generator_widget)\n        self.gen_text_input.setMaximumHeight(100)\n        self.gen_text_input.setFontPointSize(24)\n        self.gen_text_input.setPlainText(text or \"&lt;It took me a while to have a voice. And now that I have one, I am not going to be silent.&gt;\")\n        self.gen_text_input.setStatusTip(\"enter text to generate new utterances\")\n        signaller.set_gen_text.connect(self.gen_text_input.setPlainText)\n        #self.text_input.setPlainText(\"It took me a while to find a voice, and now that I have one, I am not going to be silent.\") # todo - add a \"random phrase\" generator?\n        self.btn_send_gen_text = QtWidgets.QPushButton(\"Encode\", parent=self.generator_widget)\n        self.btn_send_gen_text.clicked.connect(self.encode_text)\n        self.btn_send_gen_text.setStatusTip(\"send text to model\")\n        self.text_encoding_status = QtWidgets.QLabel(parent=self.generator_widget)\n        self.text_encoding_status.setText(\"\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad encoder feedback... \u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\")\n\n\n        self.attention_graph = AlignmentGraph(parent=self.generator_widget, image_clicked=self.image_clicked)\n        self.attention_graph.setStatusTip(\"text-audio alignments (click to jump)\")\n\n        self._generator_layout = VBoxLayout(\n                self.gen_text_input, \n                self.btn_send_gen_text,\n                self.text_encoding_status,\n                self.attention_graph,\n                spacing=0, margins=(0,0,0,0)\n        )\n        self.generator_widget.setLayout(self._generator_layout);\n\n\n        # Sampler Tab\n        self.sampler_widget = QtWidgets.QWidget(self.tabwidget);\n\n        self.samp_text_input = QtWidgets.QTextEdit(self.sampler_widget)\n        self.samp_text_input.setMaximumHeight(100)\n        self.samp_text_input.setFontPointSize(24)\n        self.samp_text_input.setPlainText(sampler_text or \"took\")\n        self.samp_text_input.setStatusTip(\"enter text to search through previous utterances in sampler mode\")\n        signaller.set_samp_text.connect(self.samp_text_input.setPlainText)\n        self.btn_send_samp_text = QtWidgets.QPushButton(\"Sample\", parent=self.sampler_widget)\n        self.btn_send_samp_text.clicked.connect(self.loop_text)\n        self.btn_send_samp_text.setStatusTip(\"set sampler loop points by text\")\n\n\n        self._sampler_layout = VBoxLayout(\n            VBoxLayout(\n                self.samp_text_input, \n                self.btn_send_samp_text,\n                spacing=0, margins=(0,0,0,0)\n            ),\n            spacing=0, margins=(12,0,12,0),\n        )\n        self.sampler_widget.setLayout(self._sampler_layout)\n\n        self.tabwidget.addTab(self.generator_widget, \"Generator\")\n        self.tabwidget.addTab(self.sampler_widget, \"Sampler\")        \n\n\n\n        # StatusBar\n        self.statusbar = QtWidgets.QStatusBar(self.main)\n        self.setStatusBar(self.statusbar)\n\n        self.latents = RaveLatents(self.main)\n        self.setToolButtonStyle(QtCore.Qt.ToolButtonFollowStyle)\n\n\n        self._main_layout = VBoxLayout(\n            self.toolbar_combined,\n            self.tabwidget,\n            self.latents,\n            self.temperature_slider,\n            spacing=0, margins=(0,0,0,0)\n        )\n        self.main.setLayout(self._main_layout)\n\n        # Keyboard Shortcuts (TODO: put these in a menu somewhere)\n        self._encode_action = QtGui.QAction(\"Encode\", self)\n        #self._encode_action.autoRepeat = False\n        self._encode_action.triggered.connect(self.btn_send_gen_text.click)\n        self._encode_action.setShortcut(\"Ctrl+Return\") # TODO: Shift+Return overridden by QTextEdit ... need to subclass?\n        self.addAction(self._encode_action) # add to the main window as a global shortcut\n\n        self.gui_update_timer = QtCore.QTimer(self)\n        self.gui_update_timer.timeout.connect(self.update)\n\n        # Used to generate fake data in update() when no backend is provided\n        self.frame = 0\n        self.tok = 30\n        self.max_tok = 40\n        self.gui_update_timer.start((1.0 / self.update_fps) * 1000)\n\n        self._generate_stop_at_end_toggle_action.setChecked(False) # make the default to babble like a river\n        self._sampler_stop_at_end_toggle_action.setChecked(True)\n\n\n    def image_clicked(self, ev):\n        # print(ev.pos)\n        self.backend.set_momentary_alignment((ev.pos().x(), 1))\n        self.backend.set_state_by_step(int(ev.pos().y()))\n        self.backend.generate()\n        # self.backend.set_momentary_alignment(\n        #     self.attention_graph.get_slidervalue_as_params())\n\n    def update(self):\n        \"\"\"\n        update method runs on a timer\n        empties the queue from the backend and updates gui elements\n        \"\"\"\n        self.frame += 1\n        new_data = list()\n        # empty message queue, update attention graph &amp; RAVE latents\n        if self.use_backend: \n\n            if self.stress_gui is not None:\n                t = time.time_ns()\n                while time.time_ns() - t &lt; self.stress_gui*1e9:\n                    pass\n\n            try:\n                if self.latents.is_init:\n                    self.backend.set_biases(self.latents.get_biases())\n                if self.mode == 'paint':\n                    self.backend.set_alignment(\n                        self.attention_graph.get_slidervalue_as_params())\n                elif self.mode == 'infer':\n                    self.backend.set_alignment(None)\n                else:\n                    raise ValueError(f\"Unknown attention mode: {self.mode} - must be &lt;infer|paint&gt;\")\n\n                # for _ in range(self.backend.frontend_q.qsize()):\n                while self.backend.parent_conn.poll():\n                    try:\n                        # framedict = self.backend.frontend_q.get_nowait()\n                        framedict = self.backend.parent_conn.recv()\n                        if not self.latents.is_init and 'num_latents' in framedict and 'use_pitch' in framedict:\n                            self.latents._init(\n                                framedict['num_latents'], framedict['use_pitch'])\n\n                        new_data.append(framedict)\n                    except queue.Empty as ex:\n                        break\n            except Exception as e:\n                print(e)\n                exit(0) ### debug\n\n        else: # Do not use backend, instead generate random data.. useful for testing the gui (maybe?)\n            new_attn_frame = np.zeros((1,self.attention_graph.num_encodings), dtype=np.float32)        \n            new_latent_frame = np.random.rand(1,8) * 0.5\n            new_latent_frame = new_latent_frame + self.latents.get_biases()\n            if self.mode == 'infer':\n                if random.random() &lt; 0.2:\n                    self.tok += random.choice([-1, 1])\n            elif self.mode == 'paint':\n                self.tok = self.attention_graph.alignment_slider.value()            \n            else:\n                raise ValueError(f\"Unknown attention mode: {self.mode} - must be &lt;infer|paint&gt;\")\n\n            if self.tok &gt;= self.max_tok-1:\n                self.tok = self.max_tok - 1\n            else:\n                new_attn_frame[0,self.tok + 1] = 0.5\n            if self.tok &lt;= 0:\n                self.tok = 0\n            else:\n                new_attn_frame[0,self.tok - 1] = 0.5\n            new_attn_frame[0,self.tok] = 1.0\n\n            new_data.append({\n                'latent_t': new_latent_frame, 'align_t': new_attn_frame})\n\n        # iterate through new_data and update the gui\n        for datadict in new_data:\n            if datadict.get('reset', False):\n                # self.finish_reset(num_tokens=datadict['align_t'].shape[-1])\n                self.finish_reset(text=datadict['text'])\n            if 'align_t' in datadict and not datadict.get('sampler', False):\n                self.attention_graph.addFrame(\n                    datadict['align_t'], datadict['text'])\n\n        latents = [d['latent_t'] for d in new_data if 'latent_t' in d]\n        if len(latents) and self.latents.is_init:\n            self.latents.set_latents(values=latents[-1])\n\n    def finish_reset(self, text):\n        \"\"\"\n        Called when backend signals a reset.\n        \"\"\"\n        self.text_encoding_status.setText(f\"\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac encoded {len(text)} embeddings \u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\")\n        self.attention_graph.reset(text=text)\n\n    def closeEvent(self, e:QtCore.QEvent):\n        \"\"\"\n        Cleanup\n        \"\"\"\n        # TODO: cleanup OSC/networking connections\n        print(f\"Application Close {e}\")\n        self.backend.cleanup()\n        e.accept()\n        #e.ignore() # Under some conditions ignore app close?\n\n    def _temperature_adjust(self, temp=1.0) -&gt; None:\n        \"\"\"\n        Private method adjust model step inference temperature\n        \"\"\"\n        if self.use_backend:\n            self.set_temperature(temp)\n\n    def _reset_latent_biases(self):\n        \"\"\"Private method to reset latent biases to 0.0\"\"\"\n        if self.latents.is_init:\n            self.latents.reset_latent_biases(0.0)\n\n    def encode_text(self):\n        \"\"\"\n        Send input text to the text encoder backend\n        \"\"\"\n        if self.use_backend:\n            txtval = self.gen_text_input.toPlainText()\n            print(f\"Encoding: {txtval}\")\n            self.text_encoding_status.setText(\"\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad encoding.... \u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\")\n            # this returns the text with start/end tokens added and loop points stripped\n            self.backend.set_text(text=txtval)\n        else:\n            print(\"No backend enabled to encode text: ignoring...\")\n\n    def sampler_step(self, step:int, autoplay:bool) -&gt; None:\n        self.backend.set_sampler_step(step)\n        if autoplay:\n            # TODO: Once sampler mode is a toggle rather than a trigger, need to implement something like this to put the GUI in sampler mode...\n            # if gui not_in_sampler_mode                \n            #     self._sampler_action.toggle(True)\n            # else:\n            self.backend.sampler()\n\n    def loop_text(self, **kw):\n        \"\"\"\n        Send input text to the sampler backend\n        \"\"\"\n        if not self.use_backend:\n            print(\"No backend enabled to loop text: ignoring...\")\n            return\n        if 'text' not in kw:\n            kw['text'] = self.samp_text_input.toPlainText()\n        self.backend.set_sampler_loop_text(**kw)\n\n    def loop_index(self, **kw):\n        \"\"\"\n        Send input text to the sampler backend\n        \"\"\"\n        if not self.use_backend:\n            print(\"No backend enabled to loop indices: ignoring...\")\n            return\n        self.backend.set_sampler_loop_index(**kw)\n\n    def play_generate(self, val:bool):\n        print(f\"Play autoregressive frame generator\")\n        ### TODO: replace this\n        # if self.backend.text_rep is None:\n            # self.encode_text()\n        if self.attention_graph.text is None:\n            self.encode_text()\n        self.backend.generate()\n\n    def play_sampler(self, val:bool):\n        print(f\"Play sampler\")\n        self.backend.sampler()\n\n    def pause(self, val:bool):\n        print(f\"Pause generation or sampler\")\n        self.backend.pause()\n\n    def reset_autoregression(self, val:bool):\n        print(f\"Reset autoregression history\")\n        # self.attention_graph.reset()\n        self.backend.reset()\n\n    def set_temperature(self, temp:float) -&gt; None:\n        \"\"\"\n        Set inference temperature (used by OSC/MIDI)\n\n        Args:\n            temp  inference temperature from 0.0-2.0\n        \"\"\"\n        if temp &gt; 2.0:\n            temp=2.0\n        elif temp &lt; 0:\n            temp=0\n        self.temperature_slider.setValue(temp)\n        self.backend.set_temperature(temp)\n\n    def add_temperature(self, temp:float) -&gt; None:\n        \"\"\"\n        Add a small value to the inference temperature (used by OSC/MIDI)\n\n        Args:\n            temp    value that will be added to inference temperature, usually small, temperature will clip at [0,2]\n        \"\"\"\n        self.set_temperature(self.temperature_slider.value() + temp)\n\n    def toggle_alignment_paint(self, toggle:bool):\n        if toggle:\n            self.mode = 'paint'\n        else:\n            self.mode = 'infer'\n        print(f\"Alignment Mode Changed To:{self.mode}\")\n\n    def toggle_latent_feedback(self, toggle:bool):\n        self.backend.set_latent_feedback(toggle)\n        print(f\"Latent Feedback status changed to:{toggle}\")\n\n    def toggle_generate_stop_at_end(self, toggle:bool):\n        self.backend.set_generate_stop_at_end(toggle)\n        print(f\"Stop Generate At End status changed to:{toggle}\")\n\n    def toggle_sampler_stop_at_end(self, toggle:bool):\n        self.backend.set_sampler_stop_at_end(toggle)\n        print(f\"Stop Sampler At End status changed to:{toggle}\")\n\n    def set_alignment_mode(self, mode:str='infer') -&gt; None:\n        \"\"\"\n        Set alignment mode directly (used by OSC/MIDI)\n        \"\"\"\n        if mode in ['infer', 'paint']:\n            self.mode = mode\n            self._alignment_paint_toggle_action.setChecked((mode == 'paint'))\n            print(f\"Alignment Mode changed to: {mode}\")\n\n    def open_settings(self):\n        if self.settings_dialog is None:\n            self.settings_dialog = SettingsDialog(context=self)\n        self.settings_dialog.show() # use show() to display a modeless dialog\n\n    def serve_osc(self, address=(\"localhost\", 7777)):\n        self.osc_controller.unserve_osc()\n        self.osc_controller.serve_osc(address=address)\n\n    def unserve_osc(self):\n        self.osc_controller.unserve_osc()\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.add_temperature","title":"<code>add_temperature(temp)</code>","text":"<p>Add a small value to the inference temperature (used by OSC/MIDI)</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def add_temperature(self, temp:float) -&gt; None:\n    \"\"\"\n    Add a small value to the inference temperature (used by OSC/MIDI)\n\n    Args:\n        temp    value that will be added to inference temperature, usually small, temperature will clip at [0,2]\n    \"\"\"\n    self.set_temperature(self.temperature_slider.value() + temp)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.closeEvent","title":"<code>closeEvent(e)</code>","text":"<p>Cleanup</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def closeEvent(self, e:QtCore.QEvent):\n    \"\"\"\n    Cleanup\n    \"\"\"\n    # TODO: cleanup OSC/networking connections\n    print(f\"Application Close {e}\")\n    self.backend.cleanup()\n    e.accept()\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.encode_text","title":"<code>encode_text()</code>","text":"<p>Send input text to the text encoder backend</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def encode_text(self):\n    \"\"\"\n    Send input text to the text encoder backend\n    \"\"\"\n    if self.use_backend:\n        txtval = self.gen_text_input.toPlainText()\n        print(f\"Encoding: {txtval}\")\n        self.text_encoding_status.setText(\"\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad encoding.... \u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\")\n        # this returns the text with start/end tokens added and loop points stripped\n        self.backend.set_text(text=txtval)\n    else:\n        print(\"No backend enabled to encode text: ignoring...\")\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.finish_reset","title":"<code>finish_reset(text)</code>","text":"<p>Called when backend signals a reset.</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def finish_reset(self, text):\n    \"\"\"\n    Called when backend signals a reset.\n    \"\"\"\n    self.text_encoding_status.setText(f\"\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac encoded {len(text)} embeddings \u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\u02ac\u02ad\")\n    self.attention_graph.reset(text=text)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.loop_index","title":"<code>loop_index(**kw)</code>","text":"<p>Send input text to the sampler backend</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def loop_index(self, **kw):\n    \"\"\"\n    Send input text to the sampler backend\n    \"\"\"\n    if not self.use_backend:\n        print(\"No backend enabled to loop indices: ignoring...\")\n        return\n    self.backend.set_sampler_loop_index(**kw)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.loop_text","title":"<code>loop_text(**kw)</code>","text":"<p>Send input text to the sampler backend</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def loop_text(self, **kw):\n    \"\"\"\n    Send input text to the sampler backend\n    \"\"\"\n    if not self.use_backend:\n        print(\"No backend enabled to loop text: ignoring...\")\n        return\n    if 'text' not in kw:\n        kw['text'] = self.samp_text_input.toPlainText()\n    self.backend.set_sampler_loop_text(**kw)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.set_alignment_mode","title":"<code>set_alignment_mode(mode='infer')</code>","text":"<p>Set alignment mode directly (used by OSC/MIDI)</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_alignment_mode(self, mode:str='infer') -&gt; None:\n    \"\"\"\n    Set alignment mode directly (used by OSC/MIDI)\n    \"\"\"\n    if mode in ['infer', 'paint']:\n        self.mode = mode\n        self._alignment_paint_toggle_action.setChecked((mode == 'paint'))\n        print(f\"Alignment Mode changed to: {mode}\")\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.set_temperature","title":"<code>set_temperature(temp)</code>","text":"<p>Set inference temperature (used by OSC/MIDI)</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_temperature(self, temp:float) -&gt; None:\n    \"\"\"\n    Set inference temperature (used by OSC/MIDI)\n\n    Args:\n        temp  inference temperature from 0.0-2.0\n    \"\"\"\n    if temp &gt; 2.0:\n        temp=2.0\n    elif temp &lt; 0:\n        temp=0\n    self.temperature_slider.setValue(temp)\n    self.backend.set_temperature(temp)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.MainWindow.update","title":"<code>update()</code>","text":"<p>update method runs on a timer empties the queue from the backend and updates gui elements</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def update(self):\n    \"\"\"\n    update method runs on a timer\n    empties the queue from the backend and updates gui elements\n    \"\"\"\n    self.frame += 1\n    new_data = list()\n    # empty message queue, update attention graph &amp; RAVE latents\n    if self.use_backend: \n\n        if self.stress_gui is not None:\n            t = time.time_ns()\n            while time.time_ns() - t &lt; self.stress_gui*1e9:\n                pass\n\n        try:\n            if self.latents.is_init:\n                self.backend.set_biases(self.latents.get_biases())\n            if self.mode == 'paint':\n                self.backend.set_alignment(\n                    self.attention_graph.get_slidervalue_as_params())\n            elif self.mode == 'infer':\n                self.backend.set_alignment(None)\n            else:\n                raise ValueError(f\"Unknown attention mode: {self.mode} - must be &lt;infer|paint&gt;\")\n\n            # for _ in range(self.backend.frontend_q.qsize()):\n            while self.backend.parent_conn.poll():\n                try:\n                    # framedict = self.backend.frontend_q.get_nowait()\n                    framedict = self.backend.parent_conn.recv()\n                    if not self.latents.is_init and 'num_latents' in framedict and 'use_pitch' in framedict:\n                        self.latents._init(\n                            framedict['num_latents'], framedict['use_pitch'])\n\n                    new_data.append(framedict)\n                except queue.Empty as ex:\n                    break\n        except Exception as e:\n            print(e)\n            exit(0) ### debug\n\n    else: # Do not use backend, instead generate random data.. useful for testing the gui (maybe?)\n        new_attn_frame = np.zeros((1,self.attention_graph.num_encodings), dtype=np.float32)        \n        new_latent_frame = np.random.rand(1,8) * 0.5\n        new_latent_frame = new_latent_frame + self.latents.get_biases()\n        if self.mode == 'infer':\n            if random.random() &lt; 0.2:\n                self.tok += random.choice([-1, 1])\n        elif self.mode == 'paint':\n            self.tok = self.attention_graph.alignment_slider.value()            \n        else:\n            raise ValueError(f\"Unknown attention mode: {self.mode} - must be &lt;infer|paint&gt;\")\n\n        if self.tok &gt;= self.max_tok-1:\n            self.tok = self.max_tok - 1\n        else:\n            new_attn_frame[0,self.tok + 1] = 0.5\n        if self.tok &lt;= 0:\n            self.tok = 0\n        else:\n            new_attn_frame[0,self.tok - 1] = 0.5\n        new_attn_frame[0,self.tok] = 1.0\n\n        new_data.append({\n            'latent_t': new_latent_frame, 'align_t': new_attn_frame})\n\n    # iterate through new_data and update the gui\n    for datadict in new_data:\n        if datadict.get('reset', False):\n            # self.finish_reset(num_tokens=datadict['align_t'].shape[-1])\n            self.finish_reset(text=datadict['text'])\n        if 'align_t' in datadict and not datadict.get('sampler', False):\n            self.attention_graph.addFrame(\n                datadict['align_t'], datadict['text'])\n\n    latents = [d['latent_t'] for d in new_data if 'latent_t' in d]\n    if len(latents) and self.latents.is_init:\n        self.latents.set_latents(values=latents[-1])\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.OSCController","title":"<code>OSCController</code>","text":"<p>               Bases: <code>Thread</code></p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class OSCController(threading.Thread):\n\n    def __init__(self, context:'MainWindow', *args, **kwargs):\n        super(OSCController, self).__init__(*args, **kwargs)\n        self.context = context\n        self.osc_server = None\n        self.osc_dispatcher = None    \n\n        # BEGIN: OSC Dispatcher Callback Functions -----------------------------\n        def osc_unknown(addr:str, *args:list[typing.Any]) -&gt; None:\n            print(f\"Unknown OSC address: {addr}  with: '{args}'\")\n\n        def osc_generate(addr:str, reset:bool=True) -&gt; None:\n            \"\"\"Run/Play generator mode\n            reset   bool if true reset to beginning of utterance before playback (useful if generate_stop_at_end is active)\n            \"\"\"\n            # TODO: maybe adds some timing wierdness having to go through the QtGui thread for the reset..?\n            if reset:\n                self.context._reset_action.trigger()\n            self.context._generate_action.trigger()\n\n        def osc_sampler(addr:str, *args:list[typing.Any]) -&gt; None:\n            \"\"\"(trigger)Activate sampler mode\"\"\"\n            self.context._sampler_action.trigger()\n\n        def osc_pause(addr:str, *args:list[typing.Any]) -&gt; None:\n            \"\"\"(trigger)Pause generator/sampler playback\"\"\"\n            self.context._pause_action.trigger()\n\n        def osc_reset(addr:str, *args:list[typing.Any]) -&gt; None:\n            \"\"\"(trigger)Reset generator/sampler playback\"\"\"\n            self.context._reset_action.trigger()\n\n        def osc_set_bias(addr:str, latent:int, bias:float) -&gt; None:\n            \"\"\"Set vocoder latent bias\n            latent      int index of latent\n            bias        float bias amount (usually not larger than +-3.0)\n\n            \"\"\"\n            self.context.latents.set_latent_bias(latent, bias)\n            print(f\"Set latent {latent} bias: {bias}\")\n\n        def osc_add_bias(addr:str, latent:int, bias:float) -&gt; None:\n            \"\"\"Set add a value to the current vocoder latent bias\n            latent      int index of latent\n            bias        float amount to add to bias (usually a small value, will clip if larger than +-3.0)\n\n            \"\"\"\n            self.context.latents.add_latent_bias(latent, bias)\n            print(f\"Add {bias} to latent {latent} bias\")\n\n\n        def osc_reset_biases(addr:str) -&gt; None:\n            \"\"\"Reset all biases to 0.0\"\"\"\n            self.context.latents.reset_latent_biases()\n            print(f\"Reset latent biases to 0.0\")\n\n\n        def osc_generate_stop_at_end(addr:str, enable:bool) -&gt; None:\n            \"\"\" Enable/disable Generator stop-at-end of utterance\n            enable     bool true or false\n            \"\"\"\n            self.context._generate_stop_at_end_toggle_action.setChecked(enable)\n\n        def osc_alignment_mode(addr:str, mode:str) -&gt; None:\n            \"\"\"Set alignment generation mode (Generator only)\n            mode    str 'infer' for alignment inference, 'paint' for alignment painting\n            \"\"\"\n            self.context.set_alignment_mode(mode)\n\n        def osc_latent_feedback(addr:str, enable:bool) -&gt; None:\n            \"\"\"Enable/disable latent feedback mode(Generator only)\n            enable     bool true or false\n            \"\"\"\n            self.context._latent_feedback_toggle_action.setChecked(enable)\n\n\n        def osc_set_gen_text(addr:str, text:str, encode:bool=True) -&gt; None:\n            \"\"\"Set generator utterance text in gui textbox (Generator only)\n            text        str new generate utterance text\n            encode      bool if true, automatically encode the new text utterance\n            \"\"\"\n            print(f\"OSCfunc: set_gen_text {addr} | {text=} | {encode=}\")\n            # NOTE: One of the main gotcha's about Qt is that you cannot call any QWidget methods from any \n            # thread other than the main GUI thread. All of your communication must be done by emitting signals \n            # from the extra threads, which will forward to the main gui. So the following line will not work!\n            # self.text_input.setText(text)\n            signaller.set_gen_text.emit(text)\n\n            if encode:\n                print(\"ENCODE TEXT\")\n                self.context.btn_send_gen_text.click()\n\n        def osc_set_alignment_as_token_idx(addr:str, tok_idx:int, force_paint:bool=False) -&gt; None:\n            \"\"\"Set alignment of generator by token index (Generator Only)\n            token_idx           int index of token\n            force_paint         bool if true, toggles on attention painting\n            \"\"\"\n            self.context.attention_graph.set_alignment_as_token_idx(tok_idx)\n            print(f\"Set alignment to token {tok_idx} - forced paint?: {force_paint}\")\n\n        def osc_set_alignment_normalized(addr:str, normalized_align:float, force_paint:bool=False) -&gt; None:\n            \"\"\"Set alignment of generator by a normalized 0.0-1.0 value (Generator Only)\n            normalized_align    float alignment value from 0-1 gets mapped to start-end token range\n            force_paint         bool if true, toggles on attention painting\n            \"\"\"\n            self.context.attention_graph.set_normalized_alignment(normalized_align)\n            print(f\"Set normalized alignment {normalized_align} - forced paint?: {force_paint}\")\n\n        def osc_set_temperature(addr:str, temp:float) -&gt; None:\n            \"\"\"Set generator sampling temperature (Generator Only)\n            temp    float temperature from 0.0-1.0 (can go up to 2.0 for more weird predictions)\n\n            \"\"\"\n            self.context.set_temperature(temp)\n            print(f\"Set sampling temp {temp}\")\n\n        def osc_add_temperature(addr:str, temp:float) -&gt; None:\n            \"\"\"Add a small value to the generator sampling temperature (Generator Only)\n            temp    float value to add to temperature (usually below 1.0) - will clip at 0 and a max value\n\n            \"\"\"\n            self.context.add_temperature(temp)\n            print(f\"Add {temp} to sampling temp\")\n\n        def osc_sampler_stop_at_end(addr:str, enable:bool) -&gt; None:\n            \"\"\" Enable/disable Sampler stop-at-end loop point\n            enable     bool true or false\n            \"\"\"\n            self.context._sampler_stop_at_end_toggle_action.setChecked(enable)\n\n        def osc_set_sampler_step(addr:str, step:int, autoplay:bool=True):\n            \"\"\"Set sampler playhead absolute position (Sampler only)\n            step        int absolute index of vocoder frame buffer to set sampler playhead, index wraps and can be negative\n            autoplay    bool if true, sampler playback is triggered if it is not already playing\n            \"\"\"\n            print(f\"OSCfunc: set_sampler_step {addr} | {step=}\")\n            self.context.sampler_step(step=step, autoplay=autoplay)\n\n        def osc_set_loop_text(addr:str, \n                text:str, n:int=-1, \n                start:bool=True, end:bool=True, reset:bool=True\n                ) -&gt; None:\n            \"\"\"Set sampler loop points to nth occurrence of matched text (Sampler only)\n            text            str text to match in sampler history, can be a single token\n            n               int which of the n text matches to loop, -1 is most recent, 0 oldest, etc..\n            start           bool if true, updates the sampler loop start at the matched text\n            end             bool if true, updates the sampler loop end at the matched text\n            reset           bool if true resets sampler playhead to start of text match        \n            \"\"\"\n            print(f\"OSCfunc: set_loop_text {addr} | {text=} | {n=} | {start=} | {end=} | {reset=}\")\n            signaller.set_samp_text.emit(text)\n\n            print(\"SET LOOP\")\n            self.context.loop_text(text=text, n=n, start=start, end=end, reset=reset)\n\n        def osc_set_loop_index(addr:str, \n                start:int=None, end:int=None, \n                reset:bool=True,\n                utterance:int=None,\n                ) -&gt; None:\n            \"\"\"Set sampler loop points by vocoder frame with option to index by utterance (Sampler only)\n            start           int global start frame index\n            end             int global end frame index\n            reset           bool if true resets sampler playhead to start index\n            utterance       global utterance index, if supplied changes start and end to be utterance-relative\n\n\n            \"\"\"\n            print(f\"OSCfunc: set_loop_index {addr} | {start=} | {end=} | {reset=} | {utterance=}\")\n            print(\"SET LOOP\")\n            self.context.loop_index(start=start, end=end, utterance=utterance, reset=reset)\n\n\n        # END:: OSC Dispatcher Callback Functions -----------------------------------------------------------------------\n\n\n        self.osc_dispatcher = pythonosc.dispatcher.Dispatcher()\n        self.osc_dispatcher.set_default_handler(osc_unknown)\n\n        # OSC Callback mappings\n        self.osc_dispatcher.map(\"/generate\", osc_generate) # TODO rename\n        self.osc_dispatcher.map(\"/sampler\", osc_sampler)\n        self.osc_dispatcher.map(\"/pause\", osc_pause)\n        self.osc_dispatcher.map(\"/reset\", osc_reset)\n\n        self.osc_dispatcher.map(\"/set_bias\", osc_set_bias)\n        self.osc_dispatcher.map(\"/add_bias\", osc_add_bias)\n        self.osc_dispatcher.map(\"/reset_biases\", osc_reset_biases)\n\n        self.osc_dispatcher.map(\"/generate_stop_at_end\", osc_generate_stop_at_end)\n        self.osc_dispatcher.map(\"/alignment_mode\", osc_alignment_mode)\n        self.osc_dispatcher.map(\"/latent_feedback\", osc_latent_feedback)\n        self.osc_dispatcher.map(\"/set_gen_text\", osc_set_gen_text)\n        self.osc_dispatcher.map(\"/set_token\", osc_set_alignment_as_token_idx)\n        self.osc_dispatcher.map(\"/set_alignment\", osc_set_alignment_normalized)\n        self.osc_dispatcher.map(\"/set_temperature\", osc_set_temperature)\n        self.osc_dispatcher.map(\"/add_temperature\", osc_add_temperature)\n\n        self.osc_dispatcher.map(\"/sampler_stop_at_end\", osc_sampler_stop_at_end)\n        self.osc_dispatcher.map(\"/set_sampler_step\", osc_set_sampler_step)\n        self.osc_dispatcher.map(\"/set_loop_text\", osc_set_loop_text)\n        self.osc_dispatcher.map(\"/set_loop_index\", osc_set_loop_index)\n\n\n\n    def run(self):\n        # Thread entry point\n        print(f\"Serving on {self.osc_server.server_address}\")\n        self.osc_server.serve_forever()\n        print(f\"Closing OSC Server...\")\n\n        # Need to update the GUI\n        # signaller.set_text.emit(\"OSC Server Listening\")\n\n\n    def unserve_osc(self):\n        if self.osc_server is not None:\n            self.osc_server.shutdown()\n            self.osc_server.server_close()\n            self.osc_server = None\n            print(\"Waiting for server to shutdown...\")\n            #self.osc_server_thread.join()\n            print(\"Server Thread closed...\")\n\n    def serve_osc(self, address=(\"localhost\", 7777)):\n\n        if self.osc_server is not None:\n            self.unserve_osc()\n\n        self.osc_server = pythonosc.osc_server.ThreadingOSCUDPServer(address, self.osc_dispatcher)\n        self.daemon = True\n        #self.osc_server_thread = threading.Thread(target=run_osc_server, args=(self,), daemon=True)\n        self.start()\n        print(f\"OSC Server Thread Started\")\n\n\n    # TODO can be improved to be co-generated together with dispatcher mappings\n    def print_osc_api(self, basepath='/tungnaa'):\n        instructions=\"\"\n\n        # TODO can be improved, this approach depends on undocumented internals of pythonosc.dispatcher\n        if self.osc_dispatcher is not None:\n            instructions += \"OSC Control API:\\n\"\n            for addr in self.osc_dispatcher._map.keys():\n                handler = self.osc_dispatcher._map[addr][0] # take first available handler, although multiple are possible\n                print(f\"{addr} - {handler}\")\n                instructions += f\"{addr} - {handler.callback.__doc__} \\n\" \n\n        instructions += f\"\"\"OSC Out API:\n        {basepath}/latents\n        {basepath}/status\"\"\"\n        return instructions\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.OSCController.__init__","title":"<code>__init__(context, *args, **kwargs)</code>","text":"Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def __init__(self, context:'MainWindow', *args, **kwargs):\n    super(OSCController, self).__init__(*args, **kwargs)\n    self.context = context\n    self.osc_server = None\n    self.osc_dispatcher = None    \n\n    # BEGIN: OSC Dispatcher Callback Functions -----------------------------\n    def osc_unknown(addr:str, *args:list[typing.Any]) -&gt; None:\n        print(f\"Unknown OSC address: {addr}  with: '{args}'\")\n\n    def osc_generate(addr:str, reset:bool=True) -&gt; None:\n        \"\"\"Run/Play generator mode\n        reset   bool if true reset to beginning of utterance before playback (useful if generate_stop_at_end is active)\n        \"\"\"\n        # TODO: maybe adds some timing wierdness having to go through the QtGui thread for the reset..?\n        if reset:\n            self.context._reset_action.trigger()\n        self.context._generate_action.trigger()\n\n    def osc_sampler(addr:str, *args:list[typing.Any]) -&gt; None:\n        \"\"\"(trigger)Activate sampler mode\"\"\"\n        self.context._sampler_action.trigger()\n\n    def osc_pause(addr:str, *args:list[typing.Any]) -&gt; None:\n        \"\"\"(trigger)Pause generator/sampler playback\"\"\"\n        self.context._pause_action.trigger()\n\n    def osc_reset(addr:str, *args:list[typing.Any]) -&gt; None:\n        \"\"\"(trigger)Reset generator/sampler playback\"\"\"\n        self.context._reset_action.trigger()\n\n    def osc_set_bias(addr:str, latent:int, bias:float) -&gt; None:\n        \"\"\"Set vocoder latent bias\n        latent      int index of latent\n        bias        float bias amount (usually not larger than +-3.0)\n\n        \"\"\"\n        self.context.latents.set_latent_bias(latent, bias)\n        print(f\"Set latent {latent} bias: {bias}\")\n\n    def osc_add_bias(addr:str, latent:int, bias:float) -&gt; None:\n        \"\"\"Set add a value to the current vocoder latent bias\n        latent      int index of latent\n        bias        float amount to add to bias (usually a small value, will clip if larger than +-3.0)\n\n        \"\"\"\n        self.context.latents.add_latent_bias(latent, bias)\n        print(f\"Add {bias} to latent {latent} bias\")\n\n\n    def osc_reset_biases(addr:str) -&gt; None:\n        \"\"\"Reset all biases to 0.0\"\"\"\n        self.context.latents.reset_latent_biases()\n        print(f\"Reset latent biases to 0.0\")\n\n\n    def osc_generate_stop_at_end(addr:str, enable:bool) -&gt; None:\n        \"\"\" Enable/disable Generator stop-at-end of utterance\n        enable     bool true or false\n        \"\"\"\n        self.context._generate_stop_at_end_toggle_action.setChecked(enable)\n\n    def osc_alignment_mode(addr:str, mode:str) -&gt; None:\n        \"\"\"Set alignment generation mode (Generator only)\n        mode    str 'infer' for alignment inference, 'paint' for alignment painting\n        \"\"\"\n        self.context.set_alignment_mode(mode)\n\n    def osc_latent_feedback(addr:str, enable:bool) -&gt; None:\n        \"\"\"Enable/disable latent feedback mode(Generator only)\n        enable     bool true or false\n        \"\"\"\n        self.context._latent_feedback_toggle_action.setChecked(enable)\n\n\n    def osc_set_gen_text(addr:str, text:str, encode:bool=True) -&gt; None:\n        \"\"\"Set generator utterance text in gui textbox (Generator only)\n        text        str new generate utterance text\n        encode      bool if true, automatically encode the new text utterance\n        \"\"\"\n        print(f\"OSCfunc: set_gen_text {addr} | {text=} | {encode=}\")\n        # NOTE: One of the main gotcha's about Qt is that you cannot call any QWidget methods from any \n        # thread other than the main GUI thread. All of your communication must be done by emitting signals \n        # from the extra threads, which will forward to the main gui. So the following line will not work!\n        # self.text_input.setText(text)\n        signaller.set_gen_text.emit(text)\n\n        if encode:\n            print(\"ENCODE TEXT\")\n            self.context.btn_send_gen_text.click()\n\n    def osc_set_alignment_as_token_idx(addr:str, tok_idx:int, force_paint:bool=False) -&gt; None:\n        \"\"\"Set alignment of generator by token index (Generator Only)\n        token_idx           int index of token\n        force_paint         bool if true, toggles on attention painting\n        \"\"\"\n        self.context.attention_graph.set_alignment_as_token_idx(tok_idx)\n        print(f\"Set alignment to token {tok_idx} - forced paint?: {force_paint}\")\n\n    def osc_set_alignment_normalized(addr:str, normalized_align:float, force_paint:bool=False) -&gt; None:\n        \"\"\"Set alignment of generator by a normalized 0.0-1.0 value (Generator Only)\n        normalized_align    float alignment value from 0-1 gets mapped to start-end token range\n        force_paint         bool if true, toggles on attention painting\n        \"\"\"\n        self.context.attention_graph.set_normalized_alignment(normalized_align)\n        print(f\"Set normalized alignment {normalized_align} - forced paint?: {force_paint}\")\n\n    def osc_set_temperature(addr:str, temp:float) -&gt; None:\n        \"\"\"Set generator sampling temperature (Generator Only)\n        temp    float temperature from 0.0-1.0 (can go up to 2.0 for more weird predictions)\n\n        \"\"\"\n        self.context.set_temperature(temp)\n        print(f\"Set sampling temp {temp}\")\n\n    def osc_add_temperature(addr:str, temp:float) -&gt; None:\n        \"\"\"Add a small value to the generator sampling temperature (Generator Only)\n        temp    float value to add to temperature (usually below 1.0) - will clip at 0 and a max value\n\n        \"\"\"\n        self.context.add_temperature(temp)\n        print(f\"Add {temp} to sampling temp\")\n\n    def osc_sampler_stop_at_end(addr:str, enable:bool) -&gt; None:\n        \"\"\" Enable/disable Sampler stop-at-end loop point\n        enable     bool true or false\n        \"\"\"\n        self.context._sampler_stop_at_end_toggle_action.setChecked(enable)\n\n    def osc_set_sampler_step(addr:str, step:int, autoplay:bool=True):\n        \"\"\"Set sampler playhead absolute position (Sampler only)\n        step        int absolute index of vocoder frame buffer to set sampler playhead, index wraps and can be negative\n        autoplay    bool if true, sampler playback is triggered if it is not already playing\n        \"\"\"\n        print(f\"OSCfunc: set_sampler_step {addr} | {step=}\")\n        self.context.sampler_step(step=step, autoplay=autoplay)\n\n    def osc_set_loop_text(addr:str, \n            text:str, n:int=-1, \n            start:bool=True, end:bool=True, reset:bool=True\n            ) -&gt; None:\n        \"\"\"Set sampler loop points to nth occurrence of matched text (Sampler only)\n        text            str text to match in sampler history, can be a single token\n        n               int which of the n text matches to loop, -1 is most recent, 0 oldest, etc..\n        start           bool if true, updates the sampler loop start at the matched text\n        end             bool if true, updates the sampler loop end at the matched text\n        reset           bool if true resets sampler playhead to start of text match        \n        \"\"\"\n        print(f\"OSCfunc: set_loop_text {addr} | {text=} | {n=} | {start=} | {end=} | {reset=}\")\n        signaller.set_samp_text.emit(text)\n\n        print(\"SET LOOP\")\n        self.context.loop_text(text=text, n=n, start=start, end=end, reset=reset)\n\n    def osc_set_loop_index(addr:str, \n            start:int=None, end:int=None, \n            reset:bool=True,\n            utterance:int=None,\n            ) -&gt; None:\n        \"\"\"Set sampler loop points by vocoder frame with option to index by utterance (Sampler only)\n        start           int global start frame index\n        end             int global end frame index\n        reset           bool if true resets sampler playhead to start index\n        utterance       global utterance index, if supplied changes start and end to be utterance-relative\n\n\n        \"\"\"\n        print(f\"OSCfunc: set_loop_index {addr} | {start=} | {end=} | {reset=} | {utterance=}\")\n        print(\"SET LOOP\")\n        self.context.loop_index(start=start, end=end, utterance=utterance, reset=reset)\n\n\n    # END:: OSC Dispatcher Callback Functions -----------------------------------------------------------------------\n\n\n    self.osc_dispatcher = pythonosc.dispatcher.Dispatcher()\n    self.osc_dispatcher.set_default_handler(osc_unknown)\n\n    # OSC Callback mappings\n    self.osc_dispatcher.map(\"/generate\", osc_generate) # TODO rename\n    self.osc_dispatcher.map(\"/sampler\", osc_sampler)\n    self.osc_dispatcher.map(\"/pause\", osc_pause)\n    self.osc_dispatcher.map(\"/reset\", osc_reset)\n\n    self.osc_dispatcher.map(\"/set_bias\", osc_set_bias)\n    self.osc_dispatcher.map(\"/add_bias\", osc_add_bias)\n    self.osc_dispatcher.map(\"/reset_biases\", osc_reset_biases)\n\n    self.osc_dispatcher.map(\"/generate_stop_at_end\", osc_generate_stop_at_end)\n    self.osc_dispatcher.map(\"/alignment_mode\", osc_alignment_mode)\n    self.osc_dispatcher.map(\"/latent_feedback\", osc_latent_feedback)\n    self.osc_dispatcher.map(\"/set_gen_text\", osc_set_gen_text)\n    self.osc_dispatcher.map(\"/set_token\", osc_set_alignment_as_token_idx)\n    self.osc_dispatcher.map(\"/set_alignment\", osc_set_alignment_normalized)\n    self.osc_dispatcher.map(\"/set_temperature\", osc_set_temperature)\n    self.osc_dispatcher.map(\"/add_temperature\", osc_add_temperature)\n\n    self.osc_dispatcher.map(\"/sampler_stop_at_end\", osc_sampler_stop_at_end)\n    self.osc_dispatcher.map(\"/set_sampler_step\", osc_set_sampler_step)\n    self.osc_dispatcher.map(\"/set_loop_text\", osc_set_loop_text)\n    self.osc_dispatcher.map(\"/set_loop_index\", osc_set_loop_index)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.Proxy","title":"<code>Proxy</code>","text":"<p>this is a somewhat of a hack to try running the backend in a separate process with otherwise minimal changes.</p> <p>wrapping the Backend object in a Proxy allows the frontend to asynchronously call methods on the backend (but they don't return)</p> <p>all communication from the backend to the frontend is through a Pipe, which works basically the same as it was previously with a Queue.</p> <p>it's not possible to read attributes of the backend.</p> <p>to me this design is suggesting to use asyncio to allow backend calls to return asynchronously when needed, and the frontend to use <code>await</code> when convenient.</p> <p>IPyC (https://pypi.org/project/IPyC/) might be helpful there.</p> <p>alternatively, Futures as in concurrent.futures (https://docs.python.org/3/library/concurrent.futures.htm) could be useful -- but I'm unsure whether ProcessPoolExecutor is a good fit for calling methods of one long-lived object in another process.</p> <p>However, this pattern might turn out to work fine -- the frontend  passes data/triggers to the backend asynchronously, and the backend streams state updates to the frontend via the Pipe.</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class Proxy:\n    \"\"\"\n    this is a somewhat of a hack to try running the backend in a separate\n    process with otherwise minimal changes.\n\n    wrapping the Backend object in a Proxy allows the frontend to asynchronously call methods on the backend (but they don't return)\n\n    all communication from the backend to the frontend is through a Pipe,\n    which works basically the same as it was previously with a Queue.\n\n    it's not possible to read attributes of the backend.\n\n    to me this design is suggesting to use asyncio to allow backend calls to\n    return asynchronously when needed, and the frontend to use `await` when convenient.\n\n    IPyC (https://pypi.org/project/IPyC/) might be helpful there.\n\n    alternatively, Futures as in concurrent.futures (https://docs.python.org/3/library/concurrent.futures.htm) could be useful -- but I'm unsure whether ProcessPoolExecutor is a good fit for calling methods of one long-lived object in another process.\n\n    However, this pattern might turn out to work fine -- the frontend \n    passes data/triggers to the backend asynchronously, and the backend\n    streams state updates to the frontend via the Pipe.\n    \"\"\"\n    def __init__(self, obj):\n        ctx = mp.get_context('spawn')\n        # backend object\n        self.obj = obj\n        # parent and child process ends of the pipe\n        self.parent_conn, child_conn = ctx.Pipe()\n        # child process eventually sets this to true and sends it through the pipe\n        self.running = False\n        # run in a new process\n        self.proc = ctx.Process(target=self._run, args=(child_conn,))\n        self.proc.start()\n\n    def __getattr__(self, name):\n        # print(self.__dict__)\n        if name in self.__dict__:\n            return self.__dict__[name]\n        elif len(self.__dict__):\n            if not hasattr(self.obj, name):\n                # this is checking the parent process copy of obj, meaning\n                # it can only find attributes which exist before the fork\n                raise AttributeError\n\n            if not self.running:\n                # first value received from the backend process says it has started\n                # if self.parent_conn.poll():\n                try:\n                    assert self.parent_conn.recv()\n                except AssertionError:\n                    print(\"\"\"backend process failed to start\"\"\")\n                    raise\n                self.running = True\n                # else:\n                    # return lambda *a, **kw: print(f'skipping {name} (backend not running yet)')\n\n            attr = getattr(self.obj, name)\n            if callable(attr):\n                # return a function which when called, defers onto the backend process\n                def f(*a, **kw):\n                    # print(name)\n                    self.parent_conn.send([name, a, kw])\n                return f\n            else:\n                print(\"\"\"backend attribute not callable\"\"\")\n\n            raise NotImplementedError\n        else:\n            # this case allows unpickling\n            raise AttributeError\n\n    def _run(self, conn):\n        print('======================BACKEND RUN======================')\n        self.obj.run(conn)\n        self.running = True\n        conn.send(self.running)\n\n        while True:\n            # call function in backend process asynchronously (no return)\n            name, a, kw = conn.recv()\n            # print(name)\n            getattr(self.obj, name)(*a, **kw)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.RaveLatents","title":"<code>RaveLatents</code>","text":"<p>               Bases: <code>QWidget</code></p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>class RaveLatents(QtWidgets.QWidget):\n    def __init__(self, parent:QtWidgets.QWidget=None):\n        super().__init__(parent)\n\n        self.latents = list()\n        self.layout = QtWidgets.QHBoxLayout(self)\n        self.is_init = False\n\n    def _init(self, num_latents, pitch_slider):\n        for idx in range(num_latents):\n            if idx==0 and pitch_slider:\n                bmin, bmax = -100., 100.\n                vmin, vmax = 50., 550.\n            else:\n                bmin, bmax = -3., 3.\n                vmin, vmax = -5., 5.\n\n            # latent widgets, each is a (SliderWidget, MeterWidget) pair\n            # the first slide is bias, second is a display\n            bias_slider = DoubleSlider(decimals=3, parent=self)\n            bias_slider.setMaximum(bmax)\n            bias_slider.setMinimum(bmin)\n            bias_slider.setValue(0.0)\n            bias_slider.doubleValueChanged.connect(\n                lambda val,latent=idx: self._bias_adjust(val, latent))\n            bias_slider.setStatusTip(f\"bias latent dimension {idx}\")\n\n            value_meter = DoubleSlider(decimals=3, parent=self)\n            value_meter.setMaximum(vmax)\n            value_meter.setMinimum(vmin)\n            value_meter.setValue(0.0)\n            value_meter.setStyleSheet(\"\"\"\n                QSlider::groove:vertical {\n                    background-color: black;\n                    width: 10px;\n                    border-radius: 5;\n                }\n\n                QSlider::add-page:vertical {\n                    background: pink;\n                    border-radius: 5;\n                }\n                QSlider::sub-page:vertical {\n                    background: black;\n                    border-radius: 5;\n                }\n                \"\"\")\n\n            self.latents.append((bias_slider, value_meter))\n            self.layout.addWidget(bias_slider)\n            self.layout.addWidget(value_meter)\n            if idx &lt; num_latents-1:\n                self.layout.addSpacing(12)\n            self.layout.setSpacing(4)\n\n        self.setMaximumHeight(200)\n\n        self.is_init = True\n\n    def _bias_adjust(self, val:float, latent:int):            \n        # NOTE: The bias values of the sliders get sampled regularly in `update`, they are read by get_biases \n        print(f\"ADJUST BIAS of LATENT {latent} = {val}\")\n\n    def get_bias(self, latent:int) -&gt; float:\n        return self.latents[latent][0].value()\n\n    def get_biases(self) -&gt; typing.List[float]:\n    # def get_biases(self) -&gt; npt.NDArray[np.float32]:\n        # res = np.zeros((1,len(self.latents)), dtype=np.float32)\n        # for idx, (bias, _) in enumerate(self.latents):\n            # res[0][idx] = bias.value()\n        # return res\n        return [bias.value() for bias,_ in self.latents]\n\n    def set_latent_bias(self, latent:int, value:float):\n        \"\"\"\n        Set a latent bias value in the GUI\n        \"\"\"\n        self.latents[latent][0].setValue(value)\n\n    def add_latent_bias(self, latent:int, value:float):\n        \"\"\"\n        Add a small value to the current bias value of a latent in the GUI\n        \"\"\"\n        newval = self.latents[latent][0].value() + value;\n        self.latents[latent][0].setValue(newval)\n\n    def reset_latent_biases(self, value:float=0.0):\n        \"\"\"\n        Reset all latent biases to 0.0\n        \"\"\"\n        for bias,_ in self.latents:\n            bias.setValue(value);\n\n    def set_latents(self, values:typing.Union[list, npt.ArrayLike]):\n        \"\"\"\n        Set latent values in the gui. \n        values &gt; the number of sliders are ignored\n\n        values  latent values as floats, in the shape (1, num_latents)\n        \"\"\"\n        values = values[0] # trim off the extra dimension\n        if len(values) &lt;= len(self.latents):\n            for idx,val in enumerate(values):\n                self.latents[idx][1].setValue(val)\n        else: # More values than sliders\n            for idx,sliders in enumerate(self.latents):\n                sliders[1].setValue(values[idx])\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.RaveLatents.add_latent_bias","title":"<code>add_latent_bias(latent, value)</code>","text":"<p>Add a small value to the current bias value of a latent in the GUI</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def add_latent_bias(self, latent:int, value:float):\n    \"\"\"\n    Add a small value to the current bias value of a latent in the GUI\n    \"\"\"\n    newval = self.latents[latent][0].value() + value;\n    self.latents[latent][0].setValue(newval)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.RaveLatents.reset_latent_biases","title":"<code>reset_latent_biases(value=0.0)</code>","text":"<p>Reset all latent biases to 0.0</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def reset_latent_biases(self, value:float=0.0):\n    \"\"\"\n    Reset all latent biases to 0.0\n    \"\"\"\n    for bias,_ in self.latents:\n        bias.setValue(value);\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.RaveLatents.set_latent_bias","title":"<code>set_latent_bias(latent, value)</code>","text":"<p>Set a latent bias value in the GUI</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_latent_bias(self, latent:int, value:float):\n    \"\"\"\n    Set a latent bias value in the GUI\n    \"\"\"\n    self.latents[latent][0].setValue(value)\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.RaveLatents.set_latents","title":"<code>set_latents(values)</code>","text":"<p>Set latent values in the gui.  values &gt; the number of sliders are ignored</p> <p>values  latent values as floats, in the shape (1, num_latents)</p> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def set_latents(self, values:typing.Union[list, npt.ArrayLike]):\n    \"\"\"\n    Set latent values in the gui. \n    values &gt; the number of sliders are ignored\n\n    values  latent values as floats, in the shape (1, num_latents)\n    \"\"\"\n    values = values[0] # trim off the extra dimension\n    if len(values) &lt;= len(self.latents):\n        for idx,val in enumerate(values):\n            self.latents[idx][1].setValue(val)\n    else: # More values than sliders\n        for idx,sliders in enumerate(self.latents):\n            sliders[1].setValue(values[idx])\n</code></pre>"},{"location":"reference/tungnaa/gui/qtgui/#tungnaa.gui.qtgui.main","title":"<code>main(tts='tungnaa_119_vctk', vocoder=None, repo='Intelligent-Instruments-Lab/tungnaa-models-public', synth_audio=True, latent_audio=False, latent_osc=False, audio_out=None, audio_block=2048, osc_out_addr='localhost:57120', osc_out_path='/tungnaa', osc_out_scsynth=False, osc_in_addr='localhost:1337', no_backend=False, buffer_frames=1, stress_gui=None, jit=False, profile=False, text=None, sampler_text=None)</code>","text":"<p>Tungnaa Text to Voice</p> <p>Parameters:</p> Name Type Description Default <code>tts</code> <code>Path</code> <p>path to TTS model, or name of model in repo</p> <code>'tungnaa_119_vctk'</code> <code>vocoder</code> <code>Path</code> <p>path to vocoder model</p> <code>None</code> <code>repo</code> <p>huggingface repo to search for models</p> <code>'Intelligent-Instruments-Lab/tungnaa-models-public'</code> <code>synth_audio</code> <code>bool</code> <p>send stereo audio from Python</p> <code>True</code> <code>latent_audio</code> <code>bool</code> <p>pack latents into a mono audio signal,  to be unpacked into multiple signals and decoded elsewhere</p> <code>False</code> <code>latent_osc</code> <code>bool</code> <p>send latents over OSC,  to be converted to audio signals and decoded elsewhere</p> <code>False</code> <code>audio_out</code> <code>Union[str, int]</code> <p>'default', or audio output device number (see 'tungnaa device' for a list of devices)</p> <code>None</code> <code>audio_block</code> <code>int</code> <p>audio block size in samples</p> <code>2048</code> <code>osc_out_addr</code> <code>str</code> <p>host:port to send OSC data to.  By default sends to localhost:57120</p> <code>'localhost:57120'</code> <code>osc_out_path</code> <code>str</code> <p>The OSC path prefix of messages sent by this app.  Uses /tungnaa by default.</p> <code>'/tungnaa'</code> <code>osc_out_scsynth</code> <code>bool</code> <p>format OSC for sending directly to scsynth</p> <code>False</code> <code>osc_in_addr</code> <code>str</code> <p>host:port for recieving OSC control messages from clients.  By default listens on localhost:1337</p> <code>'localhost:1337'</code> <code>buffer_frames</code> <code>int</code> <p>compute ahead this many model steps increases latency, but may mitigate dropouts</p> <code>1</code> <code>stress_gui</code> <code>float</code> <p>for testing, add this many seconds delay in <code>update</code></p> <code>None</code> Source code in <code>src/tungnaa/gui/qtgui.py</code> <pre><code>def main(\n    # models\n    tts:Path='tungnaa_119_vctk',\n    vocoder:Path=None,\n    repo='Intelligent-Instruments-Lab/tungnaa-models-public',\n    # output modes\n    synth_audio:bool=True,\n    latent_audio:bool=False,\n    latent_osc:bool=False,\n    # audio driver\n    audio_out:typing.Union[str,int]=None,\n    audio_block:int=2048,\n    # OSC\n    osc_out_addr:str='localhost:57120',\n    osc_out_path:str='/tungnaa',\n    osc_out_scsynth:bool=False,\n    osc_in_addr:str='localhost:1337',\n    # misc\n    no_backend:bool=False,\n    buffer_frames:int=1,\n    stress_gui:float=None,\n    jit:bool=False,\n    profile:bool=False,\n    text:str|None=None,\n    sampler_text:str|None=None,\n):\n    \"\"\"Tungnaa Text to Voice\n\n    Args:\n        tts: path to TTS model, or name of model in repo\n        vocoder: path to vocoder model\n        repo: huggingface repo to search for models\n\n        synth_audio: send stereo audio from Python\n        latent_audio: pack latents into a mono audio signal, \n            to be unpacked into multiple signals and decoded elsewhere\n        latent_osc: send latents over OSC, \n            to be converted to audio signals and decoded elsewhere\n\n        audio_out: 'default', or audio output device number (see 'tungnaa device' for a list of devices)\n        audio_block: audio block size in samples\n\n        osc_out_addr: host:port to send OSC data to. \n            By default sends to localhost:57120\n        osc_out_path: The OSC path prefix of messages sent by this app. \n            Uses /tungnaa by default.\n        osc_out_scsynth: format OSC for sending directly to scsynth\n        osc_in_addr: host:port for recieving OSC control messages from clients.   \n            By default listens on localhost:1337\n        buffer_frames: compute ahead this many model steps\n            increases latency, but may mitigate dropouts\n        stress_gui: for testing, add this many seconds delay in `update`\n\n    \"\"\"\n\n    if audio_out == 'default':\n        audio_out = None\n    if audio_out is not None:\n        if isinstance(audio_out, str) and audio_out.isdecimal():\n            audio_out = int(audio_out)\n        elif not isinstance(audio_out, int):\n            raise TypeError(f\"Invalid audio device id '{audio_out}', must be either an integer or 'default'\")\n\n    app = QtWidgets.QApplication(sys.argv)\n    QtCore.QCoreApplication.setOrganizationName(\"Intelligent Instruments Lab\")\n    QtCore.QCoreApplication.setApplicationName(\"Tungnaa\")\n    QtCore.QCoreApplication.setApplicationVersion(QtCore.qVersion())\n\n    if latent_osc:\n        if osc_out_scsynth: # Send latents to scsynth via OSC using default scsynth address localhost:57110\n            sender = tungnaa.gui.senders.SCSynthDirectOSCSender(\n                host='127.0.0.1',\n                port=57110,\n                bus_index=64,\n                latency=0.2\n            )\n        else: # Send latents out via Generic OSC\n            osc_host, osc_port = osc_out_addr.split(':')\n            osc_port = int(osc_port)\n            sender = tungnaa.gui.senders.GenericOSCSender(\n                host=osc_host, \n                port=osc_port, \n                lroute=f'{osc_out_path}/latents',\n                sroute=f'{osc_out_path}/status'\n            )\n    else:\n        sender = None\n\n    # if tts is not a local file, download model from repo\n    # also sets the vocoder unless it it explicitly set to something else\n    try:\n        with open(tts): pass\n    except FileNotFoundError:\n        print(f'searching remote repo for model \"{tts}\"...')\n        tts, vocoder = dl_model(repo, tts, vocoder)\n\n    backend = tungnaa.gui.backend.Backend(\n        checkpoint=tts, \n        rave_path=vocoder,\n        audio_out=audio_out,\n        audio_block=audio_block,\n        # audio_channels=audio_channels,\n        synth_audio=synth_audio,\n        latent_audio=latent_audio,\n        latent_osc=latent_osc,\n        osc_sender=sender,\n        buffer_frames=buffer_frames,\n        jit=jit,\n        profile=profile,\n        )\n\n    osc_host, osc_port = osc_in_addr.split(':')\n    osc_port = int(osc_port)\n    win = MainWindow(\n        use_backend=(not no_backend), \n        sender=sender,\n        backend=Proxy(backend),\n        osc_listen_addr=(osc_host, osc_port),\n        stress_gui=stress_gui,\n        text=text,\n        sampler_text=sampler_text,\n        )\n\n    available_geometry = win.screen().availableGeometry()\n    win.resize(available_geometry.width() / 2 * 1.063, available_geometry.height())\n    win.move((available_geometry.width() - win.width()), 0)\n    win.show()\n\n    # backend.start_stream()\n\n    sys.exit(app.exec())\n</code></pre>"},{"location":"reference/tungnaa/gui/senders/","title":"Senders","text":""},{"location":"reference/tungnaa/gui/senders/#tungnaa.gui.senders.GenericOSCSender","title":"<code>GenericOSCSender</code>","text":"<p>               Bases: <code>Sender</code></p> Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>class GenericOSCSender(Sender):\n    def __init__(self, \n            host:str='127.0.0.1', \n            port:int=57120, \n            lroute:str='/tungnaa/latents',\n            sroute:str='/tungnaa/status'\n            ):\n        self.client = udp_client.SimpleUDPClient(host, port)\n        self.latents_route = lroute\n        self.status_route = sroute\n\n    def __call__(self, d:Dict[str,Any]):\n        \"\"\"\n        Args:\n            d: dictionary with 'audio_t' key, other keys are ignored in this case\n\n            this should work as a callback for the backend:\n            the input dict should be returned to later be enqueued\n        \"\"\"\n        latents = [t.float().item() for t in d['latent_t'].squeeze()]\n        self.client.send_message(self.latents_route, latents)\n        return d\n\n    def send_status(self, cmd:str, value:Union[str,float]):\n        \"\"\"\n        Send a non-data message to SuperCollider\n\n        cmd     the status type/command &lt;mute|maybe more...&gt;\n        value   the value of the command, e.g. 1.0/0.0 for mute on/off\n        \"\"\"\n        self.client.send_message(self.status_route, value)\n</code></pre>"},{"location":"reference/tungnaa/gui/senders/#tungnaa.gui.senders.GenericOSCSender.__call__","title":"<code>__call__(d)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Any]</code> <p>dictionary with 'audio_t' key, other keys are ignored in this case</p> required <code>this</code> <code>should work as a callback for the backend</code> required Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def __call__(self, d:Dict[str,Any]):\n    \"\"\"\n    Args:\n        d: dictionary with 'audio_t' key, other keys are ignored in this case\n\n        this should work as a callback for the backend:\n        the input dict should be returned to later be enqueued\n    \"\"\"\n    latents = [t.float().item() for t in d['latent_t'].squeeze()]\n    self.client.send_message(self.latents_route, latents)\n    return d\n</code></pre>"},{"location":"reference/tungnaa/gui/senders/#tungnaa.gui.senders.GenericOSCSender.send_status","title":"<code>send_status(cmd, value)</code>","text":"<p>Send a non-data message to SuperCollider</p> <p>cmd     the status type/command  value   the value of the command, e.g. 1.0/0.0 for mute on/off Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def send_status(self, cmd:str, value:Union[str,float]):\n    \"\"\"\n    Send a non-data message to SuperCollider\n\n    cmd     the status type/command &lt;mute|maybe more...&gt;\n    value   the value of the command, e.g. 1.0/0.0 for mute on/off\n    \"\"\"\n    self.client.send_message(self.status_route, value)\n</code></pre>"},{"location":"reference/tungnaa/gui/senders/#tungnaa.gui.senders.SCSynthDirectOSCSender","title":"<code>SCSynthDirectOSCSender</code>","text":"<p>               Bases: <code>Sender</code></p> <p>sends timed bundles directly to scsynth to set a control bus</p> Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>class SCSynthDirectOSCSender(Sender):\n    \"\"\"sends timed bundles directly to scsynth to set a control bus\"\"\"\n    def __init__(self, \n            host:str='127.0.0.1', \n            port:int=57110, \n            bus_index:int=64,\n            latency:float=0.2,\n            ):\n        self.client = udp_client.SimpleUDPClient(host, port)\n        self.bus_index = bus_index\n        self.latency = latency\n\n    def __call__(self, d:Dict[str,Any]):\n        \"\"\"\n        Args:\n            d: dictionary with 'timestamp' and 'audio_t' keys,\n                other keys are ignored in this case\n        \"\"\"\n        bb = OscBundleBuilder(d['timestamp'] + self.latency)\n        mb = OscMessageBuilder('/c_set')\n\n        latents = [t.float().item() for t in d['latent_t'].squeeze()]\n        for i,l in enumerate(latents):\n            mb.add_arg(i+self.bus_index)\n            mb.add_arg(l)\n\n        bb.add_content(mb.build())\n        bundle = bb.build()\n\n        self.client.send(bundle)\n</code></pre>"},{"location":"reference/tungnaa/gui/senders/#tungnaa.gui.senders.SCSynthDirectOSCSender.__call__","title":"<code>__call__(d)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict[str, Any]</code> <p>dictionary with 'timestamp' and 'audio_t' keys, other keys are ignored in this case</p> required Source code in <code>src/tungnaa/gui/senders.py</code> <pre><code>def __call__(self, d:Dict[str,Any]):\n    \"\"\"\n    Args:\n        d: dictionary with 'timestamp' and 'audio_t' keys,\n            other keys are ignored in this case\n    \"\"\"\n    bb = OscBundleBuilder(d['timestamp'] + self.latency)\n    mb = OscMessageBuilder('/c_set')\n\n    latents = [t.float().item() for t in d['latent_t'].squeeze()]\n    for i,l in enumerate(latents):\n        mb.add_arg(i+self.bus_index)\n        mb.add_arg(l)\n\n    bb.add_content(mb.build())\n    bundle = bb.build()\n\n    self.client.send(bundle)\n</code></pre>"}]}